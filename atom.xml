<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[懒程序员改变世界]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://qinghua.github.io/"/>
  <updated>2016-04-29T14:04:17.000Z</updated>
  <id>http://qinghua.github.io/</id>
  
  <author>
    <name><![CDATA[Qinghua Gao]]></name>
    <email><![CDATA[ggggqh666@163.com]]></email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[轻松了解Kubernetes认证功能]]></title>
    <link href="http://qinghua.github.io/kubernetes-security/"/>
    <id>http://qinghua.github.io/kubernetes-security/</id>
    <published>2016-04-29T11:31:01.000Z</published>
    <updated>2016-04-29T14:04:17.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://kubernetes.io/docs/whatisk8s/" target="_blank" rel="external">Kubernetes</a>简称k8s，是谷歌于2014年开始主导的开源项目，提供了以容器为中心的部署、伸缩和运维平台。截止目前它的最新版本为1.2。搭建环境之前建议先了解一下kubernetes的相关知识，可以参考<a href="/kubernetes-in-mesos-1">《如果有10000台机器，你想怎么玩？》</a>系列文章。本文介绍kubernetes的安全性配置。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>首先需要搭建kubernetes集群环境，可以参考<a href="/kubernetes-installation">《轻松搭建Kubernetes 1.2版运行环境》</a>来安装自己的kubernetes集群，运行到flannel配置完成即可。接下来的api server等设置的参数可以参考本文。</p>
<p>结果应该是有三台虚拟机，一台叫做<strong>master</strong>，它的IP是<strong>192.168.33.17</strong>，运行着k8s的api server、controller manager和scheduler；另两台叫做<strong>node1</strong>和<strong>node2</strong>，它们的IP分别是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>，运行着k8s的kubelet和kube-proxy，当做k8s的两个节点。</p>
<h2 id="u90E8_u7F72"><a href="#u90E8_u7F72" class="headerlink" title="部署"></a>部署</h2><p>最简单的方式就是通过基于CSV的基本认证。首先需要创建api server的基本认证文件：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">mkdir security</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="number">123456</span>,admin,qinghua &gt; security/basic_auth.csv                      <span class="comment"># 格式：用户名,密码,用户ID</span></span><br></pre></td></tr></table></figure></p>
<p>然后就可以生成CA和api server的证书了：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> security</span><br><span class="line"></span><br><span class="line">openssl genrsa -out ca.key <span class="number">2048</span></span><br><span class="line">openssl req -x509 -new -nodes -key ca.key -subj <span class="string">"/CN=192.168.33.17"</span> -days <span class="number">10000</span> -out ca.crt</span><br><span class="line">openssl genrsa -out server.key <span class="number">2048</span></span><br><span class="line">openssl req -new -key server.key -subj <span class="string">"/CN=192.168.33.17"</span> -out server.csr</span><br><span class="line">openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br></pre></td></tr></table></figure></p>
<p>上面的命令会生成若干证书相关文件，作用如下：</p>
<ul>
<li>ca.key：自己生成的CA的私钥，用于模拟一个CA</li>
<li>ca.crt：用自己的私钥自签名的CA证书</li>
<li>server.key：api server的私钥，用于配置api server的https</li>
<li>server.csr：api server的证书请求文件，用于请求api server的证书</li>
<li>server.crt：用自己模拟的CA签发的api server的证书，用于配置api server的https</li>
</ul>
<p>接下来启动api server，参数的作用可以参考<a href="http://kubernetes.io/docs/admin/kube-apiserver/" target="_blank" rel="external">kube-apiserver官方文档</a>：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=apiserver \</span><br><span class="line">  --net=host \</span><br><span class="line">  -v /home/vagrant/security:/security \</span><br><span class="line">  gcr.io/google_containers/kube-apiserver:e68c6af15d4672feef7022e94ee4d9af \</span><br><span class="line">  kube-apiserver \</span><br><span class="line">  --advertise-address=<span class="number">192.168</span>.<span class="number">33.17</span> \</span><br><span class="line">  --admission-control=ServiceAccount \</span><br><span class="line">  --insecure-bind-address=<span class="number">0.0</span>.<span class="number">0.0</span> \</span><br><span class="line">  --etcd-servers=http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">4001</span> \</span><br><span class="line">  --service-cluster-ip-range=<span class="number">11.0</span>.<span class="number">0.0</span>/<span class="number">16</span> \</span><br><span class="line">  --tls-cert-file=/security/server.crt \</span><br><span class="line">  --tls-private-key-file=/security/server.key \</span><br><span class="line">  --secure-port=<span class="number">443</span> \</span><br><span class="line">  --basic-auth-file=/security/basic_auth.csv</span><br></pre></td></tr></table></figure></p>
<p>还需要启动controller manager，参数的作用可以参考<a href="http://kubernetes.io/docs/admin/kube-controller-manager/" target="_blank" rel="external">kube-controller-manager官方文档</a>：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=cm \</span><br><span class="line">  -v /home/vagrant/security:/security \</span><br><span class="line">  gcr.io/google_containers/kube-controller-manager:b9107c794e0564bf11719dc554213f7b \</span><br><span class="line">  kube-controller-manager \</span><br><span class="line">  --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> \</span><br><span class="line">  --cluster-cidr=<span class="number">10.245</span>.<span class="number">0.0</span>/<span class="number">16</span> \</span><br><span class="line">  --allocate-node-cidrs=<span class="literal">true</span> \</span><br><span class="line">  --root-ca-file=/security/ca.crt \</span><br><span class="line">  --service-account-private-key-file=/security/server.key</span><br></pre></td></tr></table></figure></p>
<p>最后是scheduler，参数的作用可以参考<a href="http://kubernetes.io/docs/admin/kube-scheduler/" target="_blank" rel="external">kube-scheduler官方文档</a>：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=scheduler \</span><br><span class="line">  gcr.io/google_containers/kube-scheduler:<span class="number">903</span>b34d5ed7367ec4dddf846675613c9 \</span><br><span class="line">  kube-scheduler \</span><br><span class="line">  --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<p>可以运行以下命令来确认安全配置已经生效：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -k -u admin:<span class="number">123456</span> https://<span class="number">127.0</span>.<span class="number">0.1</span>/</span><br><span class="line">curl -k -u admin:<span class="number">123456</span> https://<span class="number">127.0</span>.<span class="number">0.1</span>/api/v1</span><br></pre></td></tr></table></figure></p>
<p>最后启动kubelet和kube-proxy，参数的作用可以参考<a href="http://kubernetes.io/docs/admin/kubelet/" target="_blank" rel="external">kubelet官方文档</a>和<a href="http://kubernetes.io/docs/admin/kube-proxy/" target="_blank" rel="external">kube-proxy官方文档</a>：<br><figure class="highlight sh"><figcaption><span>node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">NODE_IP=`ifconfig eth1 | grep <span class="string">'inet addr:'</span> | cut <span class="operator">-d</span>: <span class="operator">-f</span>2 | cut <span class="operator">-d</span><span class="string">' '</span> <span class="operator">-f</span>1`</span><br><span class="line"></span><br><span class="line">sudo kubernetes/server/bin/kubelet \</span><br><span class="line">  --api-servers=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> \</span><br><span class="line">  --cluster-dns=<span class="number">11.0</span>.<span class="number">0.10</span> \</span><br><span class="line">  --cluster-domain=cluster.local \</span><br><span class="line">  --hostname-override=<span class="variable">$NODE_IP</span> \</span><br><span class="line">  --node-ip=<span class="variable">$NODE_IP</span> &gt; kubelet.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line"></span><br><span class="line">sudo kubernetes/server/bin/kube-proxy \</span><br><span class="line">  --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> \</span><br><span class="line">  --hostname-override=<span class="variable">$NODE_IP</span> &gt; proxy.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></table></figure></p>
<h2 id="u9A8C_u8BC1"><a href="#u9A8C_u8BC1" class="headerlink" title="验证"></a>验证</h2><p>如果需要通过https访问，kubectl的命令就略微有点儿麻烦了，需要用<code>basic_auth.csv</code>里配置的<code>admin/123456</code>来登录：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> https://<span class="number">192.168</span>.<span class="number">33.17</span> --insecure-skip-tls-verify=<span class="literal">true</span> --username=admin --password=<span class="number">123456</span> get po</span><br></pre></td></tr></table></figure></p>
<p>因为8080端口还开着，所以也可以通过http访问：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get po</span><br></pre></td></tr></table></figure></p>
<p>配置完成后，可以看到系统里有TYPE为<code>kubernetes.io/service-account-token</code>的秘密：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get secret</span><br></pre></td></tr></table></figure></p>
<p>里面有三条数据，分别是<code>ca.crt</code>，<code>namespace</code>和<code>token</code>，可以通过以下命令看到：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> describe secret</span><br></pre></td></tr></table></figure></p>
<p>如果你通过kubernetes启动了一个pod，就可以在容器的<code>/var/run/secrets/kubernetes.io/serviceaccount/</code>目录里看到以三个文件的形式看到这三条数据（这是<code>--admission-control=ServiceAccount</code>的功劳），当pod需要访问系统服务的时候，就可以使用它们了。可以使用以下命令看到系统的服务账号：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get serviceAccount</span><br></pre></td></tr></table></figure></p>
<h2 id="u7B80_u5316kubectl"><a href="#u7B80_u5316kubectl" class="headerlink" title="简化kubectl"></a>简化kubectl</h2><p>如果我们通过设置<code>--insecure-port=0</code>把api server的http端口关闭，那它就只能通过https访问了：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> apiserver</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=apiserver \</span><br><span class="line">  --net=host \</span><br><span class="line">  -v /home/vagrant/security:/security \</span><br><span class="line">  gcr.io/google_containers/kube-apiserver:e68c6af15d4672feef7022e94ee4d9af \</span><br><span class="line">  kube-apiserver \</span><br><span class="line">  --advertise-address=<span class="number">192.168</span>.<span class="number">33.17</span> \</span><br><span class="line">  --admission-control=ServiceAccount \</span><br><span class="line">  --insecure-bind-address=<span class="number">0.0</span>.<span class="number">0.0</span> \</span><br><span class="line">  --etcd-servers=http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">4001</span> \</span><br><span class="line">  --service-cluster-ip-range=<span class="number">11.0</span>.<span class="number">0.0</span>/<span class="number">16</span> \</span><br><span class="line">  --tls-cert-file=/security/server.crt \</span><br><span class="line">  --tls-private-key-file=/security/server.key \</span><br><span class="line">  --secure-port=<span class="number">443</span> \</span><br><span class="line">  --basic-auth-file=/security/basic_auth.csv \</span><br><span class="line">  --insecure-port=<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>这样的话，就连取个pod都得这么麻烦：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> https://<span class="number">192.168</span>.<span class="number">33.17</span> --insecure-skip-tls-verify=<span class="literal">true</span> --username=admin --password=<span class="number">123456</span> get po</span><br></pre></td></tr></table></figure></p>
<p>幸运的是，kubernetes提供了一种方式，让我们可以大大简化命令，只用这样就好了：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl get po</span><br></pre></td></tr></table></figure></p>
<p>下面就让我们来试一下吧！首先用<code>kubectl config</code>命令来配置admin用户：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl config <span class="built_in">set</span>-credentials admin --username=admin --password=<span class="number">123456</span></span><br></pre></td></tr></table></figure></p>
<p>然后是api server的访问方式，给集群起个名字叫qinghua：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl config <span class="built_in">set</span>-cluster qinghua --insecure-skip-tls-verify=<span class="literal">true</span> --server=https://<span class="number">192.168</span>.<span class="number">33.17</span></span><br></pre></td></tr></table></figure></p>
<p>接下来创建一个context，它连接用户admin和集群qinghua：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl config <span class="built_in">set</span>-context default/qinghua --user=admin --namespace=default --cluster=qinghua</span><br></pre></td></tr></table></figure></p>
<p>最后设置一下默认的context：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl config use-context default/qinghua</span><br></pre></td></tr></table></figure></p>
<p>然后就可以用我们的简化版啦：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl get po</span><br></pre></td></tr></table></figure></p>
<p>可以通过以下命令来看到当前kubectl的配置：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl config view</span><br></pre></td></tr></table></figure></p>
<p>能够看到如下内容：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    insecure-skip-tls-verify: <span class="literal">true</span></span><br><span class="line">    server: https://<span class="number">192.168</span>.<span class="number">33.17</span></span><br><span class="line">  name: qinghua</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: qinghua</span><br><span class="line">    namespace: default</span><br><span class="line">    user: admin</span><br><span class="line">  name: default/qinghua</span><br><span class="line">current-context: default/qinghua</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: admin</span><br><span class="line">  user:</span><br><span class="line">    password: <span class="string">"123456"</span></span><br><span class="line">    username: admin</span><br></pre></td></tr></table></figure></p>
<p>实际上这些配置都存放在<code>~/.kube/config</code>文件里：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.kube/config</span><br></pre></td></tr></table></figure></p>
<p>修改这个文件也可以实时生效。细心的童鞋们可以看到，cluster、context和users都是集合，也就是说如果需要切换用户和集群等，只需要设置默认context就可以了，非常方便。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://kubernetes.io/docs/whatisk8s/">Kubernetes</a>简称k8s，是谷歌于2014年开始主导的开源项目，提供了以容器为中心的部署、伸缩和运维平台。截止目前它的最新版本为1.2。搭建环境之前建议先了解一下kubernetes的相关知识，可以参考<a href="/kubernetes-in-mesos-1">《如果有10000台机器，你想怎么玩？》</a>系列文章。本文介绍kubernetes的安全性配置。<br>]]>
    
    </summary>
    
      <category term="docker" scheme="http://qinghua.github.io/tags/docker/"/>
    
      <category term="kubernetes" scheme="http://qinghua.github.io/tags/kubernetes/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[轻松了解Kubernetes部署功能]]></title>
    <link href="http://qinghua.github.io/kubernetes-deployment/"/>
    <id>http://qinghua.github.io/kubernetes-deployment/</id>
    <published>2016-04-24T06:13:20.000Z</published>
    <updated>2016-04-28T07:27:56.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://kubernetes.io/docs/whatisk8s/" target="_blank" rel="external">Kubernetes</a>简称k8s，是谷歌于2014年开始主导的开源项目，提供了以容器为中心的部署、伸缩和运维平台。截止目前它的最新版本为1.2。搭建环境之前建议先了解一下kubernetes的相关知识，可以参考<a href="/kubernetes-in-mesos-1">《如果有10000台机器，你想怎么玩？》</a>系列文章。本文介绍kubernetes的基本部署功能。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>首先需要搭建kubernetes集群环境，可以参考<a href="/kubernetes-installation">《轻松搭建Kubernetes 1.2版运行环境》</a>来安装自己的kubernetes集群。</p>
<p>结果应该是有三台虚拟机，一台叫做<strong>master</strong>，它的IP是<strong>192.168.33.17</strong>，运行着k8s的api server、controller manager和scheduler；另两台叫做<strong>node1</strong>和<strong>node2</strong>，它们的IP分别是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>，运行着k8s的kubelet和kube-proxy，当做k8s的两个节点。</p>
<h2 id="u90E8_u7F72_uFF08deployment_uFF09"><a href="#u90E8_u7F72_uFF08deployment_uFF09" class="headerlink" title="部署（deployment）"></a>部署（deployment）</h2><p>启动一个容器最简单的方法应该就是使用以下命令了：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> run my-nginx --image=nginx:<span class="number">1.7</span>.<span class="number">9</span></span><br></pre></td></tr></table></figure></p>
<p>如果一切工作正常，应该能看到一条消息<strong>deployment “my-nginx” created</strong>。<a href="http://kubernetes.io/docs/user-guide/deployments/" target="_blank" rel="external">Deployment</a>是kubernetes 1.2的一个新引入的概念，它包含着对<a href="http://kubernetes.io/docs/user-guide/pods/" target="_blank" rel="external">Pod</a>和将要代替<a href="http://kubernetes.io/docs/user-guide/replication-controller/" target="_blank" rel="external">Replication Controller</a>的<a href="http://kubernetes.io/docs/user-guide/replicasets/" target="_blank" rel="external">Replica Set</a>的描述。</p>
<p>为了简化命令，我们设置一个别名：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> kubectl=<span class="string">"kubernetes/server/bin/kubectl -s 192.168.33.17:8080"</span></span><br></pre></td></tr></table></figure></p>
<p>使用以下命令可以看到目前集群里的信息：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubectl get po                          <span class="comment"># 查看目前所有的pod</span></span><br><span class="line">kubectl get rs                          <span class="comment"># 查看目前所有的replica set</span></span><br><span class="line">kubectl get deployment                  <span class="comment"># 查看目前所有的deployment</span></span><br><span class="line">kubectl describe po my-nginx            <span class="comment"># 查看my-nginx pod的详细状态</span></span><br><span class="line">kubectl describe rs my-nginx            <span class="comment"># 查看my-nginx replica set的详细状态</span></span><br><span class="line">kubectl describe deployment my-nginx    <span class="comment"># 查看my-nginx deployment的详细状态</span></span><br></pre></td></tr></table></figure></p>
<p>用<code>kubectl describe po my-nginx</code>可以查看到这个pod被分配到哪台node上去。当<code>kubectl get po</code>显示<strong>1/1 Running</strong>时，说明容器已经启动完成了。SSH到那台虚拟机上，<code>docker ps</code>一下，能够看到有两个容器启动完成了，一个是nginx，另一个就是负责网络的pause。使用<code>docker rm -f</code>将nginx容器删除，再用<code>kubectl get po</code>查看，一会儿便会显示<strong>0/1 ContainerCreating</strong>，随即又变成<strong>1/1 Running</strong>，这是replica set的功劳。当它检测到容器挂掉的时候，便会重新启动一个容器来保证服务不中断。不仅是容器挂掉，停止虚拟机也能使容器被再分配到另一台node上，有兴趣的朋友可以自行尝试，虚拟机重启回来后记得再次运行flannel和kubelet哦。</p>
<p>使用以下命令可以删除my-nginx deployment，my-nginx replica set和my-nginx pod（抵消第一条run命令的作用）：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete deployment my-nginx</span><br></pre></td></tr></table></figure></p>
<p>可以使用<code>kubectl get</code>命令来查看删除后的结果。一般我们会把部署信息写在一个yaml格式的文件里，这样比较容易查看，并且方便写入各种参数：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;nginx.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  replicas: <span class="number">2</span></span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:<span class="number">1.7</span>.<span class="number">9</span></span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: <span class="number">80</span></span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: <span class="number">400</span>m</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create <span class="operator">-f</span> nginx.yaml --record</span><br></pre></td></tr></table></figure></p>
<p>Deployment文件的详细说明可以看<a href="http://kubernetes.io/docs/api-reference/extensions/v1beta1/definitions/" target="_blank" rel="external">Extensions API定义</a>，通用参数的详细信息可以看<a href="http://kubernetes.io/docs/api-reference/v1/definitions/" target="_blank" rel="external">Kubernetes API定义</a>。由于这次的replicas设置为2，kubernetes会帮我们启动并维持两个实例。如果我们想要更新部署的yaml，有两种方法。第一种是使用edit直接修改：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit deployment/nginx-deployment</span><br></pre></td></tr></table></figure></p>
<p>把<code>nginx:1.7.9</code>修改为<code>nginx:1.9.11</code>，保存退出。Kubernetes就会自动帮我们升级镜像。通过<code>kubectl get deployment nginx</code>的Events里可以看到升级的事件。不管是哪一种方法，升级的过程都是这样的：</p>
<ul>
<li>启动一个新容器</li>
<li>停止两个旧容器</li>
<li>启动一个新容器<br>启动一个新的容器，然后停止一个旧的，重复这个过程直到旧的容器全部停止为止。这样可以保证</li>
</ul>
<p>第二种是修改yaml文件，然后apply：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">"s/nginx:1.7.9/nginx:1.91/g"</span> nginx.yaml</span><br><span class="line">kubectl apply <span class="operator">-f</span> nginx.yaml</span><br></pre></td></tr></table></figure></p>
<p>细心的你可能已经发现我在上面的命令里把<code>nginx:1.9.1</code>打成<code>nginx:1.91</code>了。如果此时用<code>kubectl describe po nginx</code>命令，就能看到<strong>Error syncing pod, skipping: failed to “StartContainer” for “nginx” with ErrImagePull: “Tag 1.91 not found in repository docker.io/library/nginx”</strong>的错误。现在我需要停止这次升级。我们可以用这条命令来查看升级历史：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout <span class="built_in">history</span> deployment/nginx-deployment</span><br></pre></td></tr></table></figure></p>
<p>由于<code>kubectl create</code>的时候用了<code>--record</code>的标志，我们能够直接看到命令，方便定位到上一次正确的升级<strong>2 kubectl -s 192.168.33.17:8080 edit deployment/nginx-deployment</strong>，查看详细的deployment内容：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout <span class="built_in">history</span> deployment/nginx-deployment --revision=<span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>有两种方法可以回退到这个版本：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployment/nginx-deployment</span><br><span class="line">kubectl rollout undo deployment/nginx-deployment --to-revision=<span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<h2 id="u670D_u52A1"><a href="#u670D_u52A1" class="headerlink" title="服务"></a>服务</h2><p>前面创建了nginx的部署对象，那么别人如何使用nginx这个服务呢？首先要确定的是，这个nginx服务，是给内部使用的，还是外部。如果是内部使用，那就可以不用设置服务的类型（默认为ClusterIP），否则，可以将服务类型设置为NodePort，通过node的端口暴露出来给外部使用；或者是LoadBalancer，由云服务商提供一个负载均衡直接挂在服务上。这里我们使用NodePort，暴露出30088端口给外部使用。如果不指定nodePort，那么kubernetes会随机生成一个。下面让我们来启动服务：<br><figure class="highlight sh"><figcaption><span>master master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;nginx-svc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">  - port: <span class="number">80</span></span><br><span class="line">    targetPort: <span class="number">80</span></span><br><span class="line">    nodePort: <span class="number">30088</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create <span class="operator">-f</span> nginx-svc.yaml</span><br></pre></td></tr></table></figure></p>
<p>这样便可以通过<a href="http://192.168.33.18:30088" target="_blank" rel="external">http://192.168.33.18:30088</a>或<a href="http://192.168.33.19:30088" target="_blank" rel="external">http://192.168.33.19:30088</a>访问nginx服务了：<br><img src="/img/nginx.jpg" alt=""></p>
<p>可以使用以下命令来删除服务及nginx的部署：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete <span class="operator">-f</span> nginx-svc.yaml</span><br><span class="line">kubectl delete <span class="operator">-f</span> nginx.yaml</span><br></pre></td></tr></table></figure></p>
<h2 id="u4E00_u6B21_u6027_u4EFB_u52A1_uFF08job_uFF09"><a href="#u4E00_u6B21_u6027_u4EFB_u52A1_uFF08job_uFF09" class="headerlink" title="一次性任务（job）"></a>一次性任务（job）</h2><p>Kubernetes 1.1版时将<a href="http://kubernetes.io/docs/user-guide/jobs/" target="_blank" rel="external">Job</a>还是Beta版，1.2之后已经可用于生产环境。Job是包含着若干pod的一次性任务。它与Replication Controller和Replica Set最大的区别就是当容器正常停止后，不会再次重启以维持一定数量的pod提供服务。下面我们用busybox运行一个耗时30s的任务：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;busybox.yaml</span><br><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: busybox</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: busybox</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: busybox</span><br><span class="line">        image: busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">          - sleep</span><br><span class="line">          - <span class="string">"30"</span></span><br><span class="line">      restartPolicy: Never</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create <span class="operator">-f</span> busybox.yaml</span><br></pre></td></tr></table></figure></p>
<p>可以使用以下命令来取得目前的job：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get <span class="built_in">jobs</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到，30s之后，SUCCESSFUL从0变为1了，说明这个job已经顺利完成了。可以使用以下命令来查看所有的pod，包含在busybox的job里正常结束的pod：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods --show-all</span><br></pre></td></tr></table></figure></p>
<p>Job完成之后仍然会在那里。如果需要删除，运行以下两条命令之一：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete job busybox</span><br><span class="line">kubectl delete <span class="operator">-f</span> busybox.yaml</span><br></pre></td></tr></table></figure></p>
<p>相关的pod也会一并被删除。不过容器仍然会留在运行过这个pod的node上。这可以通过设置kubelet的<code>--maximum-dead-containers</code>和<code>--maximum-dead-containers-per-container</code>参数来解决。</p>
<h2 id="Daemon_Sets"><a href="#Daemon_Sets" class="headerlink" title="Daemon Sets"></a>Daemon Sets</h2><p>有时候需要每个node上都运行一个pod，比如监控或是日志收集等。这时候使用Daemon Sets就非常方便。我们用一个tomcat容器来做例子：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;tomcat.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-ds</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: tomcat</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: tomcat</span><br><span class="line">        image: tomcat:<span class="number">8.0</span>.<span class="number">30</span>-jre8</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: <span class="number">8080</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create <span class="operator">-f</span> tomcat.yaml</span><br></pre></td></tr></table></figure></p>
<p>运行完毕后，通过以下命令来取得运行结果：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get ds</span><br><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure></p>
<p>这个yaml文件和一开始的deployment的yaml文件格式很像，虽然我们没有指定replicas，但还是起了两个pod（因为我们有两个node）。可以ssh到这两个node上看看是不是每一个node上都启动了一个tomcat容器。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://kubernetes.io/docs/whatisk8s/">Kubernetes</a>简称k8s，是谷歌于2014年开始主导的开源项目，提供了以容器为中心的部署、伸缩和运维平台。截止目前它的最新版本为1.2。搭建环境之前建议先了解一下kubernetes的相关知识，可以参考<a href="/kubernetes-in-mesos-1">《如果有10000台机器，你想怎么玩？》</a>系列文章。本文介绍kubernetes的基本部署功能。<br>]]>
    
    </summary>
    
      <category term="docker" scheme="http://qinghua.github.io/tags/docker/"/>
    
      <category term="kubernetes" scheme="http://qinghua.github.io/tags/kubernetes/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[VR和其他科技的结合意味着什么]]></title>
    <link href="http://qinghua.github.io/vr-mix-with-other-techs/"/>
    <id>http://qinghua.github.io/vr-mix-with-other-techs/</id>
    <published>2016-04-23T13:23:22.000Z</published>
    <updated>2016-04-24T05:30:41.000Z</updated>
    <content type="html"><![CDATA[<p>随着VR走进现实，人们的双眼纷纷看到了它在影视、游戏等产业的巨大钱景。借用熊节在<a href="http://gigix.thoughtworkers.org/2015/8/28/driverless-cars/" target="_blank" rel="external">《无人驾驶汽车意味着什么》</a>里的一句话：“然而技术的成熟与投入实用远不是变革的终点，恰恰相反，这只是一场更为深远的变革的起点”。我脑洞大开，畅想一下VR和其他科技结合之后，将会如何影响我们的生活。看上去下一个十年又要迎来一次技术大爆炸。<br><a id="more"></a></p>
<h2 id="u65E0_u4EBA_u673A"><a href="#u65E0_u4EBA_u673A" class="headerlink" title="无人机"></a>无人机</h2><p>曾几何时，拍照、摄像还是摄影师的专利，随着数码相机的普及尤其是手机摄像头的发展，到了现在的自媒体时代，人人都喜欢拍一点什么。数量上去了，大家就开始想到提高质量。无人机给了我们这样一个上帝视角，在某种程度上圆了人类的飞行梦。现在的技术可以做到实时在手机上查看无人机正在拍摄的高清视频。VR结合无人机，会产生什么样的效果呢？看看这幅图：<br><img src="/img/vr-skydiving.jpg" alt=""></p>
<p>跳伞的话，只能下不能上，横向移动范围有限。VR结合无人机，能够带来更高一筹的高空体验，并且大大降低了安全事故发生的概率。当然无人机坏掉砸在路人脑袋上这种事就不在我们讨论范围内了…</p>
<h2 id="u673A_u5668_u4EBA"><a href="#u673A_u5668_u4EBA" class="headerlink" title="机器人"></a>机器人</h2><p>VR结合无人机，无论多么狂拽炫酷吊炸天，仍然是小众玩家的福利。而真正能够影响普通人的，应该算是和机器人结合。想像这样的世界：虽然分身乏术，但是你可以购买一个机器人，让它帮你去远方开会，到时候VR直接连接上这个机器人就好啦。这种方案的话，需要成熟的无人驾驶技术的配合，才能让机器人自动到达目的地。<br><img src="/img/vr-robot.jpg" alt=""></p>
<p>继续往里思考，为什么非要买一个机器人跑到远方去呢？是不是可以直接租赁一个远方的机器人呢？假设有一个提供机器人租赁的公司，直接在开会地点附近有一个机器人租赁点，与会人员就没必要派遣机器人过去了，直接每个人租一个机器人开会就好啦。</p>
<h2 id="3D_u6253_u5370"><a href="#3D_u6253_u5370" class="headerlink" title="3D打印"></a>3D打印</h2><p>虽然VR结合机器人听起来令人心动，但是有一个问题就是，大家都是机器人了，那我在VR里看出去都是一堆的机器人有什么用？根本不知道谁是谁，干脆直接电话会议或者视频会议不就好了嘛，反正表情动作眼神都很难感觉出来。这时候3D打印就派上用场了。假设到时候的3D打印技术非常成熟，你在家里就能很轻易地将你现在的模型传送给机器人租赁公司，而公司可以在较短的时间内打印一个像你一样的机器人，于是你在别人的眼里并不是一个机器人，而是真正的“你”。每个人都是自己的样子，开会就像是在现场一样。到时候的材料工艺也应该有很大进步，可以实现材料回收再利用，这样可以大大降低租用机器人的成本。<br><img src="/img/vr-humanoid.jpg" alt=""></p>
<p>想象一下在一个会议里，里面所有的“人”都只是躯壳，而真实的人都在其它地方，是不是有点儿感觉怪怪的了？</p>
<h2 id="u4EBA_u5DE5_u667A_u80FD"><a href="#u4EBA_u5DE5_u667A_u80FD" class="headerlink" title="人工智能"></a>人工智能</h2><p>VR结合3D打印，开始带来一丝怪异的感觉了。再加上人工智能的混入，就已经不是“怪异”两个字就能形容的了。想象你面对一个“人”，音容笑貌都是某个你熟悉的人，但“ta”可能并不是你熟悉的“ta”…“ta”可能是个具有人工智能的机器人！在良性的环境下，“ta”只可能是真人戴着VR在跟你交流，或者由于真人在忙其它事情，“ta”现在是个尽力在模仿真人的善意人工智能；在恶性的环境下，“它”可能被其他人所控制，要对你做出一些诈骗等恶意行为；最恶劣的是“ta”可能发展了自己的意识，你根本不知道“ta”在想什么，“ta”想干什么。这里引用霍金的预言：“成功制造出一台人工智能机器人将是人类历史上的里程碑。但不幸的是，它也可能会成为我们历史上最后的一个里程碑”。黑客帝国里的场景，也有可能变成现实。</p>
<h2 id="u5176_u4ED6_u7545_u60F3"><a href="#u5176_u4ED6_u7545_u60F3" class="headerlink" title="其他畅想"></a>其他畅想</h2><p>在上文的模型里，可以预见的是，信息安全将会是未来的重中之重。传输自己的模型给机器人公司，需要小心传错对象或被黑客拦截；VR连接远程机器人，需要小心机器人可能被其他人捷足先登；对人工智能，也许也需要必要的防范。虽然有许多安全上的顾虑，但也正是这些技术，可以让我们更加安全。矿工附身在机器人上，用自己的娴熟技巧采矿；消防员附身在机器人上，可以身冲火海而不会收到任何损伤；对于户外极限运动爱好者来说，这真的是加了无数条命呀。话说回来，如果要实现这样的效果，光自动驾驶是远远不够的。可能还需要如下的几点：</p>
<ul>
<li>高带宽+低延迟的移动网络：没有高带宽的支持，收到的VR场景数据可能分辨率或刷新率低，降低沉浸式体验。没有低延迟的支持，机器人反应慢就不那么适合争分夺秒型任务。</li>
<li><p>移动：现在大多数的VR都不支持移动。即使像HTC Vive那样，由于受到空间制约，也只是有限支持。如果要实用化，下图的平面移动能够满足一定的需求，不过还需考虑包含上下的三维空间。<br><img src="/img/vr-move.jpg" alt=""></p>
</li>
<li><p>材料和传感器：为了完美体验触感，也许还需要能够快速打印出具有一定质量、密度、硬度的物质，需要极高的材料工艺，并且需要机器人身上安装高精度传感器实时传回需要的材料数据。</p>
</li>
<li>环境：微风轻抚和狂风暴雨是完全不一样的感受。还要考虑温度、湿度等感觉。</li>
<li>嗅觉支持：虽然人类的鼻子没有狗鼻子那么灵敏，但是现实生活中，有时候气味是决定下一步行动的原因。比如闻到了焦糊味就去厨房看看。相比起来，对于味觉的支持就可以缓一缓了。</li>
<li>梦境或意识：除了上面的移动、材料和传感器的硬派支持，还有一种方式是以类似催眠、梦境或者存意识的方式来支持这种感觉的传递。阿凡达！！</li>
</ul>
<p>由于有了机器人替身，战争的形式也应该会发生变化。士兵们不需要搏命了，也许打战变成了一场游戏。开战前拔掉对方的VR中心可能变成了一种战略选择。毕竟如果存在由人控制的机器人对人类的战争，那结果应该像是现代兵器对战冷兵器时代，是一边倒的。希望不要有战争。即使有战争也别培育出作战型人工智能。</p>
<p>再想点儿其他贴近生活的方面，可能人们就不太需要出行了。现在的上班族大多经历过天天挤公交地铁或者开车堵在路上的罪。以后不需要了。公司摆一个你的机器人躯壳，家里来副VR眼镜，一切搞定。还有什么场景需要出行？旅游？现在的VR都已经快要解决这个问题了，各大VR视频资源库里已经有不少全景漫游的视频了。风景变化不大，甚至不需要机器人的代劳，直接数字化就好了。什么九寨沟一年四季各有各的看点？巴尔夏明神庙被炸毁？华南虎灭绝了？统统都能看到。购物也是同理，送货上门应该算是未来商店最基本的服务之一了。探亲？这个确实有些不好回答，也许有些家庭无所谓真的假的，有些家庭会更倾向于真人探亲。对了，吃饭和看病机器人就没法代劳了吧？何不反过来想想这个问题，虽然机器人不能代替你去饭店吃饭，但是大厨可以附身在你家的机器人上，给你做出美味佳肴！没有趁手的工具？3D打印！看病也是同理，就是可能会由于医疗器械比较昂贵或者各种许可证、专利问题，也许最终还是得你亲自跑一趟。上厕所？想必你每天上厕所不至于跑个几公里吧…不管怎样，VR和其他技术的结合确实能够大大减少出行的次数。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>随着VR走进现实，人们的双眼纷纷看到了它在影视、游戏等产业的巨大钱景。借用熊节在<a href="http://gigix.thoughtworkers.org/2015/8/28/driverless-cars/">《无人驾驶汽车意味着什么》</a>里的一句话：“然而技术的成熟与投入实用远不是变革的终点，恰恰相反，这只是一场更为深远的变革的起点”。我脑洞大开，畅想一下VR和其他科技结合之后，将会如何影响我们的生活。看上去下一个十年又要迎来一次技术大爆炸。<br>]]>
    
    </summary>
    
      <category term="thought" scheme="http://qinghua.github.io/tags/thought/"/>
    
      <category term="vr" scheme="http://qinghua.github.io/categories/vr/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用RADOSGW联盟网关跨域同步]]></title>
    <link href="http://qinghua.github.io/ceph-radosgw-replication/"/>
    <id>http://qinghua.github.io/ceph-radosgw-replication/</id>
    <published>2016-04-22T11:01:01.000Z</published>
    <updated>2016-04-23T13:38:47.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://ceph.com/" target="_blank" rel="external">Ceph</a>是一个高性能的PB级分布式文件系统。它能够在一个系统中提供对象存储、块存储和文件存储。本文的主要内容是如何<a href="http://docs.ceph.com/docs/master/radosgw/federated-config/" target="_blank" rel="external">通过RADOSGW备份一个可用区</a>。</p>
<ul>
<li>对如何加载使用块存储和文件存储感兴趣可以参考<a href="/ceph-demo">《用容器轻松搭建ceph实验环境》</a></li>
<li>对RADOSGW如何暴露S3和Swift接口感兴趣可以参考<a href="/ceph-radosgw">《通过RADOSGW提供ceph的S3和Swift接口》</a><a id="more"></a>
</li>
</ul>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配三台虚拟机，一台叫做<strong>us-east</strong>，它的IP是<strong>192.168.33.15</strong>；另一台叫做<strong>us-west</strong>，它的IP是<strong>192.168.33.16</strong>；第三台叫做<strong>eu-east</strong>，它的IP是<strong>192.168.33.17</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"us-east"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"us-east"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.15"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"us-west"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"us-west"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.16"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"eu-east"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"eu-east"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>然后分别在三个终端运行以下命令启动并连接三台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh us-east</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh us-west</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh eu-east</span><br></pre></td></tr></table></figure>
<h2 id="u80CC_u666F_u4ECB_u7ECD"><a href="#u80CC_u666F_u4ECB_u7ECD" class="headerlink" title="背景介绍"></a>背景介绍</h2><p><a href="http://docs.ceph.com/docs/master/radosgw/" target="_blank" rel="external">Ceph对象网关（Ceph Object Gateway，radosgw）</a>提供了S3和Swift的API，同时也支持S3的一些概念。<a href="http://docs.ceph.com/docs/master/radosgw/config-ref/#regions" target="_blank" rel="external">辖区（region）</a>表明了一个地理位置，比如us。在这个辖区里可以有多个<a href="http://docs.ceph.com/docs/master/radosgw/config-ref/#zones" target="_blank" rel="external">域（zone）</a>，比如east和west。一个域里可以有多个实例，一个实例可以有多个<a href="http://docs.ceph.com/docs/master/glossary/#term-ceph-node" target="_blank" rel="external">节点（node）</a>。同时，配置一个域需要一系列的<a href="http://docs.ceph.com/docs/master/radosgw/config-ref/#pools" target="_blank" rel="external">存储池（pool）</a>。如下图：<br><img src="/img/ceph-radosgw-replication-config.png" alt=""></p>
<p>对于这次练习，我们使用下图的架构：<br><img src="/img/ceph-radosgw-replication-example.png" alt=""></p>
<p>一个us辖区里有us-east和us-west两个域，每个域里各有一个实例，分别为us-east-1和us-west-1。还有一个eu的辖区，里面有一个eu-east的域，一个为eu-east-1的实例。我们将会首先实现同一个辖区（us）里的同步，然后是不同辖区的同步。相同辖区可以同步元数据和数据对象，而不同的辖区只能同步元数据而不能同步数据对象。元数据包括网关用户和存储桶（bucket）。</p>
<h2 id="u76F8_u540C_u8F96_u533A_u7684_u540C_u6B65"><a href="#u76F8_u540C_u8F96_u533A_u7684_u540C_u6B65" class="headerlink" title="相同辖区的同步"></a>相同辖区的同步</h2><p>首先需要安装一些ceph、radosgw的依赖包：<br><figure class="highlight sh"><figcaption><span>us-east us-west eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y --force-yes install ceph-common radosgw radosgw-agent</span><br></pre></td></tr></table></figure></p>
<p>为了提供HTTP服务，还需要所有虚拟机都安装apache2和FastCGI：<br><figure class="highlight sh"><figcaption><span>us-east us-west eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get -y --force-yes install apache2 libapache2-mod-fastcgi</span><br></pre></td></tr></table></figure></p>
<p>然后就可以分别启动ceph/demo这个容器来轻松提供ceph服务了：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=host -v /etc/ceph:/etc/ceph <span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">33.15</span> <span class="operator">-e</span> CEPH_NETWORK=<span class="number">192.168</span>.<span class="number">33.15</span>/<span class="number">24</span> --name=ceph ceph/demo</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=host -v /etc/ceph:/etc/ceph <span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">33.16</span> <span class="operator">-e</span> CEPH_NETWORK=<span class="number">192.168</span>.<span class="number">33.16</span>/<span class="number">24</span> --name=ceph ceph/demo</span><br></pre></td></tr></table></figure>
<p>然后手动在各自虚拟机上创建一些提供给域使用的存储池，这一步不是必须的，因为我们创建的网关用户是有权限自动创建存储池的：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">ceph osd pool create .us-east.rgw.root <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.rgw.control <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.rgw.gc <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.rgw.buckets <span class="number">512</span> <span class="number">512</span></span><br><span class="line">ceph osd pool create .us-east.rgw.buckets.index <span class="number">32</span> <span class="number">32</span></span><br><span class="line">ceph osd pool create .us-east.rgw.buckets.extra <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.intent-log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.usage <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.users <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.users.email <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.users.swift <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.users.uid <span class="number">16</span> <span class="number">16</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">ceph osd pool create .us-west.rgw.root <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.rgw.control <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.rgw.gc <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.rgw.buckets <span class="number">512</span> <span class="number">512</span></span><br><span class="line">ceph osd pool create .us-west.rgw.buckets.index <span class="number">32</span> <span class="number">32</span></span><br><span class="line">ceph osd pool create .us-west.rgw.buckets.extra <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.intent-log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.usage <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.users <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.users.email <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.users.swift <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.users.uid <span class="number">16</span> <span class="number">16</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<p>两台虚拟机对应两个实例，接下来为这两个实例分别创建密钥环（keyring），用它生成网关的用户和密钥（key），增加密钥的rwx权限并让其有权限访问Ceph对象集群：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph auth del client.radosgw.gateway</span><br><span class="line">sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.us-east-<span class="number">1</span> --gen-key</span><br><span class="line">sudo ceph-authtool -n client.radosgw.us-east-<span class="number">1</span> --cap osd <span class="string">'allow rwx'</span> --cap mon <span class="string">'allow rw'</span> /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.us-east-<span class="number">1</span> -i /etc/ceph/ceph.client.radosgw.keyring</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph auth del client.radosgw.gateway</span><br><span class="line">sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.us-west-<span class="number">1</span> --gen-key</span><br><span class="line">sudo ceph-authtool -n client.radosgw.us-west-<span class="number">1</span> --cap osd <span class="string">'allow rwx'</span> --cap mon <span class="string">'allow rw'</span> /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.us-west-<span class="number">1</span> -i /etc/ceph/ceph.client.radosgw.keyring</span><br></pre></td></tr></table></figure>
<p>接下来创建一个apache2的配置文件，监听80端口并把请求转发到radosgw提供的FastCGI 9000端口（稍后将会配置）上：<br><figure class="highlight sh"><figcaption><span>us-east us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; rgw.conf</span><br><span class="line">FastCgiExternalServer /var/www/s3gw.fcgi -host localhost:<span class="number">9000</span></span><br><span class="line"></span><br><span class="line">&lt;VirtualHost *:<span class="number">80</span>&gt;</span><br><span class="line">    ServerName localhost</span><br><span class="line">    ServerAlias *.localhost</span><br><span class="line">    ServerAdmin qinghua@ggg.com</span><br><span class="line">    DocumentRoot /var/www</span><br><span class="line">    RewriteEngine On</span><br><span class="line">    RewriteRule  ^/(.*) /s3gw.fcgi?%&#123;QUERY_STRING&#125; [E=HTTP_AUTHORIZATION:%&#123;HTTP:Authorization&#125;,L]</span><br><span class="line"></span><br><span class="line">    &lt;IfModule mod_fastcgi.c&gt;</span><br><span class="line">           &lt;Directory /var/www&gt;</span><br><span class="line">            Options +ExecCGI</span><br><span class="line">            AllowOverride All</span><br><span class="line">            SetHandler fastcgi-script</span><br><span class="line">            Order allow,deny</span><br><span class="line">            Allow from all</span><br><span class="line">            AuthBasicAuthoritative Off</span><br><span class="line">        &lt;/Directory&gt;</span><br><span class="line">    &lt;/IfModule&gt;</span><br><span class="line"></span><br><span class="line">    AllowEncodedSlashes On</span><br><span class="line">    ErrorLog /var/<span class="built_in">log</span>/apache2/error.log</span><br><span class="line">    CustomLog /var/<span class="built_in">log</span>/apache2/access.log combined</span><br><span class="line">    ServerSignature Off</span><br><span class="line">&lt;/VirtualHost&gt;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv rgw.conf /etc/apache2/conf-enabled/rgw.conf</span><br></pre></td></tr></table></figure></p>
<p>由于上述配置需要用到apache2默认未加载的<a href="http://httpd.apache.org/docs/current/mod/mod_rewrite.html" target="_blank" rel="external">rewrite模块</a>，所以需要加载并重新启动apache2：<br><figure class="highlight sh"><figcaption><span>us-east us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo a2enmod rewrite</span><br><span class="line">sudo service apache2 restart</span><br></pre></td></tr></table></figure></p>
<p>FastCGI需要一个脚本来启用兼容S3的接口，同样也是所有虚拟机都要配，但是实例名略有区别：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; s3gw.fcgi</span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line"><span class="built_in">exec</span> /usr/bin/radosgw -c /etc/ceph/ceph.conf -n client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv s3gw.fcgi /var/www/s3gw.fcgi</span><br><span class="line">sudo chmod +x /var/www/s3gw.fcgi</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; s3gw.fcgi</span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line"><span class="built_in">exec</span> /usr/bin/radosgw -c /etc/ceph/ceph.conf -n client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv s3gw.fcgi /var/www/s3gw.fcgi</span><br><span class="line">sudo chmod +x /var/www/s3gw.fcgi</span><br></pre></td></tr></table></figure>
<p>现在到了在<code>ceph.conf</code>配置实例的时候了：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'$a rgw region root pool = .us.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zonegroup root pool = .us.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a [client.radosgw.us-east-1]'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw region = us'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone = us-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone root pool = .us-east.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw dns name = us-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a host = us-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a keyring = /etc/ceph/ceph.client.radosgw.keyring'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw socket path = ""'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a log file = /var/log/radosgw/client.radosgw.us-east-1.log'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw frontends = fastcgi socket_port=9000 socket_host=0.0.0.0'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw print continue = false'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'$a rgw region root pool = .us.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zonegroup root pool = .us.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a [client.radosgw.us-west-1]'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw region = us'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone = us-west'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone root pool = .us-west.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw dns name = us-west'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a host = us-west'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a keyring = /etc/ceph/ceph.client.radosgw.keyring'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw socket path = ""'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a log file = /var/log/radosgw/client.radosgw.us-west-1.log'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw frontends = fastcgi socket_port=9000 socket_host=0.0.0.0'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw print continue = false'</span> /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure>
<p>配置参数及其作用可以在<a href="http://docs.ceph.com/docs/master/radosgw/config-ref/" target="_blank" rel="external">这里</a>查到，下面列出了一部分与辖区和域有关的参数：</p>
<ul>
<li><strong>rgw region root pool</strong>：v.67版本中指定辖区所使用的存储池</li>
<li><strong>rgw zonegroup root pool</strong>：Jewel版本中指定辖区所使用的存储池</li>
<li><strong>rgw region</strong>：指定该实例的辖区名</li>
<li><strong>rgw zone</strong>：指定该实例的域名</li>
<li><strong>rgw zone root pool</strong>：指定域所使用的存储池</li>
</ul>
<p>接下来在各自实例上生成一个相同的json格式的辖区文件：<br><figure class="highlight sh"><figcaption><span>us-east us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">cat &lt;&lt; EOF &gt; us.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"us"</span>,</span><br><span class="line">  <span class="string">"api_name"</span>: <span class="string">"us"</span>,</span><br><span class="line">  <span class="string">"is_master"</span>: <span class="string">"true"</span>,</span><br><span class="line">  <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.15:80\/"</span>],</span><br><span class="line">  <span class="string">"master_zone"</span>: <span class="string">"us-east"</span>,</span><br><span class="line">  <span class="string">"zones"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"us-east"</span>,</span><br><span class="line">      <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.15:80\/"</span>],</span><br><span class="line">      <span class="string">"log_meta"</span>: <span class="string">"true"</span>,</span><br><span class="line">      <span class="string">"log_data"</span>: <span class="string">"true"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"us-west"</span>,</span><br><span class="line">      <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.16:80\/"</span>],</span><br><span class="line">      <span class="string">"log_meta"</span>: <span class="string">"true"</span>,</span><br><span class="line">      <span class="string">"log_data"</span>: <span class="string">"true"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"placement_targets"</span>: [</span><br><span class="line">   &#123;</span><br><span class="line">     <span class="string">"name"</span>: <span class="string">"default-placement"</span>,</span><br><span class="line">     <span class="string">"tags"</span>: []</span><br><span class="line">   &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"default_placement"</span>: <span class="string">"default-placement"</span></span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>然后通过辖区文件生成us辖区并设置其为默认辖区：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin region <span class="built_in">set</span> --infile us.json --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">radosgw-admin region default --rgw-region=us --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin region <span class="built_in">set</span> --infile us.json --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">radosgw-admin region default --rgw-region=us --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<p>辖区搞定之后，就轮到域啦。在各自实例上生成两个相同的json格式的域文件：<br><figure class="highlight sh"><figcaption><span>us-east us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">cat &lt;&lt; EOF &gt; us-east.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"domain_root"</span>: <span class="string">".us-east.domain.rgw"</span>,</span><br><span class="line">  <span class="string">"control_pool"</span>: <span class="string">".us-east.rgw.control"</span>,</span><br><span class="line">  <span class="string">"gc_pool"</span>: <span class="string">".us-east.rgw.gc"</span>,</span><br><span class="line">  <span class="string">"log_pool"</span>: <span class="string">".us-east.log"</span>,</span><br><span class="line">  <span class="string">"intent_log_pool"</span>: <span class="string">".us-east.intent-log"</span>,</span><br><span class="line">  <span class="string">"usage_log_pool"</span>: <span class="string">".us-east.usage"</span>,</span><br><span class="line">  <span class="string">"user_keys_pool"</span>: <span class="string">".us-east.users"</span>,</span><br><span class="line">  <span class="string">"user_email_pool"</span>: <span class="string">".us-east.users.email"</span>,</span><br><span class="line">  <span class="string">"user_swift_pool"</span>: <span class="string">".us-east.users.swift"</span>,</span><br><span class="line">  <span class="string">"user_uid_pool"</span>: <span class="string">".us-east.users.uid"</span>,</span><br><span class="line">  <span class="string">"system_key"</span>: &#123;</span><br><span class="line">    <span class="string">"access_key"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"secret_key"</span>: <span class="string">""</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"placement_pools"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"key"</span>: <span class="string">"default-placement"</span>,</span><br><span class="line">      <span class="string">"val"</span>: &#123;</span><br><span class="line">        <span class="string">"index_pool"</span>: <span class="string">".us-east.rgw.buckets.index"</span>,</span><br><span class="line">        <span class="string">"data_pool"</span>: <span class="string">".us-east.rgw.buckets"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed <span class="string">'s/east/west/g'</span> us-east.json &gt; us-west.json</span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>然后通过域文件生成两个域并更新辖区图（region map）：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-east --infile us-east.json --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-west --infile us-west.json --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-east --infile us-east.json --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-west --infile us-west.json --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<p>在<code>us-east-1</code>实例上，生成<code>us-east</code>的用户，并用生成的<code>access_key</code>和<code>secret_key</code>填充刚才为空的<code>us-east.json</code>文件，并将其复制到<code>us-west</code>虚拟机上：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin user create --uid=<span class="string">"us-east"</span> --display-name=<span class="string">"Region-US Zone-East"</span> --name client.radosgw.us-east-<span class="number">1</span> --system | tee eastuser.txt</span><br><span class="line"><span class="built_in">export</span> SRC_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> eastuser.txt`</span><br><span class="line"><span class="built_in">export</span> SRC_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> eastuser.txt`</span><br><span class="line">sed -i <span class="string">"s/access_key\": \"/access_key\": \"<span class="variable">$SRC_ACCESS_KEY</span>/g"</span> us-east.json</span><br><span class="line">sed -i <span class="string">"s/secret_key\": \"/secret_key\": \"<span class="variable">$SRC_SECRET_KEY</span>/g"</span> us-east.json</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line">docker cp ceph:us-east.json us-east.json</span><br><span class="line">scp us-east.json vagrant@<span class="number">192.168</span>.<span class="number">33.16</span>:/home/vagrant    <span class="comment"># vagrant的密码也是vagrant</span></span><br></pre></td></tr></table></figure></p>
<p>在<code>us-west-1</code>实例上，生成<code>us-west</code>的用户，也用生成的<code>access_key</code>和<code>secret_key</code>填充刚才为空的<code>us-west.json</code>文件，并将其复制到<code>us-east</code>虚拟机上：<br><figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin user create --uid=<span class="string">"us-west"</span> --display-name=<span class="string">"Region-US Zone-West"</span> --name client.radosgw.us-west-<span class="number">1</span> --system | tee westuser.txt</span><br><span class="line"><span class="built_in">export</span> DEST_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> westuser.txt`</span><br><span class="line"><span class="built_in">export</span> DEST_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> westuser.txt`</span><br><span class="line">sed -i <span class="string">"s/access_key\": \"/access_key\": \"<span class="variable">$DEST_ACCESS_KEY</span>/g"</span> us-west.json</span><br><span class="line">sed -i <span class="string">"s/secret_key\": \"/secret_key\": \"<span class="variable">$DEST_SECRET_KEY</span>/g"</span> us-west.json</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line">docker cp ceph:us-west.json us-west.json</span><br><span class="line">scp us-west.json vagrant@<span class="number">192.168</span>.<span class="number">33.15</span>:/home/vagrant    <span class="comment"># vagrant的密码也是vagrant</span></span><br></pre></td></tr></table></figure></p>
<p>现在两台虚拟机的用户主文件夹里都有对方的json文件，分别复制进ceph容器里：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker cp us-west.json ceph:/us-west.json</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker cp us-east.json ceph:/us-east.json</span><br></pre></td></tr></table></figure>
<p>接下来分别在两个实例里更新带了<code>access_key</code>和<code>secret_key</code>的各两个域：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-east --infile us-east.json --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-west --infile us-west.json --name client.radosgw.us-east-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-east --infile us-east.json --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-west --infile us-west.json --name client.radosgw.us-west-<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>都完成了以后，就可以重启ceph服务和apache2啦：<br><figure class="highlight sh"><figcaption><span>us-east us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker restart ceph</span><br><span class="line">sudo /etc/init.d/radosgw start</span><br><span class="line">sudo service apache2 restart</span><br></pre></td></tr></table></figure></p>
<p>Apache2启动完成后，在浏览器打开<a href="http://192.168.33.15/" target="_blank" rel="external">http://192.168.33.15/</a>或<a href="http://192.168.33.16/" target="_blank" rel="external">http://192.168.33.16/</a>应该能看到下面的xml：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">ListAllMyBucketsResult</span> <span class="attribute">xmlns</span>=<span class="value">"http://s3.amazonaws.com/doc/2006-03-01/"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">Owner</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">ID</span>&gt;</span>anonymous<span class="tag">&lt;/<span class="title">ID</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">DisplayName</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">Owner</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">Buckets</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">ListAllMyBucketsResult</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>如果只看到了500的错误，等一会儿再刷新一次即可。如果遇到麻烦，可以这样调试：输入命令<code>sudo lsof -i :9000</code>看看是否radosgw启动了这个端口。如果没有，输入命令<code>ps -ef | grep radosgw</code>看看radosgw是否正常启动。若是正常启动，应该会有一个<code>/usr/bin/radosgw -n client.radosgw.us-east-1</code>的进程。若是没有正常启动，可以检查<code>/ect/ceph/ceph.conf</code>的内容，一般都是配置有问题。</p>
<p>搞定ceph和apache2后，在<code>us-east</code>里用python的boto库给<code>us-east-1</code>实例创建一个名为<code>my-new-bucket</code>的存储桶，并给<code>ggg</code>的key上传一句<strong>Hello world</strong>：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SRC_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> us-east.json`</span><br><span class="line"><span class="built_in">export</span> SRC_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> us-east.json`</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; s3test.py</span><br><span class="line">import boto</span><br><span class="line">import boto.s3</span><br><span class="line">import boto.s3.connection</span><br><span class="line">import os</span><br><span class="line">from boto.s3.key import Key</span><br><span class="line"></span><br><span class="line">access_key = os.environ[<span class="string">"SRC_ACCESS_KEY"</span>]</span><br><span class="line">secret_key = os.environ[<span class="string">"SRC_SECRET_KEY"</span>]</span><br><span class="line">conn = boto.connect_s3(</span><br><span class="line">  aws_access_key_id = access_key,</span><br><span class="line">  aws_secret_access_key = secret_key,</span><br><span class="line">  host = <span class="string">'192.168.33.15'</span>,</span><br><span class="line">  is_secure=False,</span><br><span class="line">  calling_format = boto.s3.connection.OrdinaryCallingFormat(),</span><br><span class="line">)</span><br><span class="line">bucket = conn.create_bucket(<span class="string">'my-new-bucket'</span>)</span><br><span class="line"></span><br><span class="line">k = Key(bucket)</span><br><span class="line">k.key = <span class="string">'ggg'</span></span><br><span class="line">k.set_contents_from_string(<span class="string">'Hello world'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> bucket <span class="keyword">in</span> conn.get_all_buckets():</span><br><span class="line">  <span class="built_in">print</span> <span class="string">"&#123;name&#125;\t&#123;created&#125;"</span>.format(</span><br><span class="line">    name = bucket.name,</span><br><span class="line">    created = bucket.creation_date,</span><br><span class="line">)</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">python s3test.py</span><br></pre></td></tr></table></figure></p>
<p>现在就该启动<code>radosgw-agent</code>来同步数据啦：<br><figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SRC_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> us-east.json`</span><br><span class="line"><span class="built_in">export</span> SRC_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> us-east.json`</span><br><span class="line"><span class="built_in">export</span> DEST_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> us-west.json`</span><br><span class="line"><span class="built_in">export</span> DEST_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> us-west.json`</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; cluster-data-sync.conf</span><br><span class="line">src_zone: us-east</span><br><span class="line"><span class="built_in">source</span>: http://<span class="number">192.168</span>.<span class="number">33.15</span></span><br><span class="line">src_access_key: <span class="variable">$SRC_ACCESS_KEY</span></span><br><span class="line">src_secret_key: <span class="variable">$SRC_SECRET_KEY</span></span><br><span class="line">dest_zone: us-west</span><br><span class="line">destination: http://<span class="number">192.168</span>.<span class="number">33.16</span></span><br><span class="line">dest_access_key: <span class="variable">$DEST_ACCESS_KEY</span></span><br><span class="line">dest_secret_key: <span class="variable">$DEST_SECRET_KEY</span></span><br><span class="line"><span class="built_in">log</span>_file: /var/<span class="built_in">log</span>/radosgw/radosgw-sync-us-east-west.log</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo radosgw-agent -c cluster-data-sync.conf</span><br></pre></td></tr></table></figure></p>
<p>再打开一个终端窗口，用以下命令查看<code>us-west-1</code>实例是不是已经把<code>my-new-bucket</code>同步过来啦：<br><figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin metadata bucket list --name client.radosgw.us-west-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>可能是由于单机<code>ceph/demo</code>容器的性能极差，在同步对象的时候基本上就一直停在<strong>INFO:radosgw_agent.worker:syncing bucket “my-new-bucket”</strong>上。如果有真实环境的ceph应该能够很快同步过来。如果同步成功，可以用以下命令来得到刚才的<strong>Hello world</strong>：<br><figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; s3download.py</span><br><span class="line">import boto</span><br><span class="line">import boto.s3</span><br><span class="line">import boto.s3.connection</span><br><span class="line">import os</span><br><span class="line">from boto.s3.key import Key</span><br><span class="line"></span><br><span class="line">access_key = os.environ[<span class="string">"SRC_ACCESS_KEY"</span>]</span><br><span class="line">secret_key = os.environ[<span class="string">"SRC_SECRET_KEY"</span>]</span><br><span class="line">conn = boto.connect_s3(</span><br><span class="line">  aws_access_key_id = access_key,</span><br><span class="line">  aws_secret_access_key = secret_key,</span><br><span class="line">  host = <span class="string">'192.168.33.16'</span>,</span><br><span class="line">  is_secure=False,</span><br><span class="line">  calling_format = boto.s3.connection.OrdinaryCallingFormat(),</span><br><span class="line">)</span><br><span class="line">bucket = conn.get_bucket(<span class="string">'my-new-bucket'</span>)</span><br><span class="line"></span><br><span class="line">key = bucket.get_key(<span class="string">'ggg'</span>)</span><br><span class="line"><span class="built_in">print</span> key.get_contents_as_string()</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">python s3download.py</span><br></pre></td></tr></table></figure></p>
<p>下面是一张同辖区同步示意图：<br><img src="http://docs.ceph.com/docs/master/_images/zone-sync.png" alt=""></p>
<h2 id="u4E0D_u540C_u8F96_u533A_u7684_u540C_u6B65"><a href="#u4E0D_u540C_u8F96_u533A_u7684_u540C_u6B65" class="headerlink" title="不同辖区的同步"></a>不同辖区的同步</h2><p>不同的辖区只能同步元数据而不能同步数据对象。接下来我们在eu-east上，尝试同步us-east的元数据。有了<a href="/ceph-radosgw-replication/#u76F8_u540C_u8F96_u533A_u7684_u540C_u6B65">相同辖区的同步</a>的经验，这回就不详细介绍下面的命令了：<br><figure class="highlight sh"><figcaption><span>eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=host -v /etc/ceph:/etc/ceph <span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">33.17</span> <span class="operator">-e</span> CEPH_NETWORK=<span class="number">192.168</span>.<span class="number">33.17</span>/<span class="number">24</span> --name=ceph ceph/demo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建存储池这一步也同上面一样是可选</span></span><br><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">ceph osd pool create .eu-east.rgw.root <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.rgw.control <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.rgw.gc <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.rgw.buckets <span class="number">512</span> <span class="number">512</span></span><br><span class="line">ceph osd pool create .eu-east.rgw.buckets.index <span class="number">32</span> <span class="number">32</span></span><br><span class="line">ceph osd pool create .eu-east.rgw.buckets.extra <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.intent-log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.usage <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.users <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.users.email <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.users.swift <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.users.uid <span class="number">16</span> <span class="number">16</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建密钥环和网关用户</span></span><br><span class="line">sudo ceph auth del client.radosgw.gateway</span><br><span class="line">sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.eu-east-<span class="number">1</span> --gen-key</span><br><span class="line">sudo ceph-authtool -n client.radosgw.eu-east-<span class="number">1</span> --cap osd <span class="string">'allow rwx'</span> --cap mon <span class="string">'allow rw'</span> /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.eu-east-<span class="number">1</span> -i /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置apache2</span></span><br><span class="line">cat &lt;&lt; EOF &gt; rgw.conf</span><br><span class="line">FastCgiExternalServer /var/www/s3gw.fcgi -host localhost:<span class="number">9000</span></span><br><span class="line"></span><br><span class="line">&lt;VirtualHost *:<span class="number">80</span>&gt;</span><br><span class="line"></span><br><span class="line">    ServerName localhost</span><br><span class="line">    ServerAlias *.localhost</span><br><span class="line">    ServerAdmin qinghua@ggg.com</span><br><span class="line">    DocumentRoot /var/www</span><br><span class="line">    RewriteEngine On</span><br><span class="line">    RewriteRule  ^/(.*) /s3gw.fcgi?%&#123;QUERY_STRING&#125; [E=HTTP_AUTHORIZATION:%&#123;HTTP:Authorization&#125;,L]</span><br><span class="line"></span><br><span class="line">    &lt;IfModule mod_fastcgi.c&gt;</span><br><span class="line">           &lt;Directory /var/www&gt;</span><br><span class="line">            Options +ExecCGI</span><br><span class="line">            AllowOverride All</span><br><span class="line">            SetHandler fastcgi-script</span><br><span class="line">            Order allow,deny</span><br><span class="line">            Allow from all</span><br><span class="line">            AuthBasicAuthoritative Off</span><br><span class="line">        &lt;/Directory&gt;</span><br><span class="line">    &lt;/IfModule&gt;</span><br><span class="line"></span><br><span class="line">    AllowEncodedSlashes On</span><br><span class="line">    ErrorLog /var/<span class="built_in">log</span>/apache2/error.log</span><br><span class="line">    CustomLog /var/<span class="built_in">log</span>/apache2/access.log combined</span><br><span class="line">    ServerSignature Off</span><br><span class="line"></span><br><span class="line">&lt;/VirtualHost&gt;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv rgw.conf /etc/apache2/conf-enabled/rgw.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置FastCGI</span></span><br><span class="line">cat &lt;&lt; EOF &gt; s3gw.fcgi</span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line"><span class="built_in">exec</span> /usr/bin/radosgw -c /etc/ceph/ceph.conf -n client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv s3gw.fcgi /var/www/s3gw.fcgi</span><br><span class="line">sudo chmod +x /var/www/s3gw.fcgi</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置ceph</span></span><br><span class="line">sudo sed -i <span class="string">'$a rgw region root pool = .eu.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zonegroup root pool = .eu.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a [client.radosgw.eu-east-1]'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw region = eu'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone = eu-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone root pool = .eu-east.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw dns name = eu-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a host = eu-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a keyring = /etc/ceph/ceph.client.radosgw.keyring'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw socket path = ""'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a log file = /var/log/radosgw/client.radosgw.eu-east-1.log'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw frontends = fastcgi socket_port=9000 socket_host=0.0.0.0'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw print continue = false'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure></p>
<p>接下来就是辖区和域的配置了。需要设置us的辖区和eu自己的辖区，否则会报错：<strong>AssertionError: No master zone found for region default</strong>。但是域只用设置eu自己的就好：<br><figure class="highlight sh"><figcaption><span>eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; us.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"us"</span>,</span><br><span class="line">  <span class="string">"api_name"</span>: <span class="string">"us"</span>,</span><br><span class="line">  <span class="string">"is_master"</span>: <span class="string">"true"</span>,</span><br><span class="line">  <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.15:80\/"</span>],</span><br><span class="line">  <span class="string">"master_zone"</span>: <span class="string">"us-east"</span>,</span><br><span class="line">  <span class="string">"zones"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"us-east"</span>,</span><br><span class="line">      <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.15:80\/"</span>],</span><br><span class="line">      <span class="string">"log_meta"</span>: <span class="string">"true"</span>,</span><br><span class="line">      <span class="string">"log_data"</span>: <span class="string">"true"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"us-west"</span>,</span><br><span class="line">      <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.16:80\/"</span>],</span><br><span class="line">      <span class="string">"log_meta"</span>: <span class="string">"true"</span>,</span><br><span class="line">      <span class="string">"log_data"</span>: <span class="string">"true"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"placement_targets"</span>: [</span><br><span class="line">   &#123;</span><br><span class="line">     <span class="string">"name"</span>: <span class="string">"default-placement"</span>,</span><br><span class="line">     <span class="string">"tags"</span>: []</span><br><span class="line">   &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"default_placement"</span>: <span class="string">"default-placement"</span></span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; eu.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"eu"</span>,</span><br><span class="line">  <span class="string">"api_name"</span>: <span class="string">"eu"</span>,</span><br><span class="line">  <span class="string">"is_master"</span>: <span class="string">"false"</span>,</span><br><span class="line">  <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.15:80\/"</span>],</span><br><span class="line">  <span class="string">"master_zone"</span>: <span class="string">"eu-east"</span>,</span><br><span class="line">  <span class="string">"zones"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"eu-east"</span>,</span><br><span class="line">      <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.17:80\/"</span>],</span><br><span class="line">      <span class="string">"log_meta"</span>: <span class="string">"true"</span>,</span><br><span class="line">      <span class="string">"log_data"</span>: <span class="string">"true"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"placement_targets"</span>: [</span><br><span class="line">   &#123;</span><br><span class="line">     <span class="string">"name"</span>: <span class="string">"default-placement"</span>,</span><br><span class="line">     <span class="string">"tags"</span>: []</span><br><span class="line">   &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"default_placement"</span>: <span class="string">"default-placement"</span></span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置辖区</span></span><br><span class="line">radosgw-admin region <span class="built_in">set</span> --infile us.json --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">radosgw-admin region <span class="built_in">set</span> --infile eu.json --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line">radosgw-admin region default --rgw-region=eu --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; eu-east.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"domain_root"</span>: <span class="string">".eu-east.domain.rgw"</span>,</span><br><span class="line">  <span class="string">"control_pool"</span>: <span class="string">".eu-east.rgw.control"</span>,</span><br><span class="line">  <span class="string">"gc_pool"</span>: <span class="string">".eu-east.rgw.gc"</span>,</span><br><span class="line">  <span class="string">"log_pool"</span>: <span class="string">".eu-east.log"</span>,</span><br><span class="line">  <span class="string">"intent_log_pool"</span>: <span class="string">".eu-east.intent-log"</span>,</span><br><span class="line">  <span class="string">"usage_log_pool"</span>: <span class="string">".eu-east.usage"</span>,</span><br><span class="line">  <span class="string">"user_keys_pool"</span>: <span class="string">".eu-east.users"</span>,</span><br><span class="line">  <span class="string">"user_email_pool"</span>: <span class="string">".eu-east.users.email"</span>,</span><br><span class="line">  <span class="string">"user_swift_pool"</span>: <span class="string">".eu-east.users.swift"</span>,</span><br><span class="line">  <span class="string">"user_uid_pool"</span>: <span class="string">".eu-east.users.uid"</span>,</span><br><span class="line">  <span class="string">"system_key"</span>: &#123;</span><br><span class="line">    <span class="string">"access_key"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"secret_key"</span>: <span class="string">""</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"placement_pools"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"key"</span>: <span class="string">"default-placement"</span>,</span><br><span class="line">      <span class="string">"val"</span>: &#123;</span><br><span class="line">        <span class="string">"index_pool"</span>: <span class="string">".eu-east.rgw.buckets.index"</span>,</span><br><span class="line">        <span class="string">"data_pool"</span>: <span class="string">".eu-east.rgw.buckets"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置域</span></span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=eu-east --infile eu-east.json --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建用户</span></span><br><span class="line">radosgw-admin user create --uid=<span class="string">"eu-east"</span> --display-name=<span class="string">"Region-EU Zone-East"</span> --name client.radosgw.eu-east-<span class="number">1</span> --system | tee eastuser.txt</span><br><span class="line"><span class="built_in">export</span> SRC_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> eastuser.txt`</span><br><span class="line"><span class="built_in">export</span> SRC_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> eastuser.txt`</span><br><span class="line">sed -i <span class="string">"s/access_key\": \"/access_key\": \"<span class="variable">$SRC_ACCESS_KEY</span>/g"</span> eu-east.json</span><br><span class="line">sed -i <span class="string">"s/secret_key\": \"/secret_key\": \"<span class="variable">$SRC_SECRET_KEY</span>/g"</span> eu-east.json</span><br><span class="line"></span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=eu-east --infile eu-east.json --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>都完成了以后，就可以重启ceph服务和apache2啦：<br><figure class="highlight sh"><figcaption><span>eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker restart ceph</span><br><span class="line">sudo /etc/init.d/radosgw start</span><br><span class="line">sudo a2enmod rewrite</span><br><span class="line">sudo service apache2 restart</span><br></pre></td></tr></table></figure></p>
<p>最后同步元数据：<br><figure class="highlight sh"><figcaption><span>eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">scp vagrant@<span class="number">192.168</span>.<span class="number">33.15</span>:/home/vagrant/us-east.json .    <span class="comment"># vagrant的密码也是vagrant</span></span><br><span class="line"></span><br><span class="line">docker cp ceph:eu-east.json eu-east.json</span><br><span class="line"><span class="built_in">export</span> SRC_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> us-east.json`</span><br><span class="line"><span class="built_in">export</span> SRC_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> us-east.json`</span><br><span class="line"><span class="built_in">export</span> DEST_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> eu-east.json`</span><br><span class="line"><span class="built_in">export</span> DEST_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> eu-east.json`</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; cluster-data-sync.conf</span><br><span class="line">src_zone: us-east</span><br><span class="line"><span class="built_in">source</span>: http://<span class="number">192.168</span>.<span class="number">33.15</span></span><br><span class="line">src_access_key: <span class="variable">$SRC_ACCESS_KEY</span></span><br><span class="line">src_secret_key: <span class="variable">$SRC_SECRET_KEY</span></span><br><span class="line">dest_zone: eu-east</span><br><span class="line">destination: http://<span class="number">192.168</span>.<span class="number">33.17</span></span><br><span class="line">dest_access_key: <span class="variable">$DEST_ACCESS_KEY</span></span><br><span class="line">dest_secret_key: <span class="variable">$DEST_SECRET_KEY</span></span><br><span class="line"><span class="built_in">log</span>_file: /var/<span class="built_in">log</span>/radosgw/radosgw-sync-eu-east-west.log</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo radosgw-agent -c cluster-data-sync.conf --metadata-only</span><br></pre></td></tr></table></figure></p>
<p>如果不加<code>--metadata-only</code>，则会报错：<strong>ERROR:root:data sync can only occur between zones in the same region</strong>。同步完成后，我们运行以下命令查看现在<code>eu-east-1</code>实例里的存储桶：<br><figure class="highlight sh"><figcaption><span>eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin metadata bucket list --name client.radosgw.eu-east-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>应该能够看到先前在<code>us-east-1</code>创建的<code>my-new-bucket</code>。下面是一张不同辖区同步示意图：<br><img src="http://docs.ceph.com/docs/master/_images/region-sync.png" alt=""></p>
<h2 id="u5E38_u89C1_u547D_u4EE4"><a href="#u5E38_u89C1_u547D_u4EE4" class="headerlink" title="常见命令"></a>常见命令</h2><p>再介绍一些ceph的常见命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rados lspools                                                <span class="comment"># 列出所有的存储池</span></span><br><span class="line">rados ls -p .rgw.root                                        <span class="comment"># 列出.rgw.root存储池的所有对象</span></span><br><span class="line">rados get zone_info.default obj.txt -p .rgw.root             <span class="comment"># 将.rgw.root存储池的zone_info.default对象内容保存到obj.txt文件</span></span><br><span class="line">rados rm region_info.default -p .us.rgw.root                 <span class="comment"># 删除.us.rgw.root存储池的region_info.default对象</span></span><br><span class="line">radosgw-admin region list --name client.radosgw.us-east-<span class="number">1</span>    <span class="comment"># 列出client.radosgw.us-east-1实例的所有辖区</span></span><br><span class="line">radosgw-admin region get --name client.radosgw.us-east-<span class="number">1</span>     <span class="comment"># 查看client.radosgw.us-east-1实例的主辖区</span></span><br><span class="line">radosgw-admin zone list --name client.radosgw.us-east-<span class="number">1</span>      <span class="comment"># 列出client.radosgw.us-east-1实例的所有域</span></span><br><span class="line">radosgw-admin zone get --name client.radosgw.us-east-<span class="number">1</span>       <span class="comment"># 查看client.radosgw.us-east-1实例的主域</span></span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://ceph.com/">Ceph</a>是一个高性能的PB级分布式文件系统。它能够在一个系统中提供对象存储、块存储和文件存储。本文的主要内容是如何<a href="http://docs.ceph.com/docs/master/radosgw/federated-config/">通过RADOSGW备份一个可用区</a>。</p>
<ul>
<li>对如何加载使用块存储和文件存储感兴趣可以参考<a href="/ceph-demo">《用容器轻松搭建ceph实验环境》</a></li>
<li>对RADOSGW如何暴露S3和Swift接口感兴趣可以参考<a href="/ceph-radosgw">《通过RADOSGW提供ceph的S3和Swift接口》</a>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://qinghua.github.io/tags/ceph/"/>
    
      <category term="radosgw" scheme="http://qinghua.github.io/tags/radosgw/"/>
    
      <category term="storage" scheme="http://qinghua.github.io/tags/storage/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[光学字符识别软件tesseract-ocr]]></title>
    <link href="http://qinghua.github.io/tesseract/"/>
    <id>http://qinghua.github.io/tesseract/</id>
    <published>2016-04-16T02:13:20.000Z</published>
    <updated>2016-04-17T10:09:45.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://github.com/tesseract-ocr/tesseract/wiki" target="_blank" rel="external">Tesseract</a>是一个可以将图片转换成文字的<a href="https://zh.wikipedia.org/wiki/%E5%85%89%E5%AD%A6%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB" target="_blank" rel="external">OCR</a>（Optical Character Recognition）软件，支持包括中文简繁体的<a href="https://github.com/tesseract-ocr/tesseract/blob/master/doc/tesseract.1.asc#languages" target="_blank" rel="external">多种语言</a>，简单易用，可以用来识别验证码。让我们来看一看吧。<br><a id="more"></a></p>
<h2 id="u5B89_u88C5"><a href="#u5B89_u88C5" class="headerlink" title="安装"></a>安装</h2><p>Tesseract只是一个小应用程序，在mac里直接安装就好啦：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install tesseract</span><br></pre></td></tr></table></figure></p>
<p>下面是张包含了一些英文的图片：<br><img src="/img/test.png" alt=""></p>
<p>把图片保存到本地之后，使用以下命令将其转成文字：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tesseract ~/Downloads/test.png out</span><br><span class="line">cat out.txt</span><br></pre></td></tr></table></figure></p>
<p>可以看到，对于正常字体来说，粗体、斜体、大小字号等的识别率还是很不错的。如果是手写体的字体，识别率将会严重下降。</p>
<h2 id="u4E2D_u6587"><a href="#u4E2D_u6587" class="headerlink" title="中文"></a>中文</h2><p>Tesseract支持多种语言，不过除了英语以外，都必须先下载语言数据：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget -c https://github.com/tesseract-ocr/tessdata/blob/master/chi_sim.traineddata?raw=<span class="literal">true</span></span><br><span class="line">mv chi_sim.traineddata\?raw\=<span class="literal">true</span> /usr/<span class="built_in">local</span>/Cellar/tesseract/<span class="number">3.04</span>.<span class="number">00</span>/share/tessdata/chi_sim.traineddata</span><br></pre></td></tr></table></figure></p>
<p>从后缀名traineddata可以看出来，tesseract是可以通过训练来提高识别率的。网上有许多教程，有兴趣的朋友可以自行尝试。下面是张包含了一些中文的图片：<br><img src="/img/testcn.png" alt=""></p>
<p>把图片保存到本地之后，使用以下命令将其转成文字：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tesseract ~/Downloads/testcn.png out <span class="operator">-l</span> chi_sim</span><br><span class="line">cat out.txt</span><br><span class="line">tesseract ~/Downloads/testcn.png out <span class="operator">-l</span> eng+chi_sim</span><br><span class="line">cat out.txt</span><br></pre></td></tr></table></figure></p>
<p>毕竟汉字内容多，这回没有英文识别率那么高了，想要更加实用可能需要更多训练和校对。</p>
<h2 id="PDF"><a href="#PDF" class="headerlink" title="PDF"></a>PDF</h2><h3 id="u8F93_u51FA"><a href="#u8F93_u51FA" class="headerlink" title="输出"></a>输出</h3><p>很简单，在<code>tesseract</code>命令的最后面加上<code>pdf</code>就好了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tesseract ~/Downloads/test.png out pdf</span><br><span class="line">open out.pdf</span><br></pre></td></tr></table></figure></p>
<h3 id="u8F93_u5165"><a href="#u8F93_u5165" class="headerlink" title="输入"></a>输入</h3><p>虽然tesseract不能直接处理PDF，但是借助<a href="https://www.imagemagick.org/script/index.php" target="_blank" rel="external">ImageMagick</a>和<a href="http://www.ghostscript.com/" target="_blank" rel="external">Ghostscript</a>可以轻松地把PDF转换成图片文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brew install imagemagick</span><br><span class="line">brew install ghostscript</span><br><span class="line">convert -density <span class="number">100</span> -trim input.pdf output%<span class="number">04</span>d.jpg</span><br></pre></td></tr></table></figure></p>
<p>这里的100表示DPI，<code>%04d</code>表示分页储存。有了图片之后就可以用tesseract随意操作啦。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="https://github.com/tesseract-ocr/tesseract/wiki">Tesseract</a>是一个可以将图片转换成文字的<a href="https://zh.wikipedia.org/wiki/%E5%85%89%E5%AD%A6%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB">OCR</a>（Optical Character Recognition）软件，支持包括中文简繁体的<a href="https://github.com/tesseract-ocr/tesseract/blob/master/doc/tesseract.1.asc#languages">多种语言</a>，简单易用，可以用来识别验证码。让我们来看一看吧。<br>]]>
    
    </summary>
    
      <category term="OCR" scheme="http://qinghua.github.io/tags/OCR/"/>
    
      <category term="tesseract" scheme="http://qinghua.github.io/tags/tesseract/"/>
    
      <category term="tool" scheme="http://qinghua.github.io/categories/tool/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[轻松搭建OpenStack Swift存储测试环境]]></title>
    <link href="http://qinghua.github.io/openstack-swift/"/>
    <id>http://qinghua.github.io/openstack-swift/</id>
    <published>2016-04-15T11:22:14.000Z</published>
    <updated>2016-04-20T00:52:32.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://docs.openstack.org/developer/swift/" target="_blank" rel="external">Swift</a>（OpenStack Object Storage）是Rackspace开发的高可用分布式对象存储，贡献给了<a href="http://www.openstack.org/" target="_blank" rel="external">OpenStack</a>。上次在<a href="/ceph-radosgw">《通过RADOSGW提供ceph的S3和Swift接口》</a>一文里介绍了ceph RADOSGW的Swift接口，这次让我们直接来试试原生的swift吧！<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配一台IP为<strong>192.168.33.17</strong>，内存为1G的虚拟机。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line">config.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">  v.memory = <span class="number">1024</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后终端运行以下命令启动并连接虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>这里参考了Swift的官方文档<a href="http://docs.openstack.org/developer/swift/development_saio.html" target="_blank" rel="external">Swift All In One</a>来搭建一个swift测试环境。首先需要安装各种依赖包：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y install curl gcc memcached rsync sqlite3 xfsprogs \</span><br><span class="line">                        git-core libffi-dev python-setuptools \</span><br><span class="line">                        liberasurecode-dev</span><br><span class="line">sudo apt-get -y install python-coverage python-dev python-nose \</span><br><span class="line">                        python-xattr python-eventlet \</span><br><span class="line">                        python-greenlet python-pastedeploy \</span><br><span class="line">                        python-netifaces python-pip python-dnspython \</span><br><span class="line">                        python-mock</span><br><span class="line">sudo pip install --upgrade pip</span><br></pre></td></tr></table></figure></p>
<p>这里就不用<code>fdisk</code>而使用较简单的环回设备来当做我们的存储：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /srv</span><br><span class="line">sudo truncate <span class="operator">-s</span> <span class="number">1</span>GB /srv/swift-disk</span><br><span class="line">sudo mkfs.xfs /srv/swift-disk</span><br><span class="line">sudo sh -c <span class="string">'echo "/srv/swift-disk /mnt/sdb1 xfs loop,noatime,nodiratime,nobarrier,logbufs=8 0 0" &gt;&gt; /etc/fstab'</span></span><br></pre></td></tr></table></figure></p>
<p>生成挂载点和link：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /mnt/sdb1</span><br><span class="line">sudo mount /mnt/sdb1</span><br><span class="line">sudo mkdir /mnt/sdb1/<span class="number">1</span> /mnt/sdb1/<span class="number">2</span> /mnt/sdb1/<span class="number">3</span> /mnt/sdb1/<span class="number">4</span></span><br><span class="line">sudo chown vagrant:vagrant /mnt/sdb1/*</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> &#123;<span class="number">1</span>..<span class="number">4</span>&#125;; <span class="keyword">do</span> sudo ln <span class="operator">-s</span> /mnt/sdb1/<span class="variable">$x</span> /srv/<span class="variable">$x</span>; <span class="keyword">done</span></span><br><span class="line">sudo mkdir -p /srv/<span class="number">1</span>/node/sdb1 /srv/<span class="number">1</span>/node/sdb5 \</span><br><span class="line">              /srv/<span class="number">2</span>/node/sdb2 /srv/<span class="number">2</span>/node/sdb6 \</span><br><span class="line">              /srv/<span class="number">3</span>/node/sdb3 /srv/<span class="number">3</span>/node/sdb7 \</span><br><span class="line">              /srv/<span class="number">4</span>/node/sdb4 /srv/<span class="number">4</span>/node/sdb8 \</span><br><span class="line">              /var/run/swift</span><br><span class="line">sudo chown -R vagrant:vagrant /var/run/swift</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> &#123;<span class="number">1</span>..<span class="number">4</span>&#125;; <span class="keyword">do</span> sudo chown -R vagrant:vagrant /srv/<span class="variable">$x</span>/; <span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>在<code>rc.local</code>里增加几条创建文件夹和授权的命令，使之能够被开机执行：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'$i mkdir -p /var/cache/swift /var/cache/swift2 /var/cache/swift3 /var/cache/swift4'</span> /etc/rc.local</span><br><span class="line">sudo sed -i <span class="string">'$i chown vagrant:vagrant /var/cache/swift*'</span> /etc/rc.local</span><br><span class="line">sudo sed -i <span class="string">'$i mkdir -p /var/run/swift'</span> /etc/rc.local</span><br><span class="line">sudo sed -i <span class="string">'$i chown vagrant:vagrant /var/run/swift'</span> /etc/rc.local</span><br></pre></td></tr></table></figure></p>
<p>接下来需要安装swift和它的客户端：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>; git <span class="built_in">clone</span> https://github.com/openstack/python-swiftclient.git</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/python-swiftclient; git checkout <span class="number">2.7</span>.<span class="number">0</span>; sudo python setup.py develop; <span class="built_in">cd</span> -</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openstack/swift.git</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/swift; git checkout <span class="number">2.7</span>.<span class="number">0</span></span><br><span class="line">sed -i <span class="string">"s/;python_version&lt;'3.0'//"</span> requirements.txt</span><br><span class="line">sed -i <span class="string">"/dnspython3&gt;=1.12.0;python_version&gt;='3.0'/d"</span> requirements.txt</span><br><span class="line">sudo pip install -r requirements.txt; sudo python setup.py develop; sudo pip install -r <span class="built_in">test</span>-requirements.txt</span><br></pre></td></tr></table></figure></p>
<p>然后需要配置rsync：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp <span class="variable">$HOME</span>/swift/doc/saio/rsyncd.conf /etc/</span><br><span class="line">sudo sed -i <span class="string">"s/&lt;your-user-name&gt;/vagrant/"</span> /etc/rsyncd.conf</span><br><span class="line">sudo sed -i <span class="string">"s/RSYNC_ENABLE=false/RSYNC_ENABLE=true/"</span> /etc/default/rsync</span><br><span class="line">sudo service rsync restart</span><br></pre></td></tr></table></figure></p>
<p>使用以下命令来验证rsync，应该能看到一堆的account、container和object：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync rsync://pub@localhost/</span><br></pre></td></tr></table></figure></p>
<p>我们前面已经安装了memcached，验证一下服务是可用的：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service memcached status</span><br></pre></td></tr></table></figure></p>
<p>接下来需要配置各个节点：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/swift/doc; sudo cp -r saio/swift /etc/swift; <span class="built_in">cd</span> -</span><br><span class="line">sudo chown -R <span class="variable">$&#123;USER&#125;</span>:<span class="variable">$&#123;USER&#125;</span> /etc/swift</span><br><span class="line">find /etc/swift/ -name \*.conf | xargs sudo sed -i <span class="string">"s/&lt;your-user-name&gt;/<span class="variable">$&#123;USER&#125;</span>/"</span></span><br></pre></td></tr></table></figure></p>
<p>然后配置swift脚本，<code>/etc/swift/test.conf</code>为我们添加了三个测试账户：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="variable">$HOME</span>/bin</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/swift/doc; cp saio/bin/* <span class="variable">$HOME</span>/bin; <span class="built_in">cd</span> -</span><br><span class="line">chmod +x <span class="variable">$HOME</span>/bin/*</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"export SAIO_BLOCK_DEVICE=/srv/swift-disk"</span> &gt;&gt; <span class="variable">$HOME</span>/.bashrc</span><br><span class="line">sed -i <span class="string">"/^find/d"</span> <span class="variable">$HOME</span>/bin/resetswift</span><br><span class="line">cp <span class="variable">$HOME</span>/swift/<span class="built_in">test</span>/sample.conf /etc/swift/test.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"export SWIFT_TEST_CONFIG_FILE=/etc/swift/test.conf"</span> &gt;&gt; <span class="variable">$HOME</span>/.bashrc</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"export PATH=<span class="variable">$&#123;PATH&#125;</span>:<span class="variable">$HOME</span>/bin"</span> &gt;&gt; <span class="variable">$HOME</span>/.bashrc</span><br><span class="line">. <span class="variable">$HOME</span>/.bashrc</span><br></pre></td></tr></table></figure></p>
<p>Swift里有一个非常重要的概念，<a href="http://docs.openstack.org/developer/swift/overview_ring.html?highlight=ring" target="_blank" rel="external">ring</a>。通过它可以找到数据的物理位置。它的存储模型是这样的：一个账号（account）里可以有多个容器（container），容器里可以有许多个键值对，字典里的值称为对象（object）。账号和容器被存储在SQLite数据库里，而对象是以文件方式存储的。账号数据库、容器数据库和每个单独对象都有自己的ring。下面我们来构建一些ring：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">remakerings</span><br></pre></td></tr></table></figure></p>
<h2 id="u542F_u52A8_u73AF_u5883"><a href="#u542F_u52A8_u73AF_u5883" class="headerlink" title="启动环境"></a>启动环境</h2><p>现在我们就可以用<code>startmain</code>启动swift啦：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'s/bind_ip = 127.0.0.1/bind_ip = 192.168.33.17/'</span> /etc/swift/proxy-server.conf</span><br><span class="line">startmain</span><br></pre></td></tr></table></figure></p>
<p>然后用<code>test:tester/testing</code>这个预先创建好的测试账户登录：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -i -H <span class="string">'X-Storage-User: test:tester'</span> -H <span class="string">'X-Storage-Pass: testing'</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span>/auth/v1.<span class="number">0</span> | tee auth.txt</span><br><span class="line">sudo apt-get -y install dos2unix</span><br><span class="line">dos2unix auth.txt</span><br><span class="line"><span class="built_in">export</span> X_AUTH_TOKEN=`cat auth.txt | sed -n <span class="string">'s/X-Auth-Token: \(.*\)/\1/p'</span>`</span><br><span class="line"><span class="built_in">export</span> X_STORAGE_URL=`cat auth.txt | sed -n <span class="string">'s/X-Storage-Url: \(.*\)/\1/p'</span>`</span><br><span class="line">curl -v -H <span class="string">"X-Auth-Token: <span class="variable">$X_AUTH_TOKEN</span>"</span> <span class="variable">$X_STORAGE_URL</span></span><br></pre></td></tr></table></figure></p>
<p>上面用到<code>dos2unix</code>是因为取到的<code>X-Storage-Url</code>最后面带着<code>^M$</code>的特殊字符。直接管道的话，下一个<code>curl</code>会报错：<code>Illegal characters found in URL</code>。可以保存成文件之后使用<code>cat -A</code>来查看这些特殊字符。<br>登录完成后，就能看到swift的状态和所有容器啦：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span>/auth/v1.<span class="number">0</span> -U <span class="built_in">test</span>:tester -K testing <span class="built_in">stat</span></span><br><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span>/auth/v1.<span class="number">0</span> -U <span class="built_in">test</span>:tester -K testing list</span><br></pre></td></tr></table></figure></p>
<h2 id="u6D4B_u8BD5_u73AF_u5883"><a href="#u6D4B_u8BD5_u73AF_u5883" class="headerlink" title="测试环境"></a>测试环境</h2><h3 id="Swift_u6D4B_u8BD5"><a href="#Swift_u6D4B_u8BD5" class="headerlink" title="Swift测试"></a>Swift测试</h3><p>有兴趣的话，还可以运行下面的单元测试、功能测试和探索性测试：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HOME</span>/swift/.unittests</span><br><span class="line"><span class="variable">$HOME</span>/swift/.functests</span><br><span class="line"><span class="variable">$HOME</span>/swift/.probetests</span><br></pre></td></tr></table></figure></p>
<h3 id="Docker_Registry_u6D4B_u8BD5"><a href="#Docker_Registry_u6D4B_u8BD5" class="headerlink" title="Docker Registry测试"></a>Docker Registry测试</h3><p>在Docker Registry的<code>config.yml</code>里使用以下配置：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">storage:&#10;  swift:&#10;    username: test:tester &#10;    password: testing &#10;    authurl: http://192.168.33.17:8080/auth/v1.0&#10;    container: swift</span><br></pre></td></tr></table></figure></p>
<p>可以测试<code>docker push</code>啦。不过记得需要先登录用户哦。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://docs.openstack.org/developer/swift/">Swift</a>（OpenStack Object Storage）是Rackspace开发的高可用分布式对象存储，贡献给了<a href="http://www.openstack.org/">OpenStack</a>。上次在<a href="/ceph-radosgw">《通过RADOSGW提供ceph的S3和Swift接口》</a>一文里介绍了ceph RADOSGW的Swift接口，这次让我们直接来试试原生的swift吧！<br>]]>
    
    </summary>
    
      <category term="openstack swift" scheme="http://qinghua.github.io/tags/openstack-swift/"/>
    
      <category term="storage" scheme="http://qinghua.github.io/tags/storage/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用RADOSGW提供ceph的S3和Swift接口]]></title>
    <link href="http://qinghua.github.io/ceph-radosgw/"/>
    <id>http://qinghua.github.io/ceph-radosgw/</id>
    <published>2016-04-12T00:07:13.000Z</published>
    <updated>2016-04-21T09:30:54.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://ceph.com/" target="_blank" rel="external">Ceph</a>是一个高性能的PB级分布式文件系统。它能够在一个系统中提供对象存储、块存储和文件存储。对如何加载使用这些存储感兴趣的话可以参考<a href="/ceph-demo">《用容器轻松搭建ceph实验环境》</a>。它还可以通过RADOSGW来实现S3和OpenStack Swift存储接口。不管RADOSGW还是块存储或文件存储都是基于对象存储来提供服务。本文的主要内容是如何通过RADOSGW来暴露S3和SWIFT接口。由于Docker Registry在2.4版本<a href="https://github.com/docker/distribution/commit/5967d333425a8dd5d36c5bb456098839654d38af" target="_blank" rel="external">移除了对rados的支持</a>，所以如果使用ceph作为后端存储就需要利用RADOSGW了。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配一台IP为<strong>192.168.33.111</strong>，内存为1G的虚拟机。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.111"</span></span><br><span class="line">config.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">  v.memory = <span class="number">1024</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后终端运行以下命令启动并连接虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>首先需要安装一些ceph、radosgw的依赖包，还有python-boto、swift客户端等工具可以用于测试。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y --force-yes install ceph-common radosgw python-boto</span><br><span class="line">sudo pip install --upgrade setuptools</span><br><span class="line">sudo pip install --upgrade python-swiftclient</span><br></pre></td></tr></table></figure></p>
<p>然后就可以启动ceph/demo这个容器来轻松提供ceph服务了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=host -v /etc/ceph:/etc/ceph <span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">33.111</span> <span class="operator">-e</span> CEPH_NETWORK=<span class="number">192.168</span>.<span class="number">33.111</span>/<span class="number">24</span> --name=ceph ceph/demo</span><br></pre></td></tr></table></figure></p>
<p>接下来的步骤主要参考的是有一点坑的<a href="http://docs.ceph.com/docs/master/radosgw/config/" target="_blank" rel="external">官方教程</a>。需要为radosgw生成一个名为<code>gateway</code>的用户：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph auth del client.radosgw.gateway</span><br><span class="line">sudo ceph auth get-or-create client.radosgw.gateway osd <span class="string">'allow rwx'</span> mon <span class="string">'allow rwx'</span> -o /etc/ceph/ceph.client.radosgw.keyring</span><br></pre></td></tr></table></figure></p>
<p>然后需要把这个用户加到<code>ceph.conf</code>配置里，提供端口为9000的<a href="https://en.wikipedia.org/wiki/FastCGI" target="_blank" rel="external">FastCGI</a>服务：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a [client.radosgw.gateway]'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a host = vagrant-ubuntu-trusty-64'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a keyring = /etc/ceph/ceph.client.radosgw.keyring'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw socket path = ""'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a log file = /var/log/radosgw/client.radosgw.gateway.log'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw frontends = fastcgi socket_port=9000 socket_host=0.0.0.0'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw print continue = false'</span> /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure></p>
<p>其中第二行的<code>vagrant-ubuntu-trusty-64</code>，必须使用<code>hostname -s</code>得出的结果。如果是按照<a href="/ceph-radosgw/#u51C6_u5907_u5DE5_u4F5C">准备工作</a>的做法，是不需要变的。另外<a href="https://segmentfault.com/q/1010000000256516" target="_blank" rel="external">这里</a>的第一个回答非常清晰地解释了CGI和FastCGI。<br>配置完成后就可以重启ceph容器并启动radosgw：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker restart ceph</span><br><span class="line">sudo /etc/init.d/radosgw start</span><br></pre></td></tr></table></figure></p>
<p>为了提供HTTP服务，需要安装apache2（Red Hat系是httpd）：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get -y --force-yes install apache2</span><br></pre></td></tr></table></figure></p>
<p>接下来创建一个apache2的配置文件，监听80端口并把请求转发到radosgw提供的FastCGI 9000端口上：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; rgw.conf</span><br><span class="line">&lt;VirtualHost *:<span class="number">80</span>&gt;</span><br><span class="line">ServerName localhost</span><br><span class="line">DocumentRoot /var/www/html</span><br><span class="line"></span><br><span class="line">ErrorLog /var/<span class="built_in">log</span>/apache2/rgw_error.log</span><br><span class="line">CustomLog /var/<span class="built_in">log</span>/apache2/rgw_access.log combined</span><br><span class="line"></span><br><span class="line"><span class="comment"># LogLevel debug</span></span><br><span class="line"></span><br><span class="line">RewriteEngine On</span><br><span class="line"></span><br><span class="line">RewriteRule .* - [E=HTTP_AUTHORIZATION:%&#123;HTTP:Authorization&#125;,L]</span><br><span class="line"></span><br><span class="line">SetEnv proxy-nokeepalive <span class="number">1</span></span><br><span class="line"></span><br><span class="line">ProxyPass / fcgi://localhost:<span class="number">9000</span>/</span><br><span class="line"></span><br><span class="line">&lt;/VirtualHost&gt;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv rgw.conf /etc/apache2/conf-enabled/rgw.conf</span><br></pre></td></tr></table></figure></p>
<p>由于上述配置需要用到一些apache2默认未加载的模块，所以需要加载并重新启动apache2：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo a2enmod rewrite</span><br><span class="line">sudo a2enmod proxy_http</span><br><span class="line">sudo a2enmod proxy_fcgi</span><br><span class="line">sudo service apache2 restart</span><br></pre></td></tr></table></figure></p>
<h2 id="u6D4B_u8BD5_u670D_u52A1"><a href="#u6D4B_u8BD5_u670D_u52A1" class="headerlink" title="测试服务"></a>测试服务</h2><h3 id="S3"><a href="#S3" class="headerlink" title="S3"></a>S3</h3><p>RADOSGW的基本配置已经完成，现在我们测试一下s3接口。它的存储模型是这样的：用户可以创建和管理多个<a href="http://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/UsingBucket.html" target="_blank" rel="external">存储桶（bucket）</a>，每个存储桶里可以存放无限多个对象（object），每个对象是一个键值对。存储桶的名称与区域无关，全球唯一。</p>
<p>接下来先创建一个s3用户：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin user create --uid=<span class="string">"testuser"</span> --display-name=<span class="string">"First User"</span> | tee user.txt</span><br><span class="line"><span class="built_in">export</span> ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> user.txt`</span><br><span class="line"><span class="built_in">export</span> SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> user.txt`</span><br></pre></td></tr></table></figure></p>
<p>使用以下python代码来测试我们的s3接口是否已经可用：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; s3test.py</span><br><span class="line">import boto</span><br><span class="line">import boto.s3.connection</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">access_key = os.environ[<span class="string">"ACCESS_KEY"</span>]</span><br><span class="line">secret_key = os.environ[<span class="string">"SECRET_KEY"</span>]</span><br><span class="line">conn = boto.connect_s3(</span><br><span class="line">aws_access_key_id = access_key,</span><br><span class="line">aws_secret_access_key = secret_key,</span><br><span class="line">host = <span class="string">'192.168.33.111'</span>,</span><br><span class="line">is_secure=False,</span><br><span class="line">calling_format = boto.s3.connection.OrdinaryCallingFormat(),</span><br><span class="line">)</span><br><span class="line">bucket = conn.create_bucket(<span class="string">'my-new-bucket'</span>)</span><br><span class="line"><span class="keyword">for</span> bucket <span class="keyword">in</span> conn.get_all_buckets():</span><br><span class="line">    <span class="built_in">print</span> <span class="string">"&#123;name&#125;\t&#123;created&#125;"</span>.format(</span><br><span class="line">        name = bucket.name,</span><br><span class="line">        created = bucket.creation_date,</span><br><span class="line">)</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">python s3test.py</span><br></pre></td></tr></table></figure></p>
<p>如果显示了<code>my-new-bucket</code>，那就说明测试成功地通过s3接口创建了一个存储桶。可以使用以下命令来获取这个存储桶和实例的信息：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin metadata bucket list</span><br><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin metadata get bucket:my-new-bucket  | tee bucket.txt</span><br><span class="line"><span class="built_in">export</span> BUCKET_ID=`cat bucket.txt | sed -n <span class="string">'s/ *"bucket_id": "\(.*\)"/\1/p'</span>`</span><br><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin metadata get bucket.instance:my-new-bucket:<span class="variable">$BUCKET_ID</span></span><br></pre></td></tr></table></figure></p>
<p>还可以修改实例的信息并PUT回去，具体做法可参见<a href="http://blog.widodh.nl/2013/11/changing-the-region-of-a-rgw-bucket/" target="_blank" rel="external">《Changing the region of a RGW bucket》</a>。</p>
<h3 id="Swift"><a href="#Swift" class="headerlink" title="Swift"></a>Swift</h3><p>接下来测试swift。对于swift来说，它的存储模型是这样的：一个账号（account）里可以有多个容器（container），容器里可以有许多个键值对，字典里的值称为对象（object）。账号和容器被存储在SQLite数据库里，而对象是以文件方式存储的。</p>
<p>首先需要创建swift用户并生成secret：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin subuser create --uid=testuser --subuser=testuser:swift --access=full</span><br><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin key create --subuser=testuser:swift --key-type=swift --gen-secret | tee subuser.txt</span><br><span class="line"><span class="built_in">export</span> PASSWORD=`sed -n <span class="string">'/testuser:swift/&#123;N;p;&#125;'</span> subuser.txt | sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span>`</span><br></pre></td></tr></table></figure></p>
<p>然后就可以用以下命令查看swift里所有的容器：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> list</span><br></pre></td></tr></table></figure></p>
<p>应该能看到刚才测试s3接口时创建的<code>my-new-bucket</code>，在这里s3的存储桶和swift的容器是同一个概念。接下来我们自己创建容器：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> post qinghua</span><br><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> list qinghua</span><br></pre></td></tr></table></figure></p>
<p>创建成功，里面没有文件。现在可以上传、下载文件试试：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World &gt; hw.txt</span><br><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> upload qinghua hw.txt</span><br><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> list qinghua</span><br><span class="line">mv hw.txt hw.bak</span><br><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> download qinghua hw.txt</span><br><span class="line">cat hw.txt</span><br></pre></td></tr></table></figure></p>
<p>搞定！</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://ceph.com/">Ceph</a>是一个高性能的PB级分布式文件系统。它能够在一个系统中提供对象存储、块存储和文件存储。对如何加载使用这些存储感兴趣的话可以参考<a href="/ceph-demo">《用容器轻松搭建ceph实验环境》</a>。它还可以通过RADOSGW来实现S3和OpenStack Swift存储接口。不管RADOSGW还是块存储或文件存储都是基于对象存储来提供服务。本文的主要内容是如何通过RADOSGW来暴露S3和SWIFT接口。由于Docker Registry在2.4版本<a href="https://github.com/docker/distribution/commit/5967d333425a8dd5d36c5bb456098839654d38af">移除了对rados的支持</a>，所以如果使用ceph作为后端存储就需要利用RADOSGW了。<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://qinghua.github.io/tags/ceph/"/>
    
      <category term="openstack swift" scheme="http://qinghua.github.io/tags/openstack-swift/"/>
    
      <category term="radosgw" scheme="http://qinghua.github.io/tags/radosgw/"/>
    
      <category term="s3" scheme="http://qinghua.github.io/tags/s3/"/>
    
      <category term="storage" scheme="http://qinghua.github.io/tags/storage/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[用容器轻松搭建Rancher运行环境]]></title>
    <link href="http://qinghua.github.io/rancher/"/>
    <id>http://qinghua.github.io/rancher/</id>
    <published>2016-04-09T03:31:01.000Z</published>
    <updated>2016-04-09T07:48:31.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://rancher.com/" target="_blank" rel="external">Rancher</a>是开源的容器平台，功能齐全，部署简单，支持Kubernets和Docker Swarm。它把自己定位在持续交付流水线上的后半段上，如下图所示:<br><img src="/img/rancher-feature.png" alt=""></p>
<p>2016年3月底刚刚发布了1.0正式版。借着这个契机，下面就让我们用容器来部署一套Rancher环境试试它的功能吧！<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配三台虚拟机，一台叫做<strong>server</strong>，它的IP是<strong>192.168.33.17</strong>；另两台分别是<strong>agent1</strong>和<strong>agent2</strong>，它们的IP是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"server"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"server"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">1024</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"agent1"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"agent1"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">1024</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"agent2"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"agent2"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">1024</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后分别在三个终端运行以下命令启动并连接三台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh server</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh agent1</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh agent2</span><br></pre></td></tr></table></figure>
<p>如果想要在接下来的步骤中获得良好体验，建议先下载以下镜像：<br><figure class="highlight sh"><figcaption><span>server agent1 agent2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker pull rancher/server:v1.<span class="number">0.0</span></span><br><span class="line">docker pull rancher/agent:v0.<span class="number">11.0</span></span><br><span class="line">docker pull rancher/agent-instance:v0.<span class="number">8.1</span></span><br><span class="line">docker pull tomcat:<span class="number">8.0</span>.<span class="number">30</span>-jre8</span><br><span class="line">docker pull busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br><span class="line">docker pull mysql:<span class="number">5.7</span>.<span class="number">10</span></span><br><span class="line">docker pull wordpress:<span class="number">4.4</span>.<span class="number">2</span></span><br><span class="line">docker pull rancher/etcd:v2.<span class="number">3.0</span></span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>启动Rancher服务器相当简单，一条命令而已：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --name=rs \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -p <span class="number">8080</span>:<span class="number">8080</span> \</span><br><span class="line">    rancher/server:v1.<span class="number">0.0</span></span><br></pre></td></tr></table></figure></p>
<p>稍待片刻，就可以访问Rancher主页<a href="http://192.168.33.17:8080" target="_blank" rel="external">http://192.168.33.17:8080</a>了：<br><img src="/img/rancher-applications.jpg" alt=""></p>
<p>不像其他的web应用一开始没有数据时都是显示一片空白，Rancher展示了非常丰富的信息来帮助我们尽快上手。菜单上的<strong>ADMIN</strong>有个红色的感叹号，这是因为我们刚启动服务器，还没有配置认证信息。点击这个感叹号就可以开始配置，除了本地设置用户名密码以外，还支持与AD、GitHub和LDAP的集成。这里我们更加关注容器管理部分，对鉴权有兴趣的朋友可以自行尝试认证信息的配置。点击菜单上的<strong>INFRASTRUCTURE</strong>并点击<strong>Add Host</strong>按钮，可以增加一个agent host。由于现在我们用的是内部IP<strong>192.168.33.17</strong>，Rancher会提示我们是否真的连接到这里，不用管它直接点击<strong>Save</strong>按钮就可以了。复制下一个页面中第5步的命令，在agent上1运行即可。在我的虚拟机上是这样子的：<br><figure class="highlight sh"><figcaption><span>agent1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run <span class="operator">-d</span> --privileged -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/rancher:/var/lib/rancher rancher/agent:v0.<span class="number">11.0</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span>/v1/scripts/B9EAC6780C8126FB739E:<span class="number">1460016000000</span>:Adj9D4Qp3smSmIscdUVT0JSCPdM</span><br></pre></td></tr></table></figure></p>
<p>然后就可以点击<strong>Close</strong>，稍待片刻，就能看到agent1已经被加入到Hosts里了：<br><img src="/img/rancher-infrastructure.jpg" alt=""></p>
<p>在agent2上重复执行一遍命令，把agent2也加入到Hosts里。Rancher的server和agent都是设置为<code>restart=true</code>的，所以重启虚拟机之类的行为也不会影响Rancher正常工作。现在看到的Hosts应该是这样的：<br><img src="/img/rancher-hosts.jpg" alt=""></p>
<h2 id="u8FD0_u884C_u5BB9_u5668"><a href="#u8FD0_u884C_u5BB9_u5668" class="headerlink" title="运行容器"></a>运行容器</h2><p>接下来运行一个tomcat容器试试。点击agent1上的<strong>Add Container</strong>按钮，如下填入参数：</p>
<ul>
<li><strong>Name</strong>：tomcat</li>
<li><strong>Select Image</strong>：tomcat:8.0.30-jre8</li>
<li><strong>Public (on Host) IP/Port</strong>：8080</li>
<li><strong>Private (in Container) Port</strong>：8080</li>
</ul>
<p>然后点击最下方的<strong>Create</strong>按钮：<br><img src="/img/rancher-add-container.jpg" alt=""></p>
<p>过一段时间，便能看到如下的容器已经启动完成了：<br><img src="/img/rancher-standalone-container.jpg" alt=""></p>
<p>之所以需要等一段时间，是因为它像kubernetes一样，需要给容器配一个网络代理Network Agent，不过功能要复杂得多，拥有跨网络通信、健康检查等功能。当前版本下使用的网络代理镜像为<code>rancher/agent-instance:v0.8.1</code>。在agent1上运行<code>docker ps</code>便能看到这两个容器。还可以通过<a href="http://192.168.33.18:8080" target="_blank" rel="external">http://192.168.33.18:8080</a>来访问tomcat服务。在页面上点击某个容器比如tomcat，可以看到容器的基本信息和一些基本监控数据。如图：<br><img src="/img/rancher-tomcat-container.jpg" alt=""></p>
<p>自行启动的容器也能被Rancher监控到。我们来启动一个小容器：<br><figure class="highlight sh"><figcaption><span>agent1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --name=bb busybox:<span class="number">1.24</span>.<span class="number">1</span> sleep <span class="number">3600</span></span><br></pre></td></tr></table></figure></p>
<p>在界面上便能看到这个bb容器已经启动完成了：<br><img src="/img/rancher-self-container.jpg" alt=""></p>
<p>通过Rancher启动的容器IP是在<code>10.42.*.*</code>区间的，自行启动的bb容器的IP是在它之外的。如果想用相同IP段，可以使用以下命令：<br><figure class="highlight sh"><figcaption><span>agent1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --name=bb2 --label io.rancher.container.network=<span class="literal">true</span> busybox:<span class="number">1.24</span>.<span class="number">1</span> sleep <span class="number">3600</span></span><br></pre></td></tr></table></figure></p>
<p>在界面上可以看到bb2容器的IP已经落入区间了：<br><img src="/img/rancher-self-container2.jpg" alt=""></p>
<h2 id="u8FD0_u884C_u5E94_u7528"><a href="#u8FD0_u884C_u5E94_u7528" class="headerlink" title="运行应用"></a>运行应用</h2><p>上面我们在指定的虚拟机上创建容器。不过对于一个真实的网络应用，我们并不关心它运行在哪里，只关心服务地址罢了。下面我们来创建一个这样的WordPress应用。它包含一个MySQL数据库，两个WordPress实例和一套负载均衡。首先点击<strong>APPLICATIONS</strong>，然后点击Default的<strong>Add Service</strong>。填入：</p>
<ul>
<li><strong>Name</strong>：database</li>
<li><strong>Select Image</strong>：mysql:5.7.10</li>
<li><strong>Always pull image before creating</strong>：false</li>
<li><strong>Environment Vars</strong>：MYSQL_ROOT_PASSWORD=pass1</li>
</ul>
<p>然后点击<strong>Create</strong>来创建这个MySQL服务。接下来是WordPress，还是像MySQL那样新建服务。填入：</p>
<ul>
<li><strong>Scale</strong>：2</li>
<li><strong>Name</strong>：mywordpress</li>
<li><strong>Select Image</strong>：wordpress:4.4.2</li>
<li><strong>Always pull image before creating</strong>：false</li>
<li><strong>Service Links</strong>：database &gt; mysql</li>
</ul>
<p>然后点击<strong>Create</strong>来创建这个WordPress服务。最后是负载均衡，点击<strong>Add Service</strong>旁边的向下箭头，选择<strong>Add Load Balancer</strong>。填入：</p>
<ul>
<li><strong>Scale</strong>：Always run one instance of this container on every host</li>
<li><strong>Name</strong>：wordpresslb</li>
<li><strong>Source IP/Port</strong>：80</li>
<li><strong>Default Target Port</strong>：80</li>
<li><strong>Target Service</strong>：mywordpress</li>
</ul>
<p>点击<strong>Save</strong>来创建这个负载均衡。稍待片刻，就可以看到wordpresslb变为Active状态了，然后就可以访问<a href="http://192.168.33.18" target="_blank" rel="external">http://192.168.33.18</a>或<a href="http://192.168.33.19" target="_blank" rel="external">http://192.168.33.19</a>来使用WordPress服务了：<br><img src="/img/wordpress.jpg" alt=""></p>
<p>Rancher负载均衡使用和网络代理一样的<code>rancher/agent-instance</code>镜像。它内置了HAProxy，默认使用轮询。</p>
<h2 id="u9884_u7F6E_u6A21_u677F"><a href="#u9884_u7F6E_u6A21_u677F" class="headerlink" title="预置模板"></a>预置模板</h2><p>点击<strong>CATALOG</strong>，便能看到Rancher为我们预置了一系列的应用模板。我们用个小镜像Etcd试试。首先找到Etcd的图标：<br><img src="/img/rancher-etcd.jpg" alt=""></p>
<p>点击<strong>View Details</strong>进入etcd详细页面，滚动到最下方。由于我们只有两个agent，在<strong>Number of Nodes</strong>里填入1，然后点击<strong>Launch</strong>按钮。很快，一个etcd服务就启动起来了。按如下参数给这个服务增加一套负载均衡：</p>
<ul>
<li><strong>Scale</strong>：Always run one instance of this container on every host</li>
<li><strong>Name</strong>：etcdlb</li>
<li><strong>Source IP/Port</strong>：2379</li>
<li><strong>Protocol</strong>：tcp</li>
<li><strong>Default Target Port</strong>：2379</li>
<li><strong>Target Service</strong>：etcd</li>
</ul>
<p>还可以点击<strong>Preview</strong>来查看<code>docker-compose.yml</code>和<code>rancher-compose.yml</code>文件，里面也有比较详细的注释。<code>docker-compose.yml</code>不必多说，<code>rancher-compose.yml</code>类似于它但更小一些。可以在任何Rancher页面的右下方点击<strong>Download CLI</strong>来下载rancher compose命令行工具，这样就可以通过命令行而非在网页上点来点去来管理容器和服务了。最后点击<strong>Save</strong>并等待负载均衡启动完成，就可以访问啦：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -L http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2379</span>/version</span><br><span class="line">curl -L http://<span class="number">192.168</span>.<span class="number">33.19</span>:<span class="number">2379</span>/version</span><br></pre></td></tr></table></figure></p>
<p>太方便了，简直是爽得不能不能的。最后送上全家福大图一张：<br><img src="/img/rancher-applications-stack.jpg" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://rancher.com/">Rancher</a>是开源的容器平台，功能齐全，部署简单，支持Kubernets和Docker Swarm。它把自己定位在持续交付流水线上的后半段上，如下图所示:<br><img src="/img/rancher-feature.png" alt=""></p>
<p>2016年3月底刚刚发布了1.0正式版。借着这个契机，下面就让我们用容器来部署一套Rancher环境试试它的功能吧！<br>]]>
    
    </summary>
    
      <category term="rancher" scheme="http://qinghua.github.io/tags/rancher/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Mac上的git图形工具GitUp]]></title>
    <link href="http://qinghua.github.io/gitup/"/>
    <id>http://qinghua.github.io/gitup/</id>
    <published>2016-04-06T13:43:21.000Z</published>
    <updated>2016-04-06T14:41:12.000Z</updated>
    <content type="html"><![CDATA[<p>已经11岁的<a href="https://git-scm.com/" target="_blank" rel="external">Git</a>现在应该算是最流行的版本管理系统了。不过它的上手过程略令人感伤：为什么要用<code>git reset HEAD</code>而不是<code>git unadd/unstage</code>？Mac的朋友们有福了，<a href="http://gitup.co/" target="_blank" rel="external">GitUp</a>来拯救懒程序员们啦。它提供了一个简约而不简单的界面，让我们可以凭直觉轻松地打出git组合拳来处理各种状况。在2016年4月的<a href="https://www.thoughtworks.com/radar/tools/gitup" target="_blank" rel="external">ThoughtWorks技术雷达</a>上，它处于试验阶段，也就是值得追求，建议尝试。让我们来看看它有什么能力吧。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u73AF_u5883"><a href="#u51C6_u5907_u73AF_u5883" class="headerlink" title="准备环境"></a>准备环境</h2><p>GitUp只是一个小应用程序，下载下来就能用啦：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir gitup</span><br><span class="line"><span class="built_in">cd</span> gitup</span><br><span class="line">wget -c https://s3-us-west-<span class="number">2</span>.amazonaws.com/gitup-builds/stable/GitUp.zip</span><br><span class="line">unzip GitUp.zip</span><br><span class="line">open GitUp.app</span><br></pre></td></tr></table></figure></p>
<p>打开GitUp就能看见下面的界面：<br><img src="/img/gitup-welcome.jpg" alt=""></p>
<p>然后我们新建一个git repo：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">git init</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> a &gt; a.txt</span><br><span class="line"><span class="built_in">echo</span> b &gt; b.txt</span><br><span class="line"><span class="built_in">echo</span> c &gt; c.txt</span><br><span class="line">git add a.txt</span><br><span class="line">git commit -m <span class="string">"a"</span></span><br><span class="line">git add b.txt</span><br><span class="line">git commit -m <span class="string">"bb"</span></span><br><span class="line">git add c.txt</span><br><span class="line">git commit -m <span class="string">"c"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="u67E5_u770B_u5386_u53F2"><a href="#u67E5_u770B_u5386_u53F2" class="headerlink" title="查看历史"></a>查看历史</h2><p>在刚才的GitUp欢迎界面上选择新建的test文件夹，就能看到简洁的版本历史图，有三个commit，其中两个是小圆点，一个是现在所处的HEAD。随便单击选择一个commit：<br><img src="/img/gitup-map.jpg" alt=""></p>
<p>可以看到这个commit的信息，按上下键可以选择其它commit，按空格切换commit详细页面。在commit上右击，便能看到所有支持的操作。我们可以先右击中间的commit，选择<strong>Edit Message</strong>把先前的提交消息”bb”改成”b”。很简单吧！比输命令易用多了。之后在HEAD上右击并选择<strong>Create Branch</strong>来新建一个分支，分支名为temp。然后就能看到下图：<br><img src="/img/gitup-new-branch.jpg" alt=""></p>
<p>可以在终端中运行<code>git branch</code>来确认自己在temp分支上。然后加点代码：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> d &gt;&gt; c.txt</span><br><span class="line">cat c.txt</span><br></pre></td></tr></table></figure></p>
<p>这时切回GitUp的界面，选择中间的视图，如下图所示：<br><img src="/img/gitup-commit.jpg" alt=""></p>
<p>看起来很像<code>git gui</code>吧。双击<code>c.txt</code>就可以切换文件的状态。输入提交消息<code>cd</code>，然后点击<strong>Commit</strong>按钮来提交。于是就能看到下图：<br><img src="/img/gitup-new-branch-commit.jpg" alt=""></p>
<p>双击master的小黄点就可以切换到master分支上了。然后也加点代码：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> e &gt;&gt; c.txt</span><br><span class="line">cat c.txt</span><br><span class="line">git commit -am <span class="string">"ce"</span></span><br></pre></td></tr></table></figure></p>
<p>这回的图变成这样了：<br><img src="/img/gitup-master-commit.jpg" alt=""></p>
<h2 id="u5408_u5E76_u548C_u884D_u5408"><a href="#u5408_u5E76_u548C_u884D_u5408" class="headerlink" title="合并和衍合"></a>合并和衍合</h2><p>我们来试一下合并分支。右击temp上的小圆圈，选择<strong>Merge into Current Branch</strong>，然后点击<strong>Merge</strong>按钮就能看到冲突了。可以使用<strong>Open with Default Editor</strong>来自己解决冲突，也可以使用<strong>Resolve in Merge Tool</strong>来解决。如果是前者，可以注意一个小细节：这里的冲突提示是ours和theirs，看起来人性化了不少。合并完成后，点击<strong>Mark as Resolved</strong>，然后<strong>Commit</strong>，就可以看到图变成这样了：<br><img src="/img/gitup-merge.jpg" alt=""></p>
<p>衍合也是类似。GitUp提供了一个逆天功能Command+Z，可以快速回退到上一次操作（再次前进是Command+Shift+Z）。这样我们很轻松就能再来一次衍合。右击temp上的小圆圈，选择<strong>Rebase Current Branch onto Here</strong>，剩下的和合并分支类似。提交之后，就可以看到图变成这样了：<br><img src="/img/gitup-rebase.jpg" alt=""></p>
<p>GitUp还提供了强大的快照功能。我们可以点击右上方的时钟按钮来选择自己想要的快照，就像Time Machine似的。如下图：<br><img src="/img/gitup-snapshot.jpg" alt=""></p>
<h2 id="u67E5_u770Bstash"><a href="#u67E5_u770Bstash" class="headerlink" title="查看stash"></a>查看stash</h2><p>那么第三个视图是干什么的呢？我们先stash一段代码：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> f &gt;&gt; c.txt</span><br><span class="line">cat c.txt</span><br><span class="line">git stash</span><br><span class="line"><span class="built_in">echo</span> g &gt;&gt; c.txt</span><br></pre></td></tr></table></figure></p>
<p>打开第三个视图，原来是stash列表，这回可以很容易看清楚了。也可以在这里stash：点击左下方的加号按钮，输入一个消息然后<strong>Save Stash</strong>，就可以看到下图：<br><img src="/img/gitup-stashes.jpg" alt=""></p>
<p>还可以在这里轻松地<strong>Apply</strong>想要的stash，这个可视化可以有。可惜还是不支持选择特定文件stash。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>已经11岁的<a href="https://git-scm.com/">Git</a>现在应该算是最流行的版本管理系统了。不过它的上手过程略令人感伤：为什么要用<code>git reset HEAD</code>而不是<code>git unadd/unstage</code>？Mac的朋友们有福了，<a href="http://gitup.co/">GitUp</a>来拯救懒程序员们啦。它提供了一个简约而不简单的界面，让我们可以凭直觉轻松地打出git组合拳来处理各种状况。在2016年4月的<a href="https://www.thoughtworks.com/radar/tools/gitup">ThoughtWorks技术雷达</a>上，它处于试验阶段，也就是值得追求，建议尝试。让我们来看看它有什么能力吧。<br>]]>
    
    </summary>
    
      <category term="GitUp" scheme="http://qinghua.github.io/tags/GitUp/"/>
    
      <category term="git" scheme="http://qinghua.github.io/tags/git/"/>
    
      <category term="tool" scheme="http://qinghua.github.io/categories/tool/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[用容器轻松搭建Prometheus运行环境]]></title>
    <link href="http://qinghua.github.io/prometheus/"/>
    <id>http://qinghua.github.io/prometheus/</id>
    <published>2016-03-30T11:01:01.000Z</published>
    <updated>2016-03-31T05:50:06.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://prometheus.io/" target="_blank" rel="external">Prometheus</a>是一个开源的监控解决方案，包括数据采集、汇聚、存储、可视化、监控、告警等。除了基本的监控数据，也支持通过自定义exporter来获取自己想要的数据。本文从零开始用容器搭建一个prometheus环境，并介绍一些基本功能。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下代码，相当于给它分配一台IP是<strong>192.168.33.18</strong>的虚拟机。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后在终端运行以下命令启动并连接虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>Prometheus的环境搭建起来非常简单，只要一个docker镜像即可。绿色的压缩包安装方式可以参考<a href="https://prometheus.io/docs/introduction/getting_started/" target="_blank" rel="external">官方文档</a>。此外还需要一个配置文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;prometheus.yml</span><br><span class="line">global:</span><br><span class="line">  scrape_interval: <span class="number">15</span>s</span><br><span class="line">  external_labels:</span><br><span class="line">    monitor: <span class="string">'codelab-monitor'</span></span><br><span class="line">scrape_configs:</span><br><span class="line">  - job_name: <span class="string">'prometheus'</span></span><br><span class="line">    scrape_interval: <span class="number">5</span>s</span><br><span class="line">    target_groups:</span><br><span class="line">      - targets: [<span class="string">'localhost:9090'</span>]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mkdir /etc/prometheus</span><br><span class="line">sudo mv prometheus.yml /etc/prometheus</span><br></pre></td></tr></table></figure></p>
<p>配置文件中，<code>scrape_interval</code>指的是数据获取间隔，<code>prometheus</code>这个任务里的<code>scrape_interval</code>将会在这个任务里覆盖掉默认的<code>global</code>全局值，也就是这个任务每5秒钟获取一次数据，其它任务则是每15秒钟。完整的配置文件格式，请参考<a href="http://prometheus.io/docs/operating/configuration/" target="_blank" rel="external">官方文档</a>。接下来启动Prometheus：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/docker run <span class="operator">-d</span> \</span><br><span class="line">    --name=prometheus \</span><br><span class="line">    --publish=<span class="number">9090</span>:<span class="number">9090</span> \</span><br><span class="line">    -v /etc/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml \</span><br><span class="line">    -v /var/prometheus/storage:/prometheus \</span><br><span class="line">    prom/prometheus:<span class="number">0.17</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>启动完成后，将会在<a href="http://192.168.33.18:9090" target="_blank" rel="external">http://192.168.33.18:9090</a>看到prometheus的首页：<br><img src="/img/prometheus-home.jpg" alt=""></p>
<h2 id="u6570_u636E_u6536_u96C6"><a href="#u6570_u636E_u6536_u96C6" class="headerlink" title="数据收集"></a>数据收集</h2><p>在<a href="http://192.168.33.18:9090/metrics" target="_blank" rel="external">http://192.168.33.18:9090/metrics</a>可以看到prometheus收集到的数据。其中有一个<code>prometheus_target_interval_length_seconds</code>，表示真实的数据获取间隔。在prometheus首页输入它并回车，就可以看到一系列的数据，它们有着不同的quantile，从0.01至0.99不等。0.99的意思是有99%的数据都在这个值以内。如果我们只关心这个数，我们可以输入<code>prometheus_target_interval_length_seconds{quantile=&quot;0.99&quot;}</code>来查看。查询还支持函数，比如<code>count(prometheus_target_interval_length_seconds)</code>可以查询数量。完整的表达式可以参考<a href="https://prometheus.io/docs/querying/basics/" target="_blank" rel="external">官方文档</a>。</p>
<p>点击<strong>Console</strong>旁边的<strong>Graph</strong>标签就可以看见时序图了：<br><img src="/img/prometheus-graph.jpg" alt=""></p>
<p>可以随意选择指标和函数试一试，比如<code>rate(prometheus_local_storage_chunk_ops_total[1m])</code>。</p>
<h2 id="Exporter"><a href="#Exporter" class="headerlink" title="Exporter"></a>Exporter</h2><p>Prometheus支持官方/非官方的许多种<a href="https://prometheus.io/docs/instrumenting/exporters/" target="_blank" rel="external">exporter</a>，如HAProxy，Jenkins，MySQL等，也有一些软件直接支持Prometheus而无需exporter，如Etcd，Kubernetes等。我们试一下node exporter：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=ne \</span><br><span class="line">  -p <span class="number">9100</span>:<span class="number">9100</span> \</span><br><span class="line">  prom/node-exporter</span><br></pre></td></tr></table></figure></p>
<p>Node exporter暴露的端口是9100，所以我们需要修改一下prometheus的配置文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;prometheus.yml</span><br><span class="line">global:</span><br><span class="line">  scrape_interval: <span class="number">15</span>s</span><br><span class="line">  external_labels:</span><br><span class="line">    monitor: <span class="string">'codelab-monitor'</span></span><br><span class="line">scrape_configs:</span><br><span class="line">  - job_name: <span class="string">'node'</span></span><br><span class="line">    scrape_interval: <span class="number">5</span>s</span><br><span class="line">    target_groups:</span><br><span class="line">      - targets: [<span class="string">'192.168.33.18:9100'</span>]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo cp prometheus.yml /etc/prometheus</span><br></pre></td></tr></table></figure></p>
<p>重启prometheus：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker stop prometheus</span><br><span class="line">sudo rm -rf /var/prometheus/storage</span><br><span class="line">docker start prometheus</span><br></pre></td></tr></table></figure></p>
<p>这样在页面上就可以选择节点的一些指标了。也可以访问<a href="http://192.168.33.18:9100/" target="_blank" rel="external">http://192.168.33.18:9100/</a>来直接查看Exporter的指标。</p>
<h2 id="Push_Gateway"><a href="#Push_Gateway" class="headerlink" title="Push Gateway"></a>Push Gateway</h2><p>Prometheus采集数据是用的pull也就是拉模型，这从我们刚才设置的5秒参数就能看出来。但是有些数据并不适合采用这样的方式，对这样的数据可以使用Push Gateway服务。它就相当于一个缓存，当数据采集完成之后，就上传到这里，由Prometheus稍后再pull过来。我们来试一下，首先启动Push Gateway：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=pg \</span><br><span class="line">  -p <span class="number">9091</span>:<span class="number">9091</span> \</span><br><span class="line">  prom/pushgateway</span><br></pre></td></tr></table></figure></p>
<p>可以访问<a href="http://192.168.33.18:9091/" target="_blank" rel="external">http://192.168.33.18:9091/</a>来查看它的页面。下个命令将会往Push Gateway上传数据：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"some_metric 3.14"</span> | curl --data-binary @- http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">9091</span>/metrics/job/some_job</span><br></pre></td></tr></table></figure></p>
<p>效果是酱紫滴：<br><img src="/img/prometheus-push-gateway.jpg" alt=""></p>
<p>而在Prometheus的配置文件里，只要把端口换成<code>9100</code>便能采集到Push Gateway的数据了。</p>
<h2 id="Grafana"><a href="#Grafana" class="headerlink" title="Grafana"></a>Grafana</h2><p><a href="https://prometheus.io/docs/visualization/grafana/" target="_blank" rel="external">Grafana</a>是目前比较流行的监控可视化UI，它从2.5.0版开始直接支持Prometheus的数据。我们来试一下。首先启动grafana：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name grafana \</span><br><span class="line">  -p <span class="number">3000</span>:<span class="number">3000</span> \</span><br><span class="line">  grafana/grafana:<span class="number">2.6</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>打开<a href="http://192.168.33.18:3000/" target="_blank" rel="external">http://192.168.33.18:3000/</a>，就能看到grafana的登录页面了。输入默认的admin/admin登录grafana。选择左侧的<strong>Data Sources</strong>，然后点击上面的<strong>Add new</strong>按钮，便可以把prometheus作为数据源导入grafana：<br><img src="/img/grafana-prometheus-data-source.jpg" alt=""></p>
<p>输入下面的值：</p>
<ul>
<li>Name：prometheus</li>
<li>Default：true</li>
<li>Type：Prometheus</li>
<li>Url：<a href="http://192.168.33.18:9090/" target="_blank" rel="external">http://192.168.33.18:9090/</a></li>
</ul>
<p>然后点击<strong>Add</strong>按钮。之后会出来一个<strong>Test Connection</strong>的按钮，点击它便可以收到<strong>Data source is working</strong>的消息。点击左边的<strong>Dashboards</strong>回到主页，点击上面的<strong>Home</strong>，选择<strong>+ New</strong>，会出来一个绿色的小竖条，点击它便会弹出来一个菜单：<br><img src="/img/grafana-dashboard-menu.jpg" alt=""></p>
<p>选择<strong>Add Panel</strong>和<strong>Graph</strong>，便会出来一个图。然后就可以在<strong>Query</strong>里输入prometheus支持的查询了：<br><img src="/img/grafana-prometheus-graph.jpg" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="https://prometheus.io/">Prometheus</a>是一个开源的监控解决方案，包括数据采集、汇聚、存储、可视化、监控、告警等。除了基本的监控数据，也支持通过自定义exporter来获取自己想要的数据。本文从零开始用容器搭建一个prometheus环境，并介绍一些基本功能。<br>]]>
    
    </summary>
    
      <category term="prometheus" scheme="http://qinghua.github.io/tags/prometheus/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[TOSCA简介]]></title>
    <link href="http://qinghua.github.io/tosca/"/>
    <id>http://qinghua.github.io/tosca/</id>
    <published>2016-03-28T11:29:04.000Z</published>
    <updated>2016-03-29T09:35:22.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://www.oasis-open.org/committees/tosca/" target="_blank" rel="external">TOSCA</a>（Topology and Orchestration Specification for Cloud Applications）是由OASIS组织制定的云应用拓扑编排规范。通俗地说，就是制定了一个标准，用来描述云平台上应用的拓扑结构。目前支持XML和YAML，Cloudiy的蓝图就是基于这个规范而来。这个规范比较庞大，本文尽量浓缩了<a href="http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html" target="_blank" rel="external">TOSCA的YAML版</a>前两章，以便用尽量少的时间了解尽量多的规范内容。<br><a id="more"></a></p>
<h2 id="u7B80_u4ECB"><a href="#u7B80_u4ECB" class="headerlink" title="简介"></a>简介</h2><p>TOSCA的基本概念只有两个：节点（node）和关系（relationship）。节点有许多类型，可以是一台服务器，一个网络，一个计算节点等等。关系描述了节点之间是如何连接的。举个栗子：一个nodejs应用（节点）部署在（关系）名为host的主机（节点）上。节点和关系都可以通过程序来扩展和实现。</p>
<p>目前它的开源实现有OpenStack (Heat-Translator，Tacker，Senlin)，Alien4Cloud，Cloudify等。</p>
<h2 id="u793A_u4F8B"><a href="#u793A_u4F8B" class="headerlink" title="示例"></a>示例</h2><h3 id="Hello_World"><a href="#Hello_World" class="headerlink" title="Hello World"></a>Hello World</h3><p>首先登场的是广大程序猿和攻城狮们都喜闻乐见的Hello World，但是其实里面并没有Hello World，只是比较简单而已。先看下面这段描述文件：</p>
<pre>
tosca_definitions_version: tosca_simple_yaml_1_0

description: Template for deploying a single server with predefined properties.

topology_template:
  node_templates:
    my_server:
      type: tosca.nodes.Compute
      capabilities:
        host:
          properties:
            num_cpus: 1
            disk_size: 10 GB
            mem_size: 4096 MB
        os:
          properties:
            architecture: x86_64
            type: linux 
            distribution: rhel 
            version: 6.5 
</pre>

<p>除了TOSCA的版本<code>tosca_definitions_version</code>和描述信息<code>description</code>以外，就是这个<code>topology_template</code>了。这里我们看到有一个名为<code>my_server</code>的节点，它的类型是<code>tosca.nodes.Compute</code>。这个类型预置了两个<code>capabilities</code>信息，一个是<code>host</code>，定义了硬件信息；另一个是<code>os</code>，定义了操作系统信息。</p>
<h3 id="u8F93_u5165_u8F93_u51FA"><a href="#u8F93_u5165_u8F93_u51FA" class="headerlink" title="输入输出"></a>输入输出</h3><p>再看看下面这个描述文件：</p>
<pre>
topology_template:
  <b style="color:magenta">inputs</b>:
    cpus:
      type: integer
      description: Number of CPUs for the server.
      constraints:
        - valid_values: [ 1, 2, 4, 8 ]

  node_templates:
    my_server:
      type: tosca.nodes.Compute
      capabilities:
        host:
          properties:
            num_cpus: { get_input: cpus }
            mem_size: 2048  MB
            disk_size: 10 GB

  <b style="color:magenta">outputs</b>:
    server_ip:
      description: The private IP address of the provisioned server.
      value: { get_attribute: [ my_server, private_address ] }
</pre>

<p>这里的<code>inputs</code>和<code>outputs</code>分别定义了输入和输出。输入的<code>cpus</code>是在1，2，4和8中的一个整数，而输出的<code>server_ip</code>就是<code>my_server</code>这个节点的<code>private_address</code>也就是私有IP地址。另外一点是TOSCA提供了一些内置函数，在上面这个文件中使用了<code>get_input</code>和<code>get_attribute</code>。输入参数可以通过<code>get_input</code>被使用。</p>
<h3 id="u5B89_u88C5_u8F6F_u4EF6"><a href="#u5B89_u88C5_u8F6F_u4EF6" class="headerlink" title="安装软件"></a>安装软件</h3><p>第三个描述文件如下：</p>
<pre>
topology_template:
  inputs:
    # 略

  node_templates:
    mysql:
      type: <b style="color:magenta">tosca.nodes.DBMS.MySQL</b>
      properties:
        root_password: { get_input: my_mysql_rootpw }
        port: { get_input: my_mysql_port }
      <b style="color:magenta">requirements</b>:
        - host: db_server

    db_server:
      type: tosca.nodes.Compute
      capabilities:
        # 略
</pre>

<p>我们看到了一个新的节点类型：<code>tosca.nodes.DBMS.MySQL</code>。这个类型允许接收<code>root_password</code>和<code>port</code>的参数。在<code>requirements</code>里定义了<code>mysql</code>这个节点需要被安装到<code>db_server</code>这个节点上，这就是“关系”。如果只想表明依赖，比如说<code>service_a</code>依赖于<code>service_b</code>，也可以直接用<code>- dependency: service_b</code>来描述。上面文件的拓扑结构如下图：<br><img src="http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/csprd02/TOSCA-Simple-Profile-YAML-v1.0-csprd02_files/image003.png" alt=""></p>
<h3 id="u521D_u59CB_u5316_u6570_u636E_u5E93"><a href="#u521D_u59CB_u5316_u6570_u636E_u5E93" class="headerlink" title="初始化数据库"></a>初始化数据库</h3><p>第四个描述文件如下：</p>
<pre>
  node_templates:
    my_db:
      type: <b style="color:magenta">tosca.nodes.Database.MySQL</b>
      properties:
        name: { get_input: database_name }
        user: { get_input: database_user }
        password: { get_input: database_password }
        port: { get_input: database_port }
      <b style="color:magenta">artifacts</b>:
        db_content:
          file: files/my_db_content.txt
          type: tosca.artifacts.File
      requirements:
        - host: mysql
      interfaces:
        <b style="color:magenta">Standard:
          create:
            implementation: db_create.sh</b>
            inputs:
              db_data: { get_artifact: [ SELF, db_content ] }

    mysql:
      type: tosca.nodes.DBMS.MySQL
      properties:
        root_password: { get_input: mysql_rootpw }
        port: { get_input: mysql_port }
      requirements:
        - host: db_server

    db_server:
      # 略
</pre>

<p>这里的<code>tosca.nodes.Database.MySQL</code>表示一个MySQL数据库的实例。在<code>artifacts</code>的<code>db_content</code>里指定了一个文本文件，而这个文件将被<code>interfaces</code>里的<code>Create</code>所用，为<code>db_create.sh</code>脚本提供数据。<code>Standard</code>表示生命周期，可能会包含<code>configure</code>、<code>start</code>、<code>stop</code>等各种操作，而<code>db_create.sh</code>本身是对<code>tosca.nodes.Database.MySQL</code>提供的默认<code>create</code>操作的一个重写。如下图：<br><img src="http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/csprd02/TOSCA-Simple-Profile-YAML-v1.0-csprd02_files/image004.png" alt=""></p>
<h3 id="u4E24_u5C42_u5E94_u7528"><a href="#u4E24_u5C42_u5E94_u7528" class="headerlink" title="两层应用"></a>两层应用</h3><p>再来看看第五个描述文件：</p>
<pre>
  node_templates:
    wordpress:
      type: tosca.nodes.WebApplication.WordPress
      properties:
        context_root: { get_input: context_root }
        admin_user: { get_input: wp_admin_username }
        admin_password: { get_input: wp_admin_password }
        db_host: { get_attribute: [ db_server, private_address ] }
      <b style="color:magenta">requirements:
        - host: apache
        - database_endpoint: wordpress_db</b>
      interfaces:
        Standard:
          inputs:
            db_host: { get_attribute: [ db_server, private_address ] }
            db_port: { get_property: [ wordpress_db, port ] }
            db_name: { get_property: [ wordpress_db, name ] }
            db_user: { get_property: [ wordpress_db, user ] }
            db_password: { get_property: [ wordpress_db, password ] }  
    apache:
      type: tosca.nodes.WebServer.Apache
      properties:
        # 略
      <b style="color:magenta">requirements:
        - host: web_server</b>
    web_server:
      type: tosca.nodes.Compute
      # 略

    wordpress_db:
      type: tosca.nodes.Database.MySQL
      # 略
    mysql:
      type: tosca.nodes.DBMS.MySQL
      # 略
    db_server:
      type: tosca.nodes.Compute
      # 略
</pre>

<p>这个文件描述了一个很常见的拓扑结构：<code>mysql</code>里有一个<code>wordpress_db</code>，运行在<code>db_server</code>上；<code>apache</code>部署了一个<code>wordpress</code>，运行在<code>web_server</code>上。<code>wordpress</code>需要<code>wordpress_db</code>。</p>
<h3 id="u5173_u7CFB_u5B9A_u5236_u5316"><a href="#u5173_u7CFB_u5B9A_u5236_u5316" class="headerlink" title="关系定制化"></a>关系定制化</h3><p>第六个描述文件：</p>
<pre>
  node_templates:
    wordpress:
      type: tosca.nodes.WebApplication.WordPress
      properties:
        # 略
      requirements:
        - host: apache
        - database_endpoint:
            node: wordpress_db
            <b style="color:magenta">relationship: my.types.WordpressDbConnection</b>
    wordpress_db:
      type: tosca.nodes.Database.MySQL
      properties:
        # 略
      requirements:
        - host: mysql
  <b style="color:magenta">relationship_templates:
    my.types.WordpressDbConnection:</b>
      type: ConnectsTo
      interfaces:
        Configure:
          pre_configure_source: scripts/wp_db_configure.sh
</pre>

<p>这里的关注点是<code>relationship</code>里的<code>my.types.WordpressDbConnection</code>。这是一个自定义的关系，在文件的下半部分描述了详细定义。它实际上是一个<code>ConnectsTo</code>类型，为<code>pre_configure_source</code>操作提供了一个自定义脚本。这个定义也可以单独提出一个文件，就像下面这样：</p>
<pre>
tosca_definitions_version: tosca_simple_yaml_1_0

description: Definition of custom WordpressDbConnection relationship type

<b style="color:magenta">relationship_types:
  my.types.WordpressDbConnection:</b>
    derived_from: tosca.relationships.ConnectsTo
    interfaces:
      Configure:
        pre_configure_source: scripts/wp_db_configure.sh
</pre>

<h3 id="u9650_u5B9A_u9700_u6C42_u8D44_u6E90"><a href="#u9650_u5B9A_u9700_u6C42_u8D44_u6E90" class="headerlink" title="限定需求资源"></a>限定需求资源</h3><p>再看一个描述文件：</p>
<pre>
  node_templates:
    mysql:
      type: tosca.nodes.DBMS.MySQL
      properties:
        # 略
      requirements:
        - host:
            <b style="color:magenta">node_filter</b>:
              capabilities:
                - host:
                    properties:
                      - num_cpus: { <b style="color:magenta">in_range</b>: [ 1, 4 ] }
                      - mem_size: { <b style="color:magenta">greater_or_equal</b>: 2 GB }
                - os:
                    properties:
                      - architecture: { <b style="color:magenta">equal</b>: x86_64 }
                      - type: linux
                      - distribution: ubuntu
</pre>

<p>需要关注的是<code>node_filter</code>。这里并没有指定mysql在哪个节点上启动，但是指定了一些节点信息，只有符合的节点才能够启动它。也可以抽出来做个模板：</p>
<pre>
  node_templates:
    mysql:
      type: tosca.nodes.DBMS.MySQL
      properties:
        # 略
      requirements:
        - host: <b style="color:magenta">mysql_compute</b>

    <b style="color:magenta">mysql_compute</b>:
      type: Compute
      node_filter:
        capabilities:
          - host:
              properties:
                num_cpus: { equal: 2 }
                mem_size: { greater_or_equal: 2 GB }
          - os:
              properties:
                architecture: { equal: x86_64 }
                type: linux
                distribution: ubuntu
</pre>

<p>数据库也可以使用：</p>
<pre>
  node_templates:
    my_app:
      type: my.types.MyApplication
      properties:
        admin_user: { get_input: admin_username }
        admin_password: { get_input: admin_password }
        db_endpoint_url: { get_property: [SELF, <b style="color:magenta">database_endpoint</b>, url_path ] }         
      requirements:
        - <b style="color:magenta">database_endpoint</b>:
            node: my.types.nodes.MyDatabase
            <b style="color:magenta">node_filter</b>:
              properties:
                - db_version: { greater_or_equal: 5.5 }
</pre>

<p>上面指定了数据库的版本。也可以抽出来做个模板：</p>
<pre>
  node_templates:
    my_app:
      type: my.types.MyApplication
      properties:
        admin_user: { get_input: admin_username }
        admin_password: { get_input: admin_password }
        db_endpoint_url: { get_property: [SELF, database_endpoint, url_path ] }         
      requirements:
        - database_endpoint: <b style="color:magenta">my_abstract_database</b>
    <b style="color:magenta">my_abstract_database</b>:
      type: my.types.nodes.MyDatabase
      properties:
        - db_version: { greater_or_equal: 5.5 }
</pre>

<h3 id="u8282_u70B9_u6A21_u677F_u66FF_u6362"><a href="#u8282_u70B9_u6A21_u677F_u66FF_u6362" class="headerlink" title="节点模板替换"></a>节点模板替换</h3><p>再看一个描述文件：</p>
<pre>
  node_templates:
    web_app:
      type: tosca.nodes.WebApplication.MyWebApp
      requirements:
        - host: web_server
        - database_endpoint: <b style="color:magenta">db</b>

    web_server:
      type: tosca.nodes.WebServer
      requirements:
        - host: server

    server:
      type: tosca.nodes.Compute
      # 略

    <b style="color:magenta">db</b>:
      # 这是一个抽象节点
      type: tosca.nodes.Database
      properties:
        user: my_db_user
        password: secret
        name: my_db_name
</pre>

<p>这里的<code>db</code>是一个抽象节点，可以被下面的描述文件所替换：</p>
<pre>
topology_template:
  inputs:
    db_user:
      type: string
    # 略
  <b style="color:magenta">substitution_mappings:
    node_type: tosca.nodes.Database
    capabilities:
      database_endpoint: [ database, database_endpoint ]</b>
  node_templates:
    database:
      type: tosca.nodes.Database
      properties:
        user: { get_input: db_user }
        # 略
      requirements:
        - host: dbms
    dbms:
      type: tosca.nodes.DBMS
      # 略
    server:
      type: tosca.nodes.Compute
      # 略
</pre>

<p>这里的<code>database_endpoint</code>是由<code>database</code>节点提供的<code>database_endpoint</code>。两个文件联系起来看，表明了上面的<code>web_app</code>不需要管<code>db</code>是什么样子的，有什么拓扑结构，它关心的只是<code>database_endpoint</code>。而下面由<code>database</code>、<code>dbms</code>和<code>server</code>三个节点组成的模板正好可以提供<code>database_endpoint</code>，从而替换掉<code>db</code>这个抽象节点。另外，这样的替换也支持嵌套。</p>
<h3 id="u8282_u70B9_u6A21_u677F_u7EC4"><a href="#u8282_u70B9_u6A21_u677F_u7EC4" class="headerlink" title="节点模板组"></a>节点模板组</h3><p>再看一个描述文件：</p>
<pre>
  node_templates:
    apache:
      type: tosca.nodes.WebServer.Apache
      properties:
        # 略
      requirements:
        - host: server
    server:
      type: tosca.nodes.Compute
        # 略
  <b style="color:magenta">groups</b>:
    <b style="color:magenta">webserver_group</b>:
      type: tosca.groups.Root
      members: [ apache, server ]

  <b style="color:magenta">policies</b>:
    - my_anti_collocation_policy:
        type: my.policies.anticolocateion
        targets: [ <b style="color:magenta">webserver_group</b> ]
        # 可以一起处理
</pre>

<p>这个例子表明了<code>apache</code>和<code>server</code>应该是一组的关系。这样它们就可以一起被处理，比如说伸缩。</p>
<h3 id="YAML_u5B8F"><a href="#YAML_u5B8F" class="headerlink" title="YAML宏"></a>YAML宏</h3><p>下面这个描述文件使用了宏来避免重复：</p>
<pre>
<b style="color:magenta">dsl_definitions:
  my_compute_node_props: &my_compute_node_props</b>
    disk_size: 10 GB
    num_cpus: 1
    mem_size: 2 GB

topology_template:
  node_templates:
    my_server:
      type: Compute
      capabilities:
        - host:
            properties: <b style="color:magenta">*my_compute_node_props</b>

    my_database:
      type: Compute
      capabilities:
        - host:
            properties: <b style="color:magenta">*my_compute_node_props</b>
</pre>

<h3 id="u4F20_u53C2"><a href="#u4F20_u53C2" class="headerlink" title="传参"></a>传参</h3><p>先看一个描述文件：</p>
<pre>
  node_templates: 
    wordpress:
      type: tosca.nodes.WebApplication.WordPress
      requirements:
        - database_endpoint: mysql_database
      interfaces:
        Standard:
          <b style="color:magenta">inputs</b>:
            wp_db_port: { get_property: [ SELF, database_endpoint, port ] }
          configure:
            implementation: wordpress_configure.sh           
            <b style="color:magenta">inputs</b>:
              wp_db_port: { get_property: [ SELF, database_endpoint, port ] }
</pre>

<p>这个例子有两个<code>inputs</code>，前者指的是为所有操作都声明一个变量，后者指的是为<code>configure</code>这个操作声明一个变量。再看下一个文件：</p>
<pre>
  node_templates: 
    frontend: 
      type: MyTypes.SomeNodeType    
      attributes: 
        url: { <b style="color:magenta">get_operation_output</b>: [ SELF, Standard, create, generated_url ] } 
      interfaces: 
        Standard: 
          create: 
            implementation: scripts/frontend/create.sh
          configure: 
            implementation: scripts/frontend/configure.sh 
            inputs: 
              data_dir: { <b style="color:magenta">get_operation_output</b>: [ SELF, Standard, create, data_dir ] }
</pre>

<p>在这个例子里有两个<code>get_operation_output</code>，前者指的是将<code>create</code>操作的环境变量<code>generated_url</code>设置到<code>url</code>里，后者是将<code>data_dir</code>传递给<code>configure</code>操作。</p>
<h3 id="u53D6_u52A8_u6001_u503C"><a href="#u53D6_u52A8_u6001_u503C" class="headerlink" title="取动态值"></a>取动态值</h3><p>最后一个描述文件：</p>
<pre>
node_types:
  ServerNode:
    derived_from: SoftwareComponent
    properties:
      <b style="color:magenta">notification_port</b>:
        type: integer
    capabilities:
      # 略
  ClientNode:
    derived_from: SoftwareComponent
    properties:
      # 略
    requirements:
      - server:
          capability: Endpoint
          node: ServerNode 
          relationship: ConnectsTo
topology_template:          
  node_templates:
    my_server:
      type: ServerNode 
      properties:
        notification_port: 8000
    my_client:
      type: ClientNode
      requirements:
        - server:
            node: my_server
            relationship: <b style="color:magenta">my_connection</b>
  relationship_templates:
    <b style="color:magenta">my_connection</b>:
      type: ConnectsTo
      interfaces:
        Configure:
          inputs:
            <b style="color:magenta">targ_notify_port: { get_attribute: [ TARGET, notification_port ] }</b>
            # 略
</pre>

<p>这个例子里，类型为<code>ClientNode</code>的<code>my_client</code>在<code>my_connection</code>关系的<code>Configure</code>操作上需要<code>notification_port</code>变量。这样的话，当类型为<code>ServerNode</code>的<code>my_server</code>连接过来时，就能取到它的<code>notification_port</code>变量，并设置到<code>targ_notify_port</code>环境变量里。有一点值得注意的是，真实的<code>notification_port</code>可能是8000，也可能不是。所以在这种情况下，不用<code>get_property</code>，而用<code>get_attribute</code>函数。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="https://www.oasis-open.org/committees/tosca/">TOSCA</a>（Topology and Orchestration Specification for Cloud Applications）是由OASIS组织制定的云应用拓扑编排规范。通俗地说，就是制定了一个标准，用来描述云平台上应用的拓扑结构。目前支持XML和YAML，Cloudiy的蓝图就是基于这个规范而来。这个规范比较庞大，本文尽量浓缩了<a href="http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html">TOSCA的YAML版</a>前两章，以便用尽量少的时间了解尽量多的规范内容。<br>]]>
    
    </summary>
    
      <category term="cloudify" scheme="http://qinghua.github.io/tags/cloudify/"/>
    
      <category term="tosca" scheme="http://qinghua.github.io/tags/tosca/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[轻松搭建Cloudify运行环境]]></title>
    <link href="http://qinghua.github.io/cloudify/"/>
    <id>http://qinghua.github.io/cloudify/</id>
    <published>2016-03-26T11:21:30.000Z</published>
    <updated>2016-03-26T11:21:09.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://getcloudify.org/" target="_blank" rel="external">Cloudify</a>是一个开源的云应用编排系统，它允许使用DSL来描述应用的拓扑结构，并部署到任意环境中。本文大量参考了<a href="http://docs.getcloudify.org/3.3.1/intro/what-is-cloudify/" target="_blank" rel="external">官方教程</a>从零开始搭建并管理一个cloudify 3.3.1集群。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，注释掉并在它的下面添加如下几行代码，相当于给它分配两台虚拟机，一台叫做<strong>manager</strong>，它的IP是<strong>192.168.33.17</strong>；另一台叫做<strong>agent</strong>，它们的IP是<strong>192.168.33.18</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"manager"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.box = <span class="string">"cloudify-virtualbox_3.3.0-ga-b300.box"</span></span><br><span class="line">  host.vm.box_url = <span class="string">"http://repository.cloudifysource.org/org/cloudify3/3.3.0/ga-RELEASE/cloudify-virtualbox_3.3.0-ga-b300.box"</span></span><br><span class="line">  host.vm.hostname = <span class="string">"manager"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">2048</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"agent"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.box = <span class="string">"minimum/ubuntu-trusty64-docker"</span></span><br><span class="line">  host.vm.hostname = <span class="string">"agent"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>虚拟机agent所用的vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。虚拟机manager用的远程镜像是cloudify官方镜像，提供了cloudify manager功能。然后分别在两个终端运行以下命令启动并连接两台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh manager</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh agent</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>在Ubuntu上安装cloudify很简单，在agent上运行以下命令即可：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget -c http://repository.cloudifysource.org/org/cloudify3/get-cloudify.py</span><br><span class="line">sudo python get-cloudify.py</span><br></pre></td></tr></table></figure></p>
<p>安装完了之后，运行以下命令可以看到cloudify命令行的版本及帮助文档：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cfy --version</span><br><span class="line">cfy -h</span><br></pre></td></tr></table></figure></p>
<h2 id="u90E8_u7F72_u5E94_u7528"><a href="#u90E8_u7F72_u5E94_u7528" class="headerlink" title="部署应用"></a>部署应用</h2><p>Cloudify的应用被称为<a href="http://docs.getcloudify.org/3.3.1/intro/blueprints/" target="_blank" rel="external">蓝图</a>（blueprint），这个名字很好地诠释了它在主页上声称的“从蓝图到生产环境（From Blueprint to Production）”。官方已经为我们的第一次使用准备了一个Hello World，让我们先下载下来：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -c https://github.com/cloudify-examples/simple-python-webserver-blueprint/archive/master.zip</span><br><span class="line">sudo apt-get install <span class="operator">-f</span> unzip</span><br><span class="line">unzip master.zip</span><br><span class="line"><span class="built_in">cd</span> simple-python-webserver-blueprint-master/</span><br></pre></td></tr></table></figure></p>
<p>接下来初始化下载的蓝图并传入端口等参数：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> init --blueprint-path blueprint.yaml --inputs <span class="string">'&#123;"webserver_port": "8000", "host_ip":"localhost"&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>Cloudify使用工作流（workflow）来管理应用程序。现在启动install工作流来部署一个python的web服务器：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> execute --workflow install</span><br><span class="line">curl localhost:<span class="number">8000</span></span><br></pre></td></tr></table></figure></p>
<p>也可以在启动vagrant虚拟机的主机上访问：<a href="http://192.168.33.18:8000" target="_blank" rel="external">http://192.168.33.18:8000</a>：<br><img src="/img/cloudify-hello-world.jpg" alt=""></p>
<p>短短几步，我们便顺利部署了一个应用。通过以下命令可以看到一些运行的参数：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> outputs</span><br></pre></td></tr></table></figure></p>
<p>我们看到的内容称之为模型（model）。蓝图是应用的模板，蓝图的实例称为部署（deployment），部署就是模型的内容之一。蓝图里的每个实体称之为节点（node），节点在部署里称为节点实例（node-instances），它们是一对多的关系。但是在这个例子里，我们有两个节点，每个节点各有一个节点实例。可以用以下命令查看节点实例：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> instances</span><br></pre></td></tr></table></figure></p>
<p>我们能看到这两个节点实例分别是host和http_web_server，其中http_web_server运行在host之上。可以用以下命令来结束部署：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> execute -w uninstall</span><br></pre></td></tr></table></figure></p>
<h2 id="u84DD_u56FE_u89E3_u6790"><a href="#u84DD_u56FE_u89E3_u6790" class="headerlink" title="蓝图解析"></a>蓝图解析</h2><p>现在让我们看一看刚才所用的蓝图的结构：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat blueprint.yaml</span><br></pre></td></tr></table></figure></p>
<p>这就是一个yaml格式的文件，里面都是cloudify的DSL。文件分为以下五个部分：</p>
<ul>
<li>tosca_definitions_version：蓝图的DSL版本，这里是cloudify_dsl_1_2</li>
<li>imports：引用yaml文件的地址</li>
<li>inputs：蓝图的配置信息，也就是一开始初始化蓝图时传入的参数</li>
<li>node_templates：描述了应用的资源以及应用是如何被部署的，可以跟刚才看到的节点实例相对应起来</li>
<li>outputs：输出信息，也就是刚才看到的模型里的内容</li>
</ul>
<p>其中包括了三个内置函数（Intrinsic Functions），分别是<code>get_input</code>，<code>get_property</code>和<code>concat</code>，只能在蓝图里使用。它们的意思也都比较明显，可以从函数名推断出来。所有的内置函数可以在<a href="http://docs.getcloudify.org/3.3.1/blueprints/spec-intrinsic-functions/" target="_blank" rel="external">这里</a>查看到。</p>
<h2 id="u90E8_u7F72_u5BB9_u5668"><a href="#u90E8_u7F72_u5BB9_u5668" class="headerlink" title="部署容器"></a>部署容器</h2><p>Cloudify通过<a href="http://docs.getcloudify.org/3.3.1/plugins/docker/" target="_blank" rel="external">docker插件</a>来支持docker。这个插件依赖于Docker Python API库，而不是Docker CLI，所以体验上有所不同。比如说，<code>docker run</code>将会被分解为<code>docker create</code>和<code>docker start</code>。接下来让我们来尝试部署一个tomcat容器。首先需要生成一个tomcat容器的蓝图：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">mkdir ../docker</span><br><span class="line"><span class="built_in">cd</span> ../docker</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; blueprint.yaml</span><br><span class="line">tosca_definitions_version: cloudify_dsl_1_2</span><br><span class="line">imports:</span><br><span class="line">  - http://www.getcloudify.org/spec/cloudify/<span class="number">3.4</span>m3/types.yaml</span><br><span class="line">  - http://www.getcloudify.org/spec/docker-plugin/<span class="number">1.3</span>.<span class="number">1</span>/plugin.yaml</span><br><span class="line">inputs:</span><br><span class="line">  host_ip:</span><br><span class="line">      description: &gt;</span><br><span class="line">        The ip of the host the application will be deployed on</span><br><span class="line">      default: <span class="number">127.0</span>.<span class="number">0.1</span></span><br><span class="line">  tomcat_container_port_bindings:</span><br><span class="line">    description: &gt;</span><br><span class="line">      A dict of port bindings <span class="keyword">for</span> the node container.</span><br><span class="line">    default:</span><br><span class="line">      <span class="number">8080</span>: <span class="number">8080</span></span><br><span class="line">node_templates:</span><br><span class="line">  host:</span><br><span class="line">    <span class="built_in">type</span>: cloudify.nodes.Compute</span><br><span class="line">    properties:</span><br><span class="line">      install_agent: <span class="literal">false</span></span><br><span class="line">      ip: &#123; get_input: host_ip &#125;</span><br><span class="line">  tomcat_container:</span><br><span class="line">    <span class="built_in">type</span>: cloudify.docker.Container</span><br><span class="line">    properties:</span><br><span class="line">      name: tomcat</span><br><span class="line">      image:</span><br><span class="line">        repository: tomcat</span><br><span class="line">        tag: <span class="number">8.0</span>.<span class="number">30</span>-jre8</span><br><span class="line">    interfaces:</span><br><span class="line">      cloudify.interfaces.lifecycle:</span><br><span class="line">        create:</span><br><span class="line">          implementation: docker.docker_plugin.tasks.create_container</span><br><span class="line">          inputs:</span><br><span class="line">            params:</span><br><span class="line">              stdin_open: <span class="literal">true</span></span><br><span class="line">              tty: <span class="literal">true</span></span><br><span class="line">        start:</span><br><span class="line">          implementation: docker.docker_plugin.tasks.start</span><br><span class="line">          inputs:</span><br><span class="line">            params:</span><br><span class="line">              port_bindings: &#123; get_input: tomcat_container_port_bindings &#125;</span><br><span class="line">    relationships:</span><br><span class="line">      - <span class="built_in">type</span>: cloudify.relationships.contained_<span class="keyword">in</span></span><br><span class="line">        target: host</span><br><span class="line">outputs:</span><br><span class="line">  http_endpoint:</span><br><span class="line">    description: Tomcat web server endpoint</span><br><span class="line">    value: &#123; <span class="string">'http://localhost:8080'</span> &#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>这个蓝图需要docker的插件，所以必须先安装一下，然后就可以初始化蓝图（这次不传参数，使用默认值）：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> create-requirements -o requirements.txt -p blueprint.yaml</span><br><span class="line">sudo pip install -r requirements.txt</span><br><span class="line">cfy <span class="built_in">local</span> init -p blueprint.yaml</span><br></pre></td></tr></table></figure></p>
<p>现在可以运行啦。由于第一次运行需要下载镜像，可能会比较慢：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> execute -w install</span><br><span class="line">docker ps</span><br><span class="line">docker images</span><br></pre></td></tr></table></figure></p>
<p>总算是可以访问了：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl localhost:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<p>也可以在启动vagrant虚拟机的主机上访问：<a href="http://192.168.33.18:8080" target="_blank" rel="external">http://192.168.33.18:8080</a>。查看运行参数和节点实例：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> outputs</span><br><span class="line">cfy <span class="built_in">local</span> instances</span><br></pre></td></tr></table></figure></p>
<p>可以用以下命令来结束部署：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> execute -w uninstall</span><br><span class="line">docker ps <span class="operator">-a</span></span><br></pre></td></tr></table></figure></p>
<p>看起来容器会被删除。感觉怎么样？你愿意天天这样来部署docker么？</p>
<h2 id="Cloudify_u7BA1_u7406_u5668"><a href="#Cloudify_u7BA1_u7406_u5668" class="headerlink" title="Cloudify管理器"></a>Cloudify管理器</h2><p>除了命令行以外，cloudify也支持使用管理器来部署应用。Cloudify管理器有自己的用户界面，提供历史记录、授权和鉴权等功能，并且支持并行运行工作流。下面我们来试着安装一个cloudify管理器。启动cloudify管理器就像是启动一个普通的蓝图一样。可是安装需要下载一大堆的依赖，比较繁琐，有兴趣的童鞋可以参考<a href="http://docs.getcloudify.org/3.3.1/manager/bootstrapping/" target="_blank" rel="external">官方教程</a>。官方另外还提供了一个<a href="http://docs.getcloudify.org/3.3.1/manager/getting-started/" target="_blank" rel="external">vagrant镜像</a>，里面已经配置好了整个Cloudify管理器，因为我们启动vagrant的时候就已经导入了，直接用它更方便。只要虚拟机启动起来（按照本教程的话，现在是起来的状态），可以直接访问<a href="http://192.168.33.17/" target="_blank" rel="external">http://192.168.33.17/</a>来打开cloudify管理器的页面了：<br><img src="/img/cloudify-manager-blueprint.jpg" alt=""></p>
<p>接下来我们来上传一个官方的蓝图，这是nodejs调用mongodb的应用：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> blueprints</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/cloudify-cosmo/cloudify-nodecellar-example</span><br><span class="line"><span class="built_in">cd</span> cloudify-nodecellar-example/</span><br><span class="line">git checkout tags/<span class="number">3.3</span></span><br><span class="line">cfy blueprints upload -b nodecellar -p simple-blueprint.yaml</span><br></pre></td></tr></table></figure></p>
<p><code>-b</code>参数的nodecellar是这个蓝图的名字。刷新蓝图的界面，我们就能看到一个名为nodecellar的蓝图。点击进去，还能看到更详细的拓扑结构、节点信息等。甚至还可以点击图上的各个组件查看详细信息：<br><img src="/img/cloudify-blueprint-topology.jpg" alt=""></p>
<p>这里有4个节点：</p>
<ol>
<li>host：部署的主机</li>
<li>mongod：mongoDB，运行在host上</li>
<li>nodejs：nodejs服务器，运行在host上</li>
<li>nodec…：显示不下的nodecellar，也就是这个酒窖应用，运行在nodejs服务器上，它会去访问mongoDB</li>
</ol>
<p>接下来让我们生成一个部署对象：<br><figure class="highlight"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &#39;s/host_ip: .*/host_ip: 192.168.33.17/&#39; ../inputs/nodecellar-singlehost.yaml&#10;cfy deployments create -b nodecellar -d nodecellar --inputs ../inputs/nodecellar-singlehost.yaml</span><br></pre></td></tr></table></figure></p>
<p><code>-d</code>参数的nodecellar是这个部署的ID。页面上点击左边的Deployments，我们就能看到ID为nodecellar的部署了。而Logs &amp; Events里面也生成了好几页日志和事件。与此同时，最左下的Nodes也出现了4条记录。接下来，真正地开始部署：<br><figure class="highlight"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy executions start -w install -d nodecellar</span><br></pre></td></tr></table></figure></p>
<p>部署需要一些时间，在笔者的mac上大约5分钟。这时如果刷新部署页面，就能看到Action显示Install，旁边还有一个<code>×</code>号，可以通过点击它来取消本次部署。点击部署页面上nodecellar的ID，就能看到一系列详细信息，甚至还有监控：<br><img src="/img/cloudify-deployments-monitoring.jpg" alt=""></p>
<p>部署完成后，就可以直接访问<a href="http://192.168.33.17:8080/" target="_blank" rel="external">http://192.168.33.17:8080/</a>来打开这个nodejs酒窖的网站了：<br><img src="/img/cloudify-node-cellar.jpg" alt=""></p>
<p>还可以用以下命令来停止nodecellar的部署，并删除这个部署：<br><figure class="highlight"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy executions start -w uninstall -d nodecellar&#10;cfy deployments delete -d nodecellar</span><br></pre></td></tr></table></figure></p>
<p>值得一提的是，刚才我们输入的命令，都可以通过cloudify manager的界面来操作。如果需要停止cloudify manager，可以用以下命令：<br><figure class="highlight"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy teardown -f</span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://getcloudify.org/">Cloudify</a>是一个开源的云应用编排系统，它允许使用DSL来描述应用的拓扑结构，并部署到任意环境中。本文大量参考了<a href="http://docs.getcloudify.org/3.3.1/intro/what-is-cloudify/">官方教程</a>从零开始搭建并管理一个cloudify 3.3.1集群。<br>]]>
    
    </summary>
    
      <category term="cloudify" scheme="http://qinghua.github.io/tags/cloudify/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[轻松搭建Kubernetes 1.2版运行环境]]></title>
    <link href="http://qinghua.github.io/kubernetes-installation/"/>
    <id>http://qinghua.github.io/kubernetes-installation/</id>
    <published>2016-03-22T13:14:31.000Z</published>
    <updated>2016-04-26T08:16:28.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://kubernetes.io/docs/whatisk8s/" target="_blank" rel="external">Kubernetes</a>简称k8s，是谷歌于2014年开始主导的开源项目，提供了以容器为中心的部署、伸缩和运维平台。截止目前它的最新版本为1.2。搭建环境之前建议先了解一下kubernetes的相关知识，可以参考<a href="/kubernetes-in-mesos-1">《如果有10000台机器，你想怎么玩？》</a>系列文章。本文从零开始搭建一个kubernetes集群。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配三台虚拟机，一台叫做<strong>master</strong>，它的IP是<strong>192.168.33.17</strong>；另两台叫做<strong>node1</strong>和<strong>node2</strong>，它们的IP是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"master"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"master"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">1024</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"node1"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"node1"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">2048</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"node2"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"node2"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">2048</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后分别在三个终端运行以下命令启动并连接三台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh master</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh node1</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh node2</span><br></pre></td></tr></table></figure>
<p>这个vagrant镜像默认的docker版本为1.9.0，如果你愿意，可以用下面的命令将其升级为1.10.3，但这不是必须的：<br><figure class="highlight sh"><figcaption><span>all or none</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:<span class="number">80</span> --recv-keys <span class="number">58118</span>E89F3A912897C070ADBF76221572C52609D</span><br><span class="line">sudo sh -c <span class="string">"echo deb https://apt.dockerproject.org/repo ubuntu-trusty main &gt; /etc/apt/sources.list.d/docker.list"</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get purge lxc-docker</span><br><span class="line">sudo apt-cache policy docker-engine</span><br><span class="line">sudo apt-get install docker-engine</span><br><span class="line">sudo service docker restart</span><br><span class="line">docker -v</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u7F51_u7EDC_u73AF_u5883"><a href="#u642D_u5EFA_u7F51_u7EDC_u73AF_u5883" class="headerlink" title="搭建网络环境"></a>搭建网络环境</h2><p>为了打通不同主机上的容器的网络连接，最简单的方法是安装一个覆盖网络，这里我们使用flannel。它使用etcd来配置，所以我们需要先运行一个etcd实例。下面在master虚拟机上用容器运行一个etcd实例：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --net=host \</span><br><span class="line">  --restart=always \</span><br><span class="line">  --name=etcd \</span><br><span class="line">  -v /var/etcd/data:/var/etcd/data \</span><br><span class="line">  kubernetes/etcd:<span class="number">2.0</span>.<span class="number">5</span> \</span><br><span class="line">  /usr/<span class="built_in">local</span>/bin/etcd \</span><br><span class="line">  --addr=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">4001</span> \</span><br><span class="line">  --bind-addr=<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">4001</span> \</span><br><span class="line">  --data-dir=/var/etcd/data</span><br></pre></td></tr></table></figure></p>
<p>接下来往etcd里插入flannel的配置数据。这里指定flannel可以使用的IP地址为<code>10.0.0.0/8</code>区间：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it etcd etcdctl <span class="built_in">set</span> /qinghua.github.io/network/config <span class="string">'&#123;"Network": "10.0.0.0/8"&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>然后安装并在后台运行flannel：<br><figure class="highlight sh"><figcaption><span>master node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -c https://github.com/coreos/flannel/releases/download/v0.<span class="number">5.5</span>/flannel-<span class="number">0.5</span>.<span class="number">5</span>-linux-amd64.tar.gz</span><br><span class="line">tar zxvf flannel-<span class="number">0.5</span>.<span class="number">5</span>-linux-amd64.tar.gz</span><br><span class="line">sudo flannel-<span class="number">0.5</span>.<span class="number">5</span>/flanneld --etcd-endpoints=http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">4001</span> --etcd-prefix=/qinghua.github.io/network --iface=eth1 &gt; flannel.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line">cat flannel.log</span><br></pre></td></tr></table></figure></p>
<p>Flannel启动完成后，会获得一个可用于分配的IP集合，并存放到<code>/run/flannel/subnet.env</code>里。我们需要配置一下docker的可用IP为可用于分配的IP：<br><figure class="highlight sh"><figcaption><span>master node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /run/flannel/subnet.env</span><br><span class="line">sudo sh -c <span class="string">"echo DOCKER_OPTS=\\\"--bip=<span class="variable">$FLANNEL_SUBNET</span> --mtu=<span class="variable">$FLANNEL_MTU</span>\\\" &gt;&gt; /etc/default/docker"</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFAk8s_u73AF_u5883"><a href="#u642D_u5EFAk8s_u73AF_u5883" class="headerlink" title="搭建k8s环境"></a>搭建k8s环境</h2><p>终于轮到k8s啦。首先需要下载并解压kubernetes 1.2.0版：<br><figure class="highlight sh"><figcaption><span>master node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget -c https://github.com/kubernetes/kubernetes/releases/download/v1.<span class="number">2.0</span>/kubernetes.tar.gz</span><br><span class="line">tar zxvf kubernetes.tar.gz</span><br><span class="line">tar zxvf kubernetes/server/kubernetes-server-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>解压出来的文件里面含了一些启动master需要的docker镜像文件，将它们导入：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker load -i kubernetes/server/bin/kube-apiserver.tar</span><br><span class="line">docker load -i kubernetes/server/bin/kube-controller-manager.tar </span><br><span class="line">docker load -i kubernetes/server/bin/kube-scheduler.tar</span><br><span class="line">docker images</span><br></pre></td></tr></table></figure></p>
<p>有条件科学上网的童鞋可以自行准备<code>gcr.io/google_containers/etcd:2.2.1</code>这个镜像，否则就凑合着使用先前的<code>kubernetes/etcd:2.0.5</code>。注意，这里为了简单起见，使用同一套etcd。真实环境里，flannel和kubernetes使用的etcd是分开的。接下来开始启动api server：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=apiserver \</span><br><span class="line">  --net=host \</span><br><span class="line">  gcr.io/google_containers/kube-apiserver:e68c6af15d4672feef7022e94ee4d9af \</span><br><span class="line">  kube-apiserver \</span><br><span class="line">  --insecure-bind-address=<span class="number">192.168</span>.<span class="number">33.17</span> \</span><br><span class="line">  --service-cluster-ip-range=<span class="number">11.0</span>.<span class="number">0.0</span>/<span class="number">16</span> \</span><br><span class="line">  --etcd-servers=http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">4001</span></span><br></pre></td></tr></table></figure></p>
<p>然后是controller manager：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=cm \</span><br><span class="line">  gcr.io/google_containers/kube-controller-manager:b9107c794e0564bf11719dc554213f7b \</span><br><span class="line">  kube-controller-manager \</span><br><span class="line">  --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<p>最后是scheduler：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=scheduler \</span><br><span class="line">  gcr.io/google_containers/kube-scheduler:<span class="number">903</span>b34d5ed7367ec4dddf846675613c9 \</span><br><span class="line">  kube-scheduler \</span><br><span class="line">  --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<p>服务器启动完毕，可以运行以下命令来查看版本，咱们用的是1.2：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> version</span><br></pre></td></tr></table></figure></p>
<p>接下来该客户端了。首先启动kubelet：<br><figure class="highlight sh"><figcaption><span>node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">NODE_IP=`ifconfig eth1 | grep <span class="string">'inet addr:'</span> | cut <span class="operator">-d</span>: <span class="operator">-f</span>2 | cut <span class="operator">-d</span><span class="string">' '</span> <span class="operator">-f</span>1`</span><br><span class="line">sudo kubernetes/server/bin/kubelet --api-servers=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> --node-ip=<span class="variable">$NODE_IP</span> &gt; kubelet.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line">cat kubelet.log</span><br></pre></td></tr></table></figure></p>
<p>Kubelet启动完成后，在master上就可以看到了：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get no</span><br></pre></td></tr></table></figure></p>
<p>最后启动kube-proxy：<br><figure class="highlight sh"><figcaption><span>node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo kubernetes/server/bin/kube-proxy --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> &gt; proxy.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line">cat proxy.log</span><br></pre></td></tr></table></figure></p>
<h2 id="u6D4B_u8BD5k8s_u73AF_u5883"><a href="#u6D4B_u8BD5k8s_u73AF_u5883" class="headerlink" title="测试k8s环境"></a>测试k8s环境</h2><p>环境安装好了，接下来试着启动一个pod。启动之前，由于kubernetes需要通过gcr.io/google_containers/pause:2.0的小镜像来管理pod的网络。它会自动下载，如果没有科学上网导致下载不到，那么可以先用docker hub上的kubernetes/pause来代替：<br><figure class="highlight sh"><figcaption><span>node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker pull kubernetes/pause</span><br><span class="line">docker tag kubernetes/pause gcr.io/google_containers/pause:<span class="number">2.0</span></span><br></pre></td></tr></table></figure></p>
<p>然后就可以用命令行在任意一台虚拟机上运行一个tomcat，并生成服务：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;tomcat.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ReplicationController</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat</span><br><span class="line">spec:</span><br><span class="line">  replicas: <span class="number">1</span></span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: tomcat</span><br><span class="line">      labels:</span><br><span class="line">        app: tomcat</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: tomcat</span><br><span class="line">        image: tomcat:<span class="number">8.0</span>.<span class="number">30</span>-jre8</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: <span class="number">8080</span></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat</span><br><span class="line">  labels: </span><br><span class="line">    app: tomcat</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat</span><br><span class="line">  ports:</span><br><span class="line">  - port: <span class="number">80</span></span><br><span class="line">    targetPort: <span class="number">8080</span></span><br><span class="line">    nodePort: <span class="number">30088</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> create <span class="operator">-f</span> tomcat.yaml</span><br></pre></td></tr></table></figure></p>
<p>一开始由于需要下载tomcat镜像可能会慢点，随时可以用下面的命令来查看进度：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> describe po tomcat</span><br></pre></td></tr></table></figure></p>
<p>可以用下面的命令来查看pod、replication controller、service和endpoint：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get po</span><br><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get rc</span><br><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get svc</span><br><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get ep</span><br></pre></td></tr></table></figure></p>
<p>我们看到的endpoint里，应该有一个tomcat。在我的虚拟机上它的ENDPOINTS是<code>10.0.8.3:8080</code>，访问一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">POD_IP=`kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get ep tomcat -o jsonpath=&#123;.subsets[*].addresses[*].ip&#125;`</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$POD_IP</span></span><br><span class="line">curl <span class="variable">$POD_IP</span>:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<p>顺利的话，这三台虚拟机任意一台都可以访问这个tomcat的endpoint。由于启动这三台vagrant虚拟机的主机上并没有安装flannel，所以目前就别想用主机的浏览器打开这个网址啦。但是，由于我们创建服务的时候类型设置为NodePort，这样外部是可以通过任意node的特定端口访问这个服务的。也就是说，下面这两个url都是可以在集群外部访问的，并且效果一样：</p>
<ul>
<li><a href="http://192.168.33.18:30088/" target="_blank" rel="external">http://192.168.33.18:30088/</a></li>
<li><a href="http://192.168.33.19:30088/" target="_blank" rel="external">http://192.168.33.19:30088/</a></li>
</ul>
<p>初步测试完毕，可以使用以下命令来删除刚才创建的tomcat系列对象：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> delete <span class="operator">-f</span> tomcat.yaml</span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://kubernetes.io/docs/whatisk8s/">Kubernetes</a>简称k8s，是谷歌于2014年开始主导的开源项目，提供了以容器为中心的部署、伸缩和运维平台。截止目前它的最新版本为1.2。搭建环境之前建议先了解一下kubernetes的相关知识，可以参考<a href="/kubernetes-in-mesos-1">《如果有10000台机器，你想怎么玩？》</a>系列文章。本文从零开始搭建一个kubernetes集群。<br>]]>
    
    </summary>
    
      <category term="docker" scheme="http://qinghua.github.io/tags/docker/"/>
    
      <category term="flannel" scheme="http://qinghua.github.io/tags/flannel/"/>
    
      <category term="kubernetes" scheme="http://qinghua.github.io/tags/kubernetes/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如果有10000台机器，你想怎么玩？（九）安全性]]></title>
    <link href="http://qinghua.github.io/kubernetes-in-mesos-9/"/>
    <id>http://qinghua.github.io/kubernetes-in-mesos-9/</id>
    <published>2016-03-21T11:24:40.000Z</published>
    <updated>2016-03-21T11:31:23.000Z</updated>
    <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的安全性，还有多租户。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a><a id="more"></a>
</li>
</ul>
<h2 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h2><h3 id="u9274_u6743_u548C_u6388_u6743"><a href="#u9274_u6743_u548C_u6388_u6743" class="headerlink" title="鉴权和授权"></a>鉴权和授权</h3><p>身份认证分为鉴权（authentication）和授权（authorization）。前者是看你能不能登录，后者是看你登录后有什么权限。Kubernetes的1.2版支持以下<a href="http://kubernetes.io/docs/admin/authentication/" target="_blank" rel="external">5种鉴权</a>方式：</p>
<ul>
<li>CA认证（Client certificate authentication）：就是基于SSL证书的认证，基本概念可以参考<a href="/certificate">证书的那些事儿</a></li>
<li>Token文件认证（Token File）：用一个CSV文件指定用户名、用户id和组名，组名是为了给下面的授权使用的</li>
<li>OpenID认证（OpenID Connect ID Token）：基于第三方的<a href="http://openid.net/specs/openid-connect-core-1_0.html" target="_blank" rel="external">OpenID</a></li>
<li>基本认证（Basic authentication）：也是CSV文件，指定了用户名、密码和用户id</li>
<li>Keystone认证（Keystone authentication）：基于openstack的身份认证服务<a href="http://docs.openstack.org/developer/keystone/" target="_blank" rel="external">Keystone</a></li>
</ul>
<p>身份验证通过，接下来就是授权。Kubernetes的1.2版支持以下<a href="http://kubernetes.io/docs/admin/authorization/" target="_blank" rel="external">4种授权</a>方式：</p>
<ul>
<li>总是拒绝（AlwaysDeny）：这个一般用于测试</li>
<li>总是允许（AlwaysAllow）：只要能登录进来，就有所有权限</li>
<li>用户配置（ABAC）：基于用户授权配置，可以对资源（比如pod、service等）和命名空间设置用户/组的只读或可写权限，多租户管理的时候很有用</li>
<li>钩子（Webhook）：也能实现类似用户配置的粒度，只不过是基于一个远程的REST服务</li>
</ul>
<h3 id="u8D26_u6237"><a href="#u8D26_u6237" class="headerlink" title="账户"></a>账户</h3><p>用户账户（User account）和<a href="http://kubernetes.io/docs/user-guide/service-accounts/" target="_blank" rel="external">服务账户</a>（Service account）在kubernetes里是不同的两个东西。用户账户是给人使用的，不可重复，目前创建的时候需要重启API Server。而服务账户是给pod里的容器使用的，在不同的命名空间中可以重复，比较轻量级一点，可以动态创建。如果创建pod的时候指定了服务账户，那就没的说，直接用就好了；但要是没有指定，k8s会自动为pod指定一个指定命名空间的名为default的服务账户。它是在创建命名空间时自动生成的。</p>
<h3 id="u79D8_u5BC6"><a href="#u79D8_u5BC6" class="headerlink" title="秘密"></a>秘密</h3><p>想象一个正常的开发场景，比如tomcat容器访问mysql，攻城狮们理所当然地把mysql的密码存放在tomcat容器里。Kubernetes提供了<a href="http://kubernetes.io/docs/user-guide/secrets/" target="_blank" rel="external">秘密</a>（Secret）这个对象用来保存这样的敏感信息。需要用的时候像挂卷一样把秘密挂载到pod里就好了，密码就不需要直接写了。有了秘密，妈妈再也不用担心我的霸气侧漏了。实际上，上面提到的服务账户本质上就是秘密的集合。</p>
<h3 id="u591A_u79DF_u6237"><a href="#u591A_u79DF_u6237" class="headerlink" title="多租户"></a>多租户</h3><p>Kubernetes本身支持用不同的命名空间来区分多租户，它们之间不会相互干扰。还有一个系统使用的命名空间叫做<code>kube-system</code>。但是系统的资源是恒定的，如果有个租户打算扶着墙进来，再扶着墙出去，是不是其他人都没得吃了？Kubernetes有一个<a href="http://kubernetes.io/docs/admin/resource-quota/" target="_blank" rel="external">资源限额</a>（Resource Quota）的概念，可以用于命名空间上。目前可以限制的资源有CPU和内存。除了资源，在pod、rc、服务等的数量上也可以进行限制。这样就能很方便地像现在的CaaS那样卖实例吧。</p>
<h2 id="Mesos"><a href="#Mesos" class="headerlink" title="Mesos"></a>Mesos</h2><p>Mesos默认使用<a href="https://en.wikipedia.org/wiki/CRAM-MD5" target="_blank" rel="external">CRAM-MD5</a><a href="http://mesos.apache.org/documentation/latest/authentication/" target="_blank" rel="external">鉴权</a>。最典型的用法是master启动的时候，指定一个文件，里面含有用户名（mesos里叫principal）和密码（mesos里叫secret）。只有通过鉴权的framework和slave才能注册进来。<a href="http://mesos.apache.org/documentation/latest/authorization/" target="_blank" rel="external">授权</a>是通过ACLs（Access Control Lists）来实现的，目前可以支持以下十种行为的授权：</p>
<ul>
<li>register_frameworks: 注册framework</li>
<li>run_tasks: 运行任务</li>
<li>teardown_frameworks: 解除framework</li>
<li>set_quotas: 设置配额</li>
<li>remove_quotas: 移除配额</li>
<li>reserve_resources: 保留资源</li>
<li>unreserve_resources: 解除保留资源</li>
<li>create_volumes: 创建持久化卷</li>
<li>destroy_volumes: 删除持久化卷</li>
<li>update_weights: 更新权重</li>
</ul>
<p>Kubernetes的这些多租户的概念，mesos基本上也都有。就是在名称和具体功效上略有差别。比如，mesos的<a href="http://mesos.apache.org/documentation/latest/roles/" target="_blank" rel="external">角色</a>（role）就像是k8s的命名空间一样，指定role的framework只能使用特定role的mesos slave。在资源限额上，mesos支持<a href="http://mesos.apache.org/documentation/latest/quota/" target="_blank" rel="external">配额</a>和<a href="http://mesos.apache.org/documentation/latest/weights/" target="_blank" rel="external">权重</a>（Weights）。配额管的是保留资源以供未来使用，权重管的是角色分配的资源比例。可惜的是对角色和权重的修改都必须重新启动mesos master，而配额可以通过http请求动态修改。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这次聊聊mesos+k8s的安全性，还有多租户。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a>]]>
    
    </summary>
    
      <category term="kubernetes" scheme="http://qinghua.github.io/tags/kubernetes/"/>
    
      <category term="mesos" scheme="http://qinghua.github.io/tags/mesos/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[证书的那些事儿]]></title>
    <link href="http://qinghua.github.io/certificate/"/>
    <id>http://qinghua.github.io/certificate/</id>
    <published>2016-03-16T14:19:31.000Z</published>
    <updated>2016-03-19T07:18:32.000Z</updated>
    <content type="html"><![CDATA[<p>现在网络这么发达，许多人都在上面购物、理财、预约挂号…网络上传送的可都是自己的重要资料，比如身份证号，信用卡密码等。因特网如何才能保证这些敏感信息的安全？本文试着来探讨一下加密、证书等那些事儿。<br><a id="more"></a></p>
<h2 id="u5386_u53F2"><a href="#u5386_u53F2" class="headerlink" title="历史"></a>历史</h2><h3 id="u9690_u85CF_u4FE1_u606F"><a href="#u9690_u85CF_u4FE1_u606F" class="headerlink" title="隐藏信息"></a>隐藏信息</h3><p>古代交通不便，一般都是通过送信的方式传递信息。那么如何保证信件的内容不被泄露呢？一种方法是隐藏字迹。中国古代使用矾书，也就是用明矾水来书写保密书信。水干之后没有任何痕迹，泡水时字才显示。还有一招是使用淀粉水在纸上写字，再把纸泡在碘水中显示字迹。据知乎的水波说<a href="https://www.zhihu.com/question/20986883/answer/16811680" target="_blank" rel="external">在西方世界里，也有使用牛奶或者羊奶书写信息，待干掉以后再用高温烘烤使之重新显现字迹的说法</a>。这些办法一旦为人所知，便会轻易被破解，而且还有点儿此地无银三百两的意思。</p>
<h3 id="u52A0_u5BC6_u4FE1_u606F"><a href="#u52A0_u5BC6_u4FE1_u606F" class="headerlink" title="加密信息"></a>加密信息</h3><p>除了隐藏信息以外，还有加密的方法。在宋代兵书《武经总要》里，<a href="https://www.zhihu.com/question/34846340/answer/60080691" target="_blank" rel="external">约定了40个常用的军事短语</a>，送信内容为一首40个字的五言律诗，每个字代表一个军事短语。然后在相应的字上做标记，对方就明白了，这个叫字验。这个就有点儿密码表的意思了。从知乎某匿名用户的回答上找了一张图：<br><img src="/img/ziyan.jpg" alt=""></p>
<p>在西方世界里，古希腊军队将长条羊皮纸缠绕在约定长度和粗细的木棍上书写，木棍称为Scytale。把羊皮纸解下来后就变成没有意义的字母了。古罗马的凯撒大帝用的是字母移位的办法。比如有封信写着：IFMMP，多半我们是不知道啥意思的。要是对方事先说明了把每个字母都右移一位的方法，也就是A变成B，B变成C…Z变成A，那我们就能很轻松地把IFMMP变成HELLO，明白对方的意思。</p>
<h2 id="u73B0_u4EE3"><a href="#u73B0_u4EE3" class="headerlink" title="现代"></a>现代</h2><h3 id="u5BF9_u79F0_u52A0_u5BC6"><a href="#u5BF9_u79F0_u52A0_u5BC6" class="headerlink" title="对称加密"></a>对称加密</h3><p>对上文说的凯撒大帝移位法而言，“右移”就是算法（algorithm），一位的“1”就是密钥（key）。计算机普及后，最早的有影响力的算法是<a href="https://en.wikipedia.org/wiki/Data_Encryption_Standard" target="_blank" rel="external">DES</a>（Data Encryption Standard），它的秘钥是64位，但只有56位会用来计算。所以，只要最多尝试2<sup>56</sup>（大约是七万亿）次的暴力破解，就能解密。随着计算机硬件的发展，这个数量级在1998年只要56小时就能破解，到了1999年变成了24小时以内。这样就不够安全了。于是四年之后，<a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard" target="_blank" rel="external">AES</a>（Advanced Encryption Standard）取代了DES成为了新的标准。它可以使用128、192或256位密钥。最低的128位也能承受大约三千万亿亿次的暴力破解，至少目前看起来还算是比较安全的。在DES向AES的过渡期间，使用了<a href="https://en.wikipedia.org/wiki/Triple_DES" target="_blank" rel="external">3DES</a>算法。所谓3DES，可以理解成重要的事情做三遍，用不同密钥的DES加密三次，几乎也就等同于56×3=168位的密钥，这样也算是比较安全了。</p>
<h3 id="u975E_u5BF9_u79F0_u52A0_u5BC6"><a href="#u975E_u5BF9_u79F0_u52A0_u5BC6" class="headerlink" title="非对称加密"></a>非对称加密</h3><p>对称密钥有一个问题，就是密钥通过什么渠道来传输。加密算法再好，密钥被偷走了，也无济于事。在这样的背景下，非对称加密问世了。它的密钥包含着一个公钥和一个私钥。私钥顾名思义是只有你自己的电脑才知道的，公钥是公开的。用私钥加密的数据只有用公钥才能解开，同样的，用公钥加密的数据只有用私钥才能解开。假如两台电脑甲和乙相互通信，双方先把自己的公钥告诉对方。当甲给乙发消息时，用乙的公钥来加密，由于只有乙有相对应的私钥，所以只有乙能解密，甲要是忘记了消息内容连自己都解不了，更遑论第三方了。反之亦然。非对称加密最常用的算法是<a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)" target="_blank" rel="external">RSA</a>（发明者Rivest、Shmir和Adleman的姓氏首字母）。它的私钥和公钥是通过至少百位的大质数来生成的。由于在数学上很难计算大整数的因数，所以目前来说是比较安全的。万一哪天数学或<a href="http://www.guokr.com/question/530973/" target="_blank" rel="external">量子计算机</a>上有了突破，这种算法也就不再可靠了。1999年，512位的RSA被成功分解。2009年，768位的RSA也被成功分解。现在通用的1024位密钥可能也不那么可靠了，从安全的角度来说应该升级到2048位或以上。密钥这么大，非对称加密的速度比较慢（与对称加密有千倍的差距）也是可以理解的。所以在实际的使用当中一般用对称加密来加密数据，非对称加密来加密对称加密的密钥。</p>
<h3 id="u6563_u5217_u7B97_u6CD5"><a href="#u6563_u5217_u7B97_u6CD5" class="headerlink" title="散列算法"></a>散列算法</h3><p>好吧，密码破解不了，但是攻击者可以截取加密后的数据包，然后篡改。这样“传位十四皇子”就变成了“传位于四皇子”啦。散列算法堵死了这条路。它能把很长的数据变成固定长度的文本。也称为数据的<a href="https://en.wikipedia.org/w/index.php?title=Message_digest&amp;redirect=no" target="_blank" rel="external">摘要</a>（digest）或<a href="https://en.wikipedia.org/wiki/Fingerprint_(computing)" target="_blank" rel="external">指纹</a>（fingerprint）。相同数据的摘要一定是一样的，不同数据的摘要有可能一样，但是通常不一样。正因如此，散列算法是不可逆的，也就是说不能从数据摘要倒推出数据来。但是它可以用来校验数据是否被更改。如果数据有变化，那么摘要通常都是不一样的，概率取决于散列的长度，越长越不容易相同。配合上文所说的非对称加密，如果我用自己的私钥加密了某个摘要，那它就只有用我的公钥才能解密。这就说明了这个摘要一定是我发出的，因为别人不可能有我的私钥。进而推导出摘要所代表的消息也是由我发出的。这个加了密的摘要称为<a href="https://en.wikipedia.org/wiki/Digital_signature" target="_blank" rel="external">数字签名</a>（Digital signature）。它保证数据的完整性，确定数据的发出者，是具有法律效力的。</p>
<p>原来常用的散列算法有<a href="https://en.wikipedia.org/wiki/MD5" target="_blank" rel="external">MD5</a>和<a href="https://en.wikipedia.org/wiki/SHA-1" target="_blank" rel="external">SHA1</a>，2004年后，山东大学的王小云教授通过碰撞法分别攻破了这两种算法。也就是说，在已知摘要的情况下，可以很快计算出另一个拥有相同摘要的文本，这样就实现了文本篡改。行话叫散列碰撞（Hash Collision）。所以<a href="https://en.wikipedia.org/wiki/National_Security_Agency" target="_blank" rel="external">NSA</a>推出了<a href="https://en.wikipedia.org/wiki/SHA-2" target="_blank" rel="external">SHA2</a>和<a href="https://en.wikipedia.org/wiki/SHA-3" target="_blank" rel="external">SHA3</a>成为了新的标准。虽然MD5和SHA1被破解，也不用特别在意。因为虽然能找到相同摘要的数据，但是篡改者并不能随心所欲地把数据修改成自己想要的样子。</p>
<h2 id="u534F_u8BAE"><a href="#u534F_u8BAE" class="headerlink" title="协议"></a>协议</h2><h3 id="SSL/TLS"><a href="#SSL/TLS" class="headerlink" title="SSL/TLS"></a>SSL/TLS</h3><p>有了加密算法，是不是传输就安全了呢？确实如果严格去用的话，是安全了，但是也麻烦了很多。每次都要跟对方沟通用什么算法，传送密钥，传送摘要…累不累？所以需要有一套协议，大家都遵守这样的协议，就能少掉很多我们不需要太关注的、技术上的事情。这个协议叫做<a href="https://en.wikipedia.org/w/index.php?title=Secure_Sockets_Layer&amp;redirect=no" target="_blank" rel="external">SSL</a>（Secure Sockets Layer）。它所做的事情，主要就是上面说的交换公钥，用对方的公钥加密数据，用自己的私钥解密，还支持MD5用于验证数据的完整性。SSL是网景公司发明的，由于应用广泛，成为了事实上的标准。<a href="https://en.wikipedia.org/wiki/Internet_Engineering_Task_Force" target="_blank" rel="external">IETF</a>（Internet Engineering Task Force）在1999年把SSL 3.0标准化，称为<a href="https://en.wikipedia.org/wiki/Transport_Layer_Security" target="_blank" rel="external">TLS</a>（Transport Layer Security）。除了标准化以外，相对SSL来说TLS也更加安全一些。</p>
<h3 id="HTTP/HTTPS"><a href="#HTTP/HTTPS" class="headerlink" title="HTTP/HTTPS"></a>HTTP/HTTPS</h3><p>因特网发明后，大部分的网站都是用的<a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol" target="_blank" rel="external">HTTP</a>协议。这个协议关心的是如何方便地获取到自己需要的资源，并不关心安全性。如果你的网络被监视（比如你的wifi提供者，运营商等等），对方是可以明文看到你的所有信息的，行话叫嗅探（sniffer）。在HTTP上应用SSL/TLS的协议叫做<a href="https://en.wikipedia.org/wiki/HTTPS" target="_blank" rel="external">HTTPS</a>（HTTP over SSL/TLS）。有了它，我们就还可以像以前的HTTP那样方便地在网上冲浪，而不用太担心安全问题，但不是完全不需要担心，一会儿我们会说到。</p>
<p>顺便说一句，SSL/TLS并不是只能用于HTTP，还可以用于<a href="https://en.wikipedia.org/wiki/File_Transfer_Protocol" target="_blank" rel="external">FTP</a>（File Transfer Protocol）、<a href="https://en.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol" target="_blank" rel="external">SMTP</a>（Simple Mail Transfer Protocol）等一系列应用层协议。</p>
<h2 id="CA_u53CA_u8BC1_u4E66"><a href="#CA_u53CA_u8BC1_u4E66" class="headerlink" title="CA及证书"></a>CA及证书</h2><h3 id="u6570_u5B57_u8BC1_u4E66"><a href="#u6570_u5B57_u8BC1_u4E66" class="headerlink" title="数字证书"></a>数字证书</h3><p>前面说到了HTTPS，为什么有了这么安全的技术我们还是不能高枕无忧呢？这是因为：虽然它能保证我们的信息不被第三方破解，但是它绕不开一个很现实的绕口问题：你怎么知道他就是他声称的那个他？举个栗子：你以为正在访问的网站是某个银行，但是却不知道其实你被钓鱼（phishing）了。对方正等着你输入你的银行卡号和密码，好用它们去真正的银行网站给自己转账。这一切发生得这么自然，你的信息只有钓鱼网站能解密看到，钓鱼网站的信息也只有你能解密看到…就算是你注意到了对方的域名，也有可能因为0和O、1和l傻傻分不清楚或者<a href="https://en.wikipedia.org/wiki/Domain_hijacking" target="_blank" rel="external">域名劫持</a>（domain hijacking）而上当。数字证书就是用来阻止这事儿发生的。虽然我不认识你，但是如果我认识一个很知名的家伙，他肯为你做担保，那我也可以相信你。这个知名的家伙就叫<a href="https://en.wikipedia.org/wiki/Certificate_authority" target="_blank" rel="external">CA</a>（Certificate Authority）。它为通信的双方起了一个中间人的作用。CA的担保就叫<a href="https://en.wikipedia.org/wiki/Public_key_certificate" target="_blank" rel="external">数字证书</a>（Digital Certificate或Public Key Certificate）。这个证书是什么格式，有什么内容呢？这是由<a href="https://en.wikipedia.org/wiki/Public_key_infrastructure" target="_blank" rel="external">PKI</a>（Public Key Infrastructure）标准决定的。常用的标准有<a href="https://en.wikipedia.org/wiki/X.509" target="_blank" rel="external">X.509</a>和<a href="https://en.wikipedia.org/wiki/PKCS_12" target="_blank" rel="external">PKCS #12</a>。它们定义了证书里应该含有签发机构名、证书用户名、有效期、算法、公钥等信息。</p>
<h3 id="u4FE1_u4EFB_u94FE"><a href="#u4FE1_u4EFB_u94FE" class="headerlink" title="信任链"></a>信任链</h3><p>回到刚才的问题：你怎么知道他就是他声称的那个他？我无条件地信任CA，网站又有了CA的数字证书，我就能信任他就是证书里声称的那个他。如果我因为对CA的信任而有了损失，则证书可以用来追究CA的法律责任。如果我们信任A，A信任B，B信任C，那么我们也会信任B和C，这个叫做<a href="https://en.wikipedia.org/wiki/Chain_of_trust" target="_blank" rel="external">证书信任链</a>（Chain of trust）。中间的B称为<a href="https://hk.godaddy.com/en/help/what-is-an-intermediate-certificate-868" target="_blank" rel="external">中级证书</a>（Intermediate certificate），位于信任链顶端的A称为<a href="https://en.wikipedia.org/wiki/Root_certificate" target="_blank" rel="external">根证书</a>（Root certificate）。如果根证书出了问题，那么它所信任的其它证书也就不再可信了。这个后果可是非常严重，可能会影响整个因特网的信任体系。所以，需要尽可能地少动用根证书以减少根证书的私钥被盗用的风险。如果网站想要被认证，直接用中级证书认证就好了。万一中级证书出问题，吊销掉中级证书也只会影响一部分客户，比整个根证书被吊销掉强。但事情也不绝对。对CA来说，公信力就是一切。说个案例：<a href="https://en.wikipedia.org/wiki/China_Internet_Network_Information_Center" target="_blank" rel="external">中国互联网络信息中心</a>（China Internet Network Information Center，CNNIC）是中国的顶级域名<code>.cn</code>和中文域名的注册管理机构。MCS集团用CNNIC签发的中级证书，发行了多个冒充成Google的假证书。于是在2015年4月份，chrome、firefox都宣布不再信任CNNIC的证书。如果你用chrome来打开<a href="https://www.cnnic.net.cn/" target="_blank" rel="external">https://www.cnnic.net.cn/</a>，应该会看到：<br><img src="/img/https_untrust.png" alt=""></p>
<p>而不是：<br><img src="/img/https_trust.png" alt=""></p>
<p>一般来说操作系统和浏览器都会内置一些信任的CA，比较著名的有公信力的CA有Verisign，GeoTrust等。我们打开google的时候，也能点击小锁看到它的证书：<br><img src="/img/google-ca.jpg" alt=""></p>
<p>再点击证书信息就能看到它的证书链。最上面的是GeoTrust的根证书，它包含的内容都在里面写着了，感兴趣的话就自己看看吧：<br><img src="/img/ca1.jpg" alt=""></p>
<p>中间的是谷歌自己的中级证书：<br><img src="/img/ca2.jpg" alt=""></p>
<p>最后是站点自己的证书，这个有效期一般比较短：<br><img src="/img/ca3.jpg" alt=""></p>
<p>证书是可以自签名的，也就是说，你自己作为CA来发行这个证书。当然，这个证书也只有你自己会信任，广大的网友同志们的眼睛是雪亮的，不会无缘无故地信任你的。那如果我是大企业，是不是就值得信任了？谷歌、微软等通常都是可以信任的。12306，你信任它吗？每个人都会有自己的答案吧。打开<a href="https://www.12306.cn/" target="_blank" rel="external">12306</a>看看它的根证书，SRCA（SinoRail Certification Authority）是个什么鬼？这个是中国铁路自己啊。知道为什么首页上总有一个“为保障您顺畅购票，请下载安装根证书”的提示了吧。</p>
<h3 id="u7533_u8BF7_u8BC1_u4E66"><a href="#u7533_u8BF7_u8BC1_u4E66" class="headerlink" title="申请证书"></a>申请证书</h3><p>首先，如果你觉得自己的网站没有什么信息不能公开的，那就没必要费那钱和时间去购买证书。</p>
<p>证书有很多种类型，价格也很不一样，大约一年数千到数万元人民币吧。简单列出几种：</p>
<ul>
<li>通配符证书（Wildcard SSL Certificates）：自己的域名和下一级子域名可以使用。比如<code>google.com</code>和<code>maps.google.com</code>就是域名和二级子域名的关系</li>
<li>多域名证书（Subject Alternative Name SSL Certificates）：不仅限于子域名，不同域名也能使用</li>
<li>增强型证书（Extended Validation SSL Certificate）：总之就是验证流程更麻烦，结果就是可以在地址栏显示公司的名称，增加可信度。就像这样：<br><img src="/img/github-ca.jpg" alt=""></li>
</ul>
<p>申请证书需要给CA提供一个<a href="https://en.wikipedia.org/wiki/Certificate_signing_request" target="_blank" rel="external">CSR</a>（certificate sigining request）文件。通常做法就是通过程序把域名、联系人和公钥等信息都放在这个文件里，发送给某个CA。交费之后，如果CA方面没有问题，就会对CSR文件设置有效期等操作，当然还需要用自己的私钥对证书签名，再发送给用户。最后用户把证书绑定到自己的网站上。</p>
<h2 id="u52A8_u624B_u65F6_u95F4"><a href="#u52A8_u624B_u65F6_u95F4" class="headerlink" title="动手时间"></a>动手时间</h2><h3 id="u52A0_u89E3_u5BC6"><a href="#u52A0_u89E3_u5BC6" class="headerlink" title="加解密"></a>加解密</h3><p><a href="https://en.wikipedia.org/wiki/OpenSSL" target="_blank" rel="external">OpenSSL</a>是SSL/TLS的开源实现，我们就用它来练练手吧。首先，用DES加解密文本：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World! | openssl enc <span class="operator">-e</span> -des <span class="operator">-a</span>    <span class="comment">## 需要输入两次密码，如果你输入的都是1，那么可以用下一条命令解密</span></span><br><span class="line"><span class="built_in">echo</span> U2FsdGVkX19NMXhTRoTNJE0YV+TKcRL0+xzT9UMUN5Y= | openssl enc <span class="operator">-d</span> -des <span class="operator">-a</span></span><br></pre></td></tr></table></figure></p>
<p>其中的<code>-a</code>代表base64编码。还可以多试几次加密，就算文本、密码相同，每次加密的结果也很可能是不一样的。把上面的<code>-des</code>变成<code>-des3</code>或者是<code>-aes-256-cbc</code>就可以自行尝试DES3和AES加解密了。接下来尝试RSA。首先生成RSA的私钥：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openssl genrsa -out private.pem <span class="number">2048</span></span><br><span class="line">cat private.pem</span><br></pre></td></tr></table></figure></p>
<p>PEM表示这是base64编码的密钥，也可以改成DER即二进制的密钥，加上一个<code>-outform DER</code>的参数就行。然后通过私钥生成公钥：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openssl rsa -in private.pem -pubout -out public.pem</span><br><span class="line">cat public.pem</span><br></pre></td></tr></table></figure></p>
<p>接下来用刚刚生成的公钥加密，私钥解密：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World! | openssl rsautl -encrypt -pubin -inkey public.pem -out encrypt</span><br><span class="line">cat encrypt</span><br><span class="line">cat encrypt | openssl rsautl -decrypt -inkey private.pem</span><br></pre></td></tr></table></figure></p>
<p>然后私钥加密，公钥解密。注意这个的意义其实是私钥签名，公钥认证：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World! | openssl rsautl -sign -inkey private.pem -out encrypt</span><br><span class="line">cat encrypt</span><br><span class="line">cat encrypt | openssl rsautl -verify -pubin -inkey public.pem</span><br></pre></td></tr></table></figure></p>
<h3 id="u6563_u5217"><a href="#u6563_u5217" class="headerlink" title="散列"></a>散列</h3><p>现在轮到散列算法了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World! | openssl dgst -md5</span><br><span class="line"><span class="built_in">echo</span> Hello World! | openssl dgst -sha1</span><br></pre></td></tr></table></figure></p>
<p>也可以用linux自带的小工具实现：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World! | md5sum</span><br><span class="line"><span class="built_in">echo</span> Hello World! | sha1sum</span><br><span class="line"><span class="built_in">echo</span> Hello World! | sha224sum</span><br><span class="line"><span class="built_in">echo</span> Hello World! | sha256sum</span><br><span class="line"><span class="built_in">echo</span> Hello World! | sha384sum</span><br><span class="line"><span class="built_in">echo</span> Hello World! | sha512sum</span><br></pre></td></tr></table></figure></p>
<p>是不是越来越长了？散列越长越不容易被碰撞。</p>
<h3 id="u8BC1_u4E66"><a href="#u8BC1_u4E66" class="headerlink" title="证书"></a>证书</h3><p>申请证书，首先需要生成CSR文件。而CSR文件需要先生成私钥：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openssl genrsa -out private.pem <span class="number">2048</span></span><br><span class="line">openssl req -new -key private.pem -out domain.csr</span><br><span class="line">cat domain.csr</span><br></pre></td></tr></table></figure></p>
<p>生成私钥文件是可以加密的，加上一个参数比如<code>-des3</code>就可以了。生成CSR文件的时候需要填写各种信息，没耐心就随便写点什么甚至一路回车也行，反正又不是真的去找CA。如果你真的有需求，<a href="https://support.rackspace.com/how-to/generate-a-csr-with-openssl/#create-a-csr" target="_blank" rel="external">这里有一张表格</a>说明了应该怎么填。填完的东西可以这么看：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl req -noout -text -in domain.csr</span><br></pre></td></tr></table></figure></p>
<p>现在我们假装自己是个CA，有自己的密钥对，然后对刚才提交的CSR文件签名：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openssl genrsa -out private_ca.pem <span class="number">2048</span></span><br><span class="line">openssl x509 -req -days <span class="number">365</span> -in domain.csr -signkey private_ca.pem -out my_domain.crt</span><br><span class="line">cat my_domain.crt</span><br></pre></td></tr></table></figure></p>
<p>大功告成！生成的my_domain.crt就是我们要的证书。由于这个是自签名证书，默认是不被我们的操作系统信任的。如果我们需要增加信任，可以参考<a href="https://briansnelson.com/How_to_add_trusted_root_certificates" target="_blank" rel="external">这里</a>。比如在Ubuntu/Debian里可以这么做：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo cp my_domain.crt /usr/<span class="built_in">local</span>/share/ca-certificates/</span><br><span class="line">sudo update-ca-certificates</span><br></pre></td></tr></table></figure></p>
<p>如果要取消信任，可以这么做：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo rm /usr/<span class="built_in">local</span>/share/ca-certificates/my_domain.crt</span><br><span class="line">sudo update-ca-certificates --fresh</span><br></pre></td></tr></table></figure></p>
<h2 id="u53C2_u8003_u8D44_u6599"><a href="#u53C2_u8003_u8D44_u6599" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://computer.howstuffworks.com/encryption.htm" target="_blank" rel="external">How Encryption Works</a><br><a href="http://victor1980.blog.51cto.com/3664622/1659447" target="_blank" rel="external">有关SSL证书的一些事儿</a><br><a href="http://seanlook.com/2015/01/07/tls-ssl/" target="_blank" rel="external">SSL/TLS原理详解</a><br><a href="http://seanlook.com/2015/01/15/openssl-certificate-encryption/" target="_blank" rel="external">OpenSSL 与 SSL 数字证书概念贴</a><br><a href="https://program-think.blogspot.com/2010/02/introduce-digital-certificate-and-ca.html" target="_blank" rel="external">数字证书及 CA 的扫盲介绍</a><br><a href="https://program-think.blogspot.com/2014/11/https-ssl-tls-1.html" target="_blank" rel="external">扫盲 HTTPS 和 SSL/TLS 协议</a><br><a href="http://www.ruanyifeng.com/blog/2011/08/what_is_a_digital_signature.html" target="_blank" rel="external">数字签名是什么？</a><br><a href="http://boxingp.github.io/blog/2015/04/04/should-cnnic-certificate-to-be-trusted/" target="_blank" rel="external">CNNIC证书值得信任吗？</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>现在网络这么发达，许多人都在上面购物、理财、预约挂号…网络上传送的可都是自己的重要资料，比如身份证号，信用卡密码等。因特网如何才能保证这些敏感信息的安全？本文试着来探讨一下加密、证书等那些事儿。<br>]]>
    
    </summary>
    
      <category term="certficate" scheme="http://qinghua.github.io/tags/certficate/"/>
    
      <category term="encryption" scheme="http://qinghua.github.io/tags/encryption/"/>
    
      <category term="https" scheme="http://qinghua.github.io/tags/https/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[用容器轻松搭建Portus运行环境]]></title>
    <link href="http://qinghua.github.io/portus/"/>
    <id>http://qinghua.github.io/portus/</id>
    <published>2016-03-13T11:02:31.000Z</published>
    <updated>2016-03-30T06:51:55.000Z</updated>
    <content type="html"><![CDATA[<p>Docker官方并没有提供docker registry的用户界面，对权限的控制粒度也比较粗。SUSE的<a href="http://port.us.org/" target="_blank" rel="external">Portus</a>很好地解决了这个问题。除了界面以外，它还提供了更细粒度的权限控制、用户认证等功能。本文尝试从零开始用容器搭建一个portus环境。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配一台IP是<strong>192.168.33.18</strong>的虚拟机。Registry配上portus会比较耗内存，所以我们给它2G内存，默认是512M。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line">config.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">  v.memory = <span class="number">2048</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后在终端运行以下命令启动并连接虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>我们将会把docker registry和portus都安装在同一台虚拟机上。一方面是比较方便，另一方面也避免了<a href="https://github.com/SUSE/Portus/issues/510" target="_blank" rel="external">时钟同步问题</a>。为了启动一个带认证的docker registry，首先要生成自签名证书：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; ssl.conf</span><br><span class="line">[ req ]</span><br><span class="line">prompt             = no</span><br><span class="line">distinguished_name = req_subj</span><br><span class="line">x509_extensions    = x509_ext</span><br><span class="line"></span><br><span class="line">[ req_subj ]</span><br><span class="line">CN = Localhost</span><br><span class="line"></span><br><span class="line">[ x509_ext ]</span><br><span class="line">subjectKeyIdentifier   = <span class="built_in">hash</span></span><br><span class="line">authorityKeyIdentifier = keyid,issuer</span><br><span class="line">basicConstraints       = CA:<span class="literal">true</span></span><br><span class="line">subjectAltName         = @alternate_names</span><br><span class="line"></span><br><span class="line">[ alternate_names ]</span><br><span class="line">DNS.<span class="number">1</span> = localhost</span><br><span class="line">IP.<span class="number">1</span>  = <span class="number">192.168</span>.<span class="number">33.18</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mkdir /certs</span><br><span class="line">sudo sh -c <span class="string">"openssl req -config ssl.conf \</span><br><span class="line">-new -x509 -nodes -sha256 -days 365 -newkey rsa:4096 \</span><br><span class="line">-keyout /certs/server-key.pem -out /certs/server-crt.pem"</span></span><br></pre></td></tr></table></figure></p>
<p>证书生成好了，但是由于这是自签名证书，客户端还需要配置证书文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /etc/docker/certs.d/<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span></span><br><span class="line">sudo cp /certs/server-crt.pem /etc/docker/certs.d/<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/ca.crt</span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<p>接下来生成一个registry的配置文件，里面指定刚才的证书和token方式的认证。认证服务器设置到一会儿要启动的portus去：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; config.yml</span><br><span class="line">version: <span class="number">0.1</span></span><br><span class="line">loglevel: debug</span><br><span class="line">storage:</span><br><span class="line">    cache:</span><br><span class="line">        blobdescriptor: inmemory</span><br><span class="line">    filesystem:</span><br><span class="line">        rootdirectory: /var/lib/registry</span><br><span class="line">    delete:</span><br><span class="line">        enabled: <span class="literal">true</span></span><br><span class="line">http:</span><br><span class="line">    addr: :<span class="number">5000</span></span><br><span class="line">    headers:</span><br><span class="line">        X-Content-Type-Options: [nosniff]</span><br><span class="line">    tls:</span><br><span class="line">        certificate: /certs/server-crt.pem</span><br><span class="line">        key: /certs/server-key.pem</span><br><span class="line">auth:</span><br><span class="line">    token:</span><br><span class="line">        realm: https://<span class="number">192.168</span>.<span class="number">33.18</span>/v2/token</span><br><span class="line">        service: <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span></span><br><span class="line">        issuer: <span class="number">192.168</span>.<span class="number">33.18</span></span><br><span class="line">        rootcertbundle: /certs/server-crt.pem</span><br><span class="line">notifications:</span><br><span class="line">    endpoints:</span><br><span class="line">      - name: portus</span><br><span class="line">        url: https://<span class="number">192.168</span>.<span class="number">33.18</span>/v2/webhooks/events</span><br><span class="line">        timeout: <span class="number">500</span>ms</span><br><span class="line">        threshold: <span class="number">5</span></span><br><span class="line">        backoff: <span class="number">1</span>s</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>然后就可以启动registry容器了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --name registry \</span><br><span class="line">    -p <span class="number">5000</span>:<span class="number">5000</span> \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /var/lib/registry:/var/lib/registry \</span><br><span class="line">    -v /certs:/certs \</span><br><span class="line">    -v `<span class="built_in">pwd</span>`/config.yml:/etc/docker/registry/config.yml \</span><br><span class="line">    registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>Docker registry配置完成后，就该准备portus了。Portus需要一个数据库来存储信息，官方推荐<a href="https://mariadb.org/" target="_blank" rel="external">MariaDB</a>，当然mysql也是没问题的。我们把数据库启动起来：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --name mariadb \</span><br><span class="line">    --net=host \</span><br><span class="line">    --restart=always \</span><br><span class="line">    <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">    <span class="operator">-e</span> TERM=xterm \</span><br><span class="line">    mariadb:<span class="number">10.1</span>.<span class="number">10</span></span><br></pre></td></tr></table></figure></p>
<p>等数据库启动完成，我们连接上去：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mariadb mysql -uroot -p123456</span><br></pre></td></tr></table></figure></p>
<p>为portus创建用户和数据库：<br><figure class="highlight sh"><figcaption><span>sql</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create database portus;</span><br><span class="line">GRANT ALL ON portus.* TO <span class="string">'portus'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'portus'</span>;</span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>万事俱备，让我们来启动portus：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">docker run -it <span class="operator">-d</span> \</span><br><span class="line">    --name portus \</span><br><span class="line">    --net host \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /certs:/certs \</span><br><span class="line">    -v /usr/sbin/update-ca-certificates:/usr/sbin/update-ca-certificates \</span><br><span class="line">    -v /etc/ca-certificates:/etc/ca-certificates \</span><br><span class="line">    --env DB_ADAPTER=mysql2 \</span><br><span class="line">    --env DB_ENCODING=utf8 \</span><br><span class="line">    --env DB_HOST=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    --env DB_PORT=<span class="number">3306</span> \</span><br><span class="line">    --env DB_USERNAME=portus \</span><br><span class="line">    --env DB_PASSWORD=portus \</span><br><span class="line">    --env DB_DATABASE=portus \</span><br><span class="line">    --env RACK_ENV=production \</span><br><span class="line">    --env RAILS_ENV=production \</span><br><span class="line">    --env PUMA_SSL_KEY=/certs/server-key.pem \</span><br><span class="line">    --env PUMA_SSL_CRT=/certs/server-crt.pem \</span><br><span class="line">    --env PUMA_PORT=<span class="number">443</span> \</span><br><span class="line">    --env PUMA_WORKERS=<span class="number">4</span> \</span><br><span class="line">    --env MACHINE_FQDN=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    --env SECRETS_SECRET_KEY_BASE=secret-goes-here \</span><br><span class="line">    --env SECRETS_ENCRYPTION_PRIVATE_KEY_PATH=/certs/server-key.pem \</span><br><span class="line">    --env SECRETS_PORTUS_PASSWORD=portuspw \</span><br><span class="line">    h0tbird/portus:v2.<span class="number">0.2</span>-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>启动完成后，在浏览器打开<code>https://192.168.33.18/</code>，应该会看到证书不被浏览器所信任的提示。无视之，选择继续的话，应该就能看到注册页面啦：<br><img src="/img/portus-sign-up.jpg" alt=""></p>
<h2 id="u6743_u9650_u7BA1_u7406"><a href="#u6743_u9650_u7BA1_u7406" class="headerlink" title="权限管理"></a>权限管理</h2><p>Portus现在只能管理一个私有库。它有一个团队的概念，每一个团队可以有多个命名空间，每个命名空间就是多个镜像的集合。每个团队有三种角色：</p>
<ul>
<li>查看者（Viewer）：只能pull镜像</li>
<li>贡献者（Contributor）：除了pull，还可以push镜像</li>
<li>所有者（Owner）：除了推拉镜像，还可以对团队成员进行管理</li>
</ul>
<p>由于角色是定义在团队里的，所以命名空间就不需要再考虑权限问题了，它只是镜像的集合而已。命名空间也有三种类型：</p>
<ul>
<li>全局（Global）：只有管理员可以push，其他人只能pull</li>
<li>团队（Team）：团队成员可以做自己角色支持的操作</li>
<li>个人（Personal）：只有所有者和管理员可以推拉</li>
</ul>
<p>命名空间还可以设置为public，这样不需要login也能pull。</p>
<p>说完一些基本概念，让我们来尝试一下。首先，portus需要配置一个用户，来调用docker registry的API，与其进行同步。<a href="http://port.us.org/docs/How-to-setup-secure-registry.html#synchronizing-the-registry-and-portus" target="_blank" rel="external">同步</a>有<a href="http://port.us.org/features/1_Synchronizing-the-Registry-and-Portus.html" target="_blank" rel="external">两种方式</a>：一是在docker registry的配置文件里写的<code>notifications</code>，这样每当有人push一个新镜像上去，docker registry将会通知portus修改数据库。可是时间长了，有可能数据库偶尔挂掉或是网络不稳定啥的导致两边数据不一致。Portus针对这种情况也提供了一个crono的job，设置定时运行即可，一会儿我们会试验。现在先让我们来创建这个用户：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> portus bundle <span class="built_in">exec</span> rake portus:create_api_account</span><br></pre></td></tr></table></figure></p>
<p>接下来就可以在注册页面自行注册啦，注册完毕后会跳转到登记registry页面：<br><img src="/img/portus-new-registry.jpg" alt=""></p>
<p>照上图填入registry，点击<strong>Create</strong>按钮创建一个docker registry。接着创建一个新用户。点击左边的<strong>Admin</strong>，再点击中间的<strong>Users</strong>，然后点击右边的<strong>Create new user</strong>，填写用户信息：<br><img src="/img/portus-new-user.jpg" alt=""></p>
<p>点击<strong>Add</strong>按钮就可以创建一个新用户了。接下来创建一个团队。点击左边的<strong>Teams</strong>，再点击右边的<strong>Create new team</strong>，填写团队信息：<br><img src="/img/portus-new-team.jpg" alt=""></p>
<p>点击<strong>Add</strong>按钮就可以创建一个新团队了。点击刚刚创建好的团队，再点击右边的<strong>Add namespace</strong>，填写命名空间信息：<br><img src="/img/portus-new-namespace.jpg" alt=""></p>
<p>点击<strong>Add</strong>按钮就可以创建一个新命名空间了。接下来把用户添加到这个团队中。点击右边的<strong>Add members</strong>，填写刚才增加的用户信息：<br><img src="/img/portus-new-member.jpg" alt=""></p>
<p>点击<strong>Add</strong>按钮就可以把用户加进来了。回到控制台，搞一个镜像，push一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker pull microbox/etcd:<span class="number">2.1</span>.<span class="number">1</span></span><br><span class="line">docker tag microbox/etcd:<span class="number">2.1</span>.<span class="number">1</span> <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/microbox/etcd:<span class="number">2.1</span>.<span class="number">1</span></span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/microbox/etcd:<span class="number">2.1</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>出错了：<strong>unauthorized: authentication required</strong>，我们必须用docker先登录：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u gggg <span class="operator">-e</span> gggg@<span class="number">123</span>.com <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span></span><br></pre></td></tr></table></figure></p>
<p>填上自己刚才设置的密码，登录成功之后，再试着push一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/microbox/etcd:<span class="number">2.1</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>Bingo！换一个命名空间试试看：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker tag h0tbird/portus:v2.<span class="number">0.2</span>-<span class="number">1</span> <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/h0tbird/portus:v2.<span class="number">0.2</span>-<span class="number">1</span></span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/h0tbird/portus:v2.<span class="number">0.2</span>-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>出错了：<strong>unauthorized: authentication required</strong>，可见我们的权限控制确实起作用了。</p>
<h2 id="u955C_u50CF_u540C_u6B65"><a href="#u955C_u50CF_u540C_u6B65" class="headerlink" title="镜像同步"></a>镜像同步</h2><p>接下来我们试试定时同步任务。首先需要在容器里信任我们的自签名证书：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> portus mkdir /usr/<span class="built_in">local</span>/share/ca-certificates</span><br><span class="line">docker cp /certs/server-crt.pem portus:/usr/<span class="built_in">local</span>/share/ca-certificates/ca.crt</span><br><span class="line">docker <span class="built_in">exec</span> portus update-ca-certificates</span><br></pre></td></tr></table></figure></p>
<p>然后启动定时同步任务，设置为每10秒钟同步一次：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it portus bash</span><br><span class="line">RAILS_ENV=production CATALOG_CRON=<span class="string">"10.seconds"</span> bundle <span class="built_in">exec</span> crono</span><br></pre></td></tr></table></figure></p>
<p>等十秒钟，就会看到<strong>[catalog] Created the tag ‘2.1.1’</strong>的提示。如果先前没有信任自签名证书，同步的时候会报<strong>certificate verify failed</strong>的错误。现在回到portus的界面，点击左边的<strong>Dashboard</strong>，就能看到刚才push的microbox/etcd镜像已经显示在右边了：<br><img src="/img/portus-dashboard.jpg" alt=""></p>
<p>最后一步就是自动同步了，先把刚才的crono给Ctrl+C掉，Ctrl+D退出portus容器。由于docker registry需要调用portus的API，所以我们需要在registry容器里也信任这个证书：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker cp /certs/server-crt.pem registry:/usr/<span class="built_in">local</span>/share/ca-certificates/ca.crt</span><br><span class="line">docker <span class="built_in">exec</span> registry update-ca-certificates</span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<p>然后再push一个镜像：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker tag registry:<span class="number">2.3</span>.<span class="number">0</span> <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/microbox/registry:<span class="number">2.3</span>.<span class="number">0</span></span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/microbox/registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>到portus的dashboard刷新一下，搞定！在中间的<strong>Recent activities</strong>还能看到是谁push的这个镜像，对审计、追踪来说很有帮助。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Docker官方并没有提供docker registry的用户界面，对权限的控制粒度也比较粗。SUSE的<a href="http://port.us.org/">Portus</a>很好地解决了这个问题。除了界面以外，它还提供了更细粒度的权限控制、用户认证等功能。本文尝试从零开始用容器搭建一个portus环境。<br>]]>
    
    </summary>
    
      <category term="docker registry" scheme="http://qinghua.github.io/tags/docker-registry/"/>
    
      <category term="portus" scheme="http://qinghua.github.io/tags/portus/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HTC VIVE初体验]]></title>
    <link href="http://qinghua.github.io/vr-htc-vive/"/>
    <id>http://qinghua.github.io/vr-htc-vive/</id>
    <published>2016-03-12T13:39:10.000Z</published>
    <updated>2016-03-12T15:37:30.000Z</updated>
    <content type="html"><![CDATA[<p>没想到到深圳来出差，竟然碰上了<a href="https://bbs.htc.com/cn/news.php?mod=viewthread&amp;tid=68841" target="_blank" rel="external">HTC Vive中国开发者峰会</a>。虽然我不是VR开发者，但怎么说也是一名消费者，借着周末赶紧去体验了一把。趁着现在热乎劲还没过去，写一下我的一些见闻及感受吧。<br><a id="more"></a></p>
<h2 id="u65E5_u7A0B"><a href="#u65E5_u7A0B" class="headerlink" title="日程"></a>日程</h2><p>3月9~11日是开发者峰会，11~18日的每天10:00~18:00是公众开放体验时间，可以在深圳大学附近的威盛科技大厦29层参与免费体验。一共有8个项目，分为A、B两组，每组4个项目。一次排队只能排一个组，每人有十分钟的时间，可以自己选择两个项目来体验。A组有蓝色世界、空间画刷、神秘商店和太空海盗，较B组来说会稍微刺激一些。B组有机器人修理、亚利桑那阳光、未来办公室和预算消减（真个怪名字）。具体项目内容下文会详细叙述。</p>
<h2 id="u4F53_u9A8C"><a href="#u4F53_u9A8C" class="headerlink" title="体验"></a>体验</h2><p>Vive包含一个头戴设备（含耳机），两个手柄和两个基站。玩之前先坐在椅子上，首先工作人员会帮忙戴上头戴设备及耳机。Vive是可以把眼镜一起套在设备里的。第一次体验的时候，戴得比较正，几乎感觉不出来里面还有个我自己的眼镜。第二次的工作人员就有点儿不那么专业，戴着不那么舒服，也影响了效果。可见姿势很重要。这时候眼前看到的是一片白色的背景，还有很多经纬线。往下看的话，有很多同心圆，中心就是自己所在的位置。如果转向了，可以根据脚下的显示回到中心。这感觉像是Vive的操作系统。上下左右前后都是有内容的，都可以扭头、转身去看。但是光转动眼珠子是不行的哦，因为Vive是根据陀螺仪来判断方位的。底下稍稍有点儿漏光，不过不注意的话也不太能感觉出来。这时旁边的工作人员会帮助选择要体验的项目，教玩家一些基本用法。之后工作人员把手柄递给我。这时我眼前看到的是两个悬空的手柄在等着我伸手去够。在没有准备的时候可能会吓一小跳。手柄下面有扳机，两侧有按钮，大拇指可以够着正面，有一个圆形的触摸板，也可以当作按钮按下。然后工作人员让我站起来，我可以自由移动了。如果快要碰到实体墙的话，眼镜里会有虚拟的透明墙出现，这时就知道不要再过去了，不然要碰鼻子滴。</p>
<p>很快工作人员用电脑帮我调到第一个项目：亚利桑那阳光。这是个打僵尸的游戏，那两个带扳机的手柄就是我的手枪，按下触摸板可以装子弹。工作人员提醒我向左边看，我一扭头，原来是几个玻璃瓶。用手枪瞄准玻璃瓶，把它们都打碎，游戏也就正式开始了。手枪带有激光瞄准，准心处是有红点的，所以知道子弹会射向哪里，但是因为手总是不可能完全静止，会有轻微的抖动，所以射击出来也不是那么的准，需要适应一下。画面随即切换到了户外，估计是亚利桑那洲吧，有几个僵尸向我冲过来。用刚才的手枪把僵尸们都干掉吧。随着剧情的深入，可以拿到更高级的武器，打起来也就更爽了。可惜手柄没有震动反馈，不然加上后坐力一定更能带来更深的沉浸感。如果仔细观看，还是有一些颗粒感的。</p>
<p>五分钟很快就结束了，接下来是未来办公室项目。这是一个虚拟的办公室，工作人员会引导我用手柄拿杯子，接咖啡，开抽屉，拿文件，开电脑什么的。中间咖啡杯没拿稳还摔地上了，可惜了一整杯咖啡 ：） 吐个槽，怎么未来办公室的电脑屏幕还是CRT的呢，也太不和谐了。</p>
<p>B组的两个项目体验完了，我就到A组去，这回排了一个半小时。听说这还是快的了，去年12月在北京的活动，都需要排3个小时队。接下来我选的项目是神秘商店。我在一个商店里，寻找白色的图腾。每找到一个，商店就会把我传送到另一个地方去。可能出现各种奇怪的东西，比如一个盒子里蹦出来一个弹簧人，吓了我一跳。可以拿手上的手柄（这次是魔法杖）狠狠地抽它。</p>
<p>然后是太空海盗。这也是一个射击游戏。两个手柄一个是枪，一个是盾。海盗们源源不断地乘小飞机逼近并射击我，需要用盾挡住他们的进攻并用枪把他们击落。由于这个比较立体，不像上面打僵尸那个游戏，僵尸只会在地面跑，所以难度还是会大一点的。</p>
<p>A组也体验了两个项目，接下来继续回到B组。中午了，排队的人少了一些，这回只用了一个小时就排到了。首先是预算消减。这是一个密室逃脱类游戏，由于空间限制，不可能无限制地走动，所以是通过手柄来移动的。剧情上，大约就是费劲心思搞到一把刀，然后用它去捅某个怪物…我比较愚钝一点，反倒是被怪物给捅了…</p>
<p>下一个是机器人修理。一开始先是开抽屉等基本动作，然后大门打开，有个有问题的机器人过来，让我帮忙修理它。也不知道是我水平不行还是程序设计如此，总之没有修好，然后地板自动翻开，机器人变成一堆零件掉了下去。虽然知道实际上只是普通的地板而已，但是看着脚下的深渊，还是不由的让人感叹如此深的沉浸感。</p>
<p>本想就此结束体验，但是听说剩下的A组两个项目都是最好玩的…于是咬咬牙又等了一个半小时。这回是蓝色世界。我在一艘沉船上，看着周围的鱼儿游来游去。后来来了一条大鲸鱼，细节之处看得非常清楚。最后鲸鱼游走，大尾巴打过来，那种感觉还是很震撼的。</p>
<p>最后一个项目是google开发的空间画刷。一支手柄是菜单，另一支是画笔。只要旋转菜单手柄就可以切换菜单，再用画笔手柄来选择。中间工作人员会帮助切换不同的背景，比如雪世界等等。而画笔也能画出不同的效果，比如火焰般的燃烧效果。而这一切都是在三维空间上存在着的，于是乎就有了神笔马良般的感觉。Google出品，果然不一般。可惜啊，体验时间很快又结束了。就这么的一天过去了。不过一个项目也没落下，还是很值得的。</p>
<h2 id="u884C_u4E1A_u73B0_u72B6"><a href="#u884C_u4E1A_u73B0_u72B6" class="headerlink" title="行业现状"></a>行业现状</h2><p>现在的现实模拟技术主要分为两类：VR（Virtual Reality）虚拟现实和AR（Augmented Reality）增强现实。前者就像玩游戏那样，一切都是假的。后者是在现实中增加虚拟的物体，代表产品有谷歌眼镜和微软的HoloLens。VR贵则数千，便宜则一两百甚至几十。高级的代表产品有Oculus Rift（这就是被facebook以20亿刀收购的公司开发的产品），PS VR（还未发售，据说快了）和HTC Vive（最贵，但效果也最好，支持几平米级别的移动）。为了减少头晕现象，需要提高刷新率至90Hz，降低延迟至15ms。除了PS VR有PS4以外，另外两款都需要配合高性能PC（至少970以上显卡才能玩转，整机估计得上万元了）才能跑得动，这也有点儿限制了高端VR的普及。便宜的VR以三星Gear VR、暴风魔镜等为代表，不像高端VR那样自带屏幕，一般就是透镜加手机。甚至谷歌搞出一个Cardboard，就是纸盒加透镜。由于大众手机分辨率目前一般是1920×1080，分成左右眼的两个屏幕后就变成960×1080，如果看片的话，由于影片的宽高比，1080还得往下降到540左右，所以导致颗粒感比较明显。如果配上更高的分辨率和源文件，那效果就会更好一些。当然这个价格的话，体验体验还是可以的哈。其实还有一个混合现实（Mixed Reality），它包括增强现实和增强虚拟，在新的可视化环境里物理和数字对象共存，并实时互动。不过目前还是以研究居多，暂时还没有代表产品问世。</p>
<p>体验完最顶级的VR后，正好这几天又碰上阿尔法狗大胜李世石。看来黑客帝国的时代离我们人类不远了。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>没想到到深圳来出差，竟然碰上了<a href="https://bbs.htc.com/cn/news.php?mod=viewthread&amp;tid=68841">HTC Vive中国开发者峰会</a>。虽然我不是VR开发者，但怎么说也是一名消费者，借着周末赶紧去体验了一把。趁着现在热乎劲还没过去，写一下我的一些见闻及感受吧。<br>]]>
    
    </summary>
    
      <category term="HTC Vive" scheme="http://qinghua.github.io/tags/HTC-Vive/"/>
    
      <category term="vr" scheme="http://qinghua.github.io/categories/vr/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[轻松搭建Docker Registry运行环境]]></title>
    <link href="http://qinghua.github.io/docker-registry/"/>
    <id>http://qinghua.github.io/docker-registry/</id>
    <published>2016-03-06T14:08:51.000Z</published>
    <updated>2016-03-07T08:59:01.000Z</updated>
    <content type="html"><![CDATA[<p>我们知道Docker官方提供了一个公有的registry叫做Docker Hub。但是企业内部可能有些镜像还是不方便放到公网上去，所以docker也提供了registry镜像来让需要的人自己搭建私有仓库。本文从零开始搭建Docker Registry的运行环境，并添加用户界面和认证功能。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配两台虚拟机，一台叫做<strong>registry</strong>，它的IP是<strong>192.168.33.18</strong>；另一台叫做<strong>client</strong>，它的IP是<strong>192.168.33.19</strong>。Registry配上界面会比较耗内存，所以我们给它1G内存，默认是512M。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"registry"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"registry"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">1024</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"client"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"client"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后分别在两个终端运行以下命令启动并连接两台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh registry</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh client</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>启动一个registry是很容易的：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">5000</span>:<span class="number">5000</span> \</span><br><span class="line">    --name registry \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /var/lib/registry:/var/lib/registry \</span><br><span class="line">    registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>这里指定了一个<code>/var/lib/registry</code>的卷，是为了把真实的镜像数据储存在主机上，而别在容器挂掉之后丢失数据。就算这样，也还是不保险。要是主机挂了呢？Docker官方建议可以放到<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/rados.md" target="_blank" rel="external">ceph</a>、<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/swift.md" target="_blank" rel="external">swift</a>这样的存储里，或是<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/s3.md" target="_blank" rel="external">亚马逊S3</a>、<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/azure.md" target="_blank" rel="external">微软Azure</a>、<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/gcs.md" target="_blank" rel="external">谷歌GCS</a>、<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/oss.md" target="_blank" rel="external">阿里云OSS</a>之类的云商那里。Docker registry提供了配置文件，可以从容器里复制出来查看：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker cp registry:/etc/docker/registry/config.yml config.yml</span><br><span class="line">cat config.yml</span><br></pre></td></tr></table></figure></p>
<p>配置文件里有一个<code>storage</code>，按照<a href="https://github.com/docker/distribution/blob/master/docs/configuration.md#storage" target="_blank" rel="external">这里</a>写的配置，然后执行以下命令重新挂载这个文件来启动registry就可以了，有条件的话可以去试一试：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker rm -fv registry</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">5000</span>:<span class="number">5000</span> \</span><br><span class="line">    --name registry \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /var/lib/registry:/var/lib/registry \</span><br><span class="line">    -v `<span class="built_in">pwd</span>`/config.yml:/etc/docker/registry/config.yml \</span><br><span class="line">    registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>Docker Registry配置完了，我们在client上传一个镜像试试：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker pull busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br><span class="line">docker tag busybox:<span class="number">1.24</span>.<span class="number">1</span> <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>结果push的时候就挂了。原来是我们没有配置认证信息，所以这是一个“不安全”的registry。Docker要求在docker daemon的启动参数里增加<code>--insecure-registry</code>，才能允许我们上传镜像：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"--insecure-registry 192.168.33.18:5000\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>这回就没问题啦。同样地在registry端也配置一下，然后把registry:2.3.0这个镜像上传：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"--insecure-registry 192.168.33.18:5000\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br><span class="line">docker tag registry:<span class="number">2.3</span>.<span class="number">0</span> <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/library/registry:<span class="number">2.3</span>.<span class="number">0</span></span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/library/registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>如果是没有用户的镜像（通常是官方镜像），打标签和上传都需要加一个<code>library/</code>。客户端必须再配置一个参数<code>--registry-mirror</code>才能在我们自己的私有registry里下载镜像：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'$d'</span> /etc/default/docker</span><br><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"--insecure-registry 192.168.33.18:5000 --registry-mirror http://192.168.33.18:5000\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br><span class="line">docker pull registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>应该有飞一般的感觉了吧。如果镜像不在registry里，客户端会自动去docker hub下载。但是每次打标签再上传岂不是很麻烦？所幸docker提供了一个proxy的功能。只要在<code>config.yml</code>里增加如下配置，重启registry容器即可。这样，客户端pull的镜像，也会自动同步到registry里去。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proxy:&#10;  remoteurl: https://registry-1.docker.io</span><br></pre></td></tr></table></figure></p>
<h2 id="u754C_u9762"><a href="#u754C_u9762" class="headerlink" title="界面"></a>界面</h2><p>Docker官方只提供了REST API，并没有给我们一个界面。好在有热心人士出马，所以我们只需执行以下命令就可以给我们的私有库提供一个UI了：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">8080</span>:<span class="number">8080</span> \</span><br><span class="line">    --name web \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_HOST=<span class="number">172.17</span>.<span class="number">0.1</span> \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_PORT=<span class="number">5000</span>\</span><br><span class="line">    hyper/docker-registry-web</span><br></pre></td></tr></table></figure></p>
<p>然后打开<code>http://192.168.33.18:8080</code>，应该就能看到如下界面：<br><img src="/img/docker-registry-web.png" alt=""></p>
<p>上面是个简易版，如果有更深入的需求，可以尝试SUSE的<a href="http://port.us.org/" target="_blank" rel="external">Portus</a>。除了界面以外，它还提供了更细粒度的权限控制、用户认证等功能。</p>
<h2 id="u8BA4_u8BC1"><a href="#u8BA4_u8BC1" class="headerlink" title="认证"></a>认证</h2><p>我们刚刚配好的insecure registry是不支持认证的，如果要上产品环境，找CA申请一个证书吧。我们自己测试的话，可以用自签名证书。我们准备使用IP代替域名，所以需要在证书里面包含我们的IP：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /certs</span><br><span class="line">sudo sed -i <span class="string">'/^\[ v3_ca \]$/a subjectAltName = IP:192.168.33.18'</span> /etc/ssl/openssl.cnf</span><br><span class="line">sudo sh -c <span class="string">"openssl req \</span><br><span class="line">    -newkey rsa:4096 -nodes -sha256 -keyout /certs/domain.key \</span><br><span class="line">    -x509 -days 365 -out /certs/domain.crt"</span></span><br></pre></td></tr></table></figure></p>
<p>随便填点值完成这繁琐的流程，就能看见certs里面多了两个文件。现在可以用以下命令来启动registry：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> registry</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">5000</span>:<span class="number">5000</span> \</span><br><span class="line">    --name registry \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /var/lib/registry:/var/lib/registry \</span><br><span class="line">    -v /certs:/certs \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_HTTP_TLS_KEY=/certs/domain.key \</span><br><span class="line">    registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>客户端现在就不需要<code>--insecure-registry</code>了，但是由于这是自签名证书，客户端还需要把证书文件复制过去：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'$d'</span> /etc/default/docker</span><br><span class="line">sudo mkdir -p /etc/docker/certs.d/<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/</span><br><span class="line">sudo scp vagrant@<span class="number">192.168</span>.<span class="number">33.18</span>:/certs/domain.crt /etc/docker/certs.d/<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/ca.crt</span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<p>注意vagrant的默认密码也是vagrant。现在push就没有问题了：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>提示镜像已经存在，并没有阻止我们提交。接下来我们加上认证。首先在registry生成用户名hello和密码world：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /auth</span><br><span class="line">sudo sh -c <span class="string">"docker run --entrypoint htpasswd registry:2.3.0 -Bbn hello world &gt; /auth/htpasswd"</span></span><br></pre></td></tr></table></figure></p>
<p>还得指定认证方式和认证文件等参数，重新启动registry容器：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> registry</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">5000</span>:<span class="number">5000</span> \</span><br><span class="line">    --name registry \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /var/lib/registry:/var/lib/registry \</span><br><span class="line">    -v /auth:/auth \</span><br><span class="line">    -v /certs:/certs \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_HTTP_TLS_KEY=/certs/domain.key \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_AUTH=htpasswd \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_AUTH_HTPASSWD_REALM=<span class="string">"Registry Realm"</span> \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \</span><br><span class="line">    registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>这回客户端用<code>docker push 192.168.33.18:5000/busybox:1.24.1</code>来尝试push就会失败啦。但是我们可以用用户名hello和密码world登录啦：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u hello -p world <span class="operator">-e</span> email_whatever <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span></span><br></pre></td></tr></table></figure></p>
<p>再次push，就没有问题了：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>我们知道Docker官方提供了一个公有的registry叫做Docker Hub。但是企业内部可能有些镜像还是不方便放到公网上去，所以docker也提供了registry镜像来让需要的人自己搭建私有仓库。本文从零开始搭建Docker Registry的运行环境，并添加用户界面和认证功能。<br>]]>
    
    </summary>
    
      <category term="docker" scheme="http://qinghua.github.io/tags/docker/"/>
    
      <category term="docker registry" scheme="http://qinghua.github.io/tags/docker-registry/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[轻松搭建nfs存储环境]]></title>
    <link href="http://qinghua.github.io/nfs-demo/"/>
    <id>http://qinghua.github.io/nfs-demo/</id>
    <published>2016-03-03T11:17:45.000Z</published>
    <updated>2016-03-12T15:33:08.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Network_File_System" target="_blank" rel="external">NFS</a>是1984年SUN公司开发的一套网络文件系统。它能够让用户像在本机一样操作远程文件。本文用一个比较简单的方式来搭建NFS试验环境。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init ubuntu/trusty64</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;ubuntu/trusty64&quot;</code>，在它的下面添加如下几行代码，相当于给它分配两台虚拟机，一台叫做<strong>server</strong>，它的IP是<strong>192.168.33.17</strong>；另外一台叫做<strong>client</strong>，它的IP是<strong>192.168.33.18</strong>。我们将会在<strong>server</strong>上启动nfs服务并共享一个目录，然后在client挂载并操作这个目录。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"server"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"server"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"client"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"client"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>分别在两个终端运行以下命令启动并连接两台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh server</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh client</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>启动nfs服务需要nfs-kernel-server这个包，让我们安装并启动：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y nfs-kernel-server</span><br><span class="line">sudo service nfs-kernel-server start</span><br></pre></td></tr></table></figure></p>
<p>服务启动完成后，会生成一个<code>/etc/exports</code>文件，里面就是nfs的配置。我们创建一个文件夹<code>/tmp/nfs</code>，把它配置成nfs的目录，然后重新启动nfs服务：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /tmp/nfs</span><br><span class="line">sudo sh -c <span class="string">"echo '/tmp/nfs *(rw,no_subtree_check,no_root_squash)' &gt;&gt; /etc/exports"</span></span><br><span class="line">sudo service nfs-kernel-server start</span><br></pre></td></tr></table></figure></p>
<p>往<code>/etc/exports</code>里写的那一行里，<code>*</code>代表允许所有的客户端访问。如果只想让192.168.33.18访问，把星号改为这个IP就好了。需要支持多个IP的话，用逗号分隔就行。括号里的<code>rw</code>表示允许读写。<code>no_subtree_check</code>表示在服务器端不检查此nfs文件夹的父目录权限。<code>no_root_squash</code>表示客户端root用户也被视为对此nfs文件夹有全部权限的服务端root用户，生产环境不建议使用，因为那样nfs上的文件就可能被随意篡改。想要了解更多的设置，请自行<code>man exports</code>。</p>
<h2 id="u6D4B_u8BD5"><a href="#u6D4B_u8BD5" class="headerlink" title="测试"></a>测试</h2><p>这时候我们就可以去client挂载server的这个nfs目录了：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t nfs <span class="number">192.168</span>.<span class="number">33.17</span>:/tmp/nfs /mnt</span><br><span class="line">ls /mnt</span><br></pre></td></tr></table></figure></p>
<p>往里面写一个文件：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Hello World!"</span> &gt; /mnt/hw.txt</span><br><span class="line">ll /mnt</span><br></pre></td></tr></table></figure></p>
<p>然后去server端看一下，是不是已经被同步过来了：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll /tmp/nfs</span><br></pre></td></tr></table></figure></p>
<p>Server端也可以往nfs的文件夹里写文件：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Hello NFS"</span> &gt; /tmp/nfs/hn.txt</span><br></pre></td></tr></table></figure></p>
<p>照样可以同步到client端：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll /mnt</span><br></pre></td></tr></table></figure></p>
<p>无论server端还是client端，都可以通过下面这个命令来查看<code>192.168.33.17</code>上公开了哪些nfs文件夹：<br><figure class="highlight sh"><figcaption><span>server or client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">showmount <span class="operator">-e</span> <span class="number">192.168</span>.<span class="number">33.17</span></span><br></pre></td></tr></table></figure></p>
<p>再来一个nfs2文件夹：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /tmp/nfs2</span><br><span class="line">sudo sh -c <span class="string">"echo '/tmp/nfs2 *(rw,no_subtree_check,no_root_squash)' &gt;&gt; /etc/exports"</span></span><br></pre></td></tr></table></figure></p>
<p>这时候再<code>showmount -e 192.168.33.17</code>看一下，<code>/tmp/nfs2</code>并没有出现在列表中。因为nfs并不会去监视<code>/etc/exports</code>这个文件，所以这时候它还不知道我们改变了配置。虽然重新启动nfs服务可以解决这个问题，但是它会中断现存的nfs服务。建议使用以下命令：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo exportfs -ar    <span class="comment"># a代表所有的文件夹，r代表重新导出</span></span><br></pre></td></tr></table></figure></p>
<p>这时候在再<code>showmount -e 192.168.33.17</code>看一下，<code>/tmp/nfs2</code>已经出来啦。加载一下：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo umount /mnt</span><br><span class="line">sudo mount -t nfs <span class="number">192.168</span>.<span class="number">33.17</span>:/tmp/nfs2 /mnt</span><br><span class="line">ls /mnt</span><br></pre></td></tr></table></figure></p>
<p>完全没问题。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="https://en.wikipedia.org/wiki/Network_File_System">NFS</a>是1984年SUN公司开发的一套网络文件系统。它能够让用户像在本机一样操作远程文件。本文用一个比较简单的方式来搭建NFS试验环境。<br>]]>
    
    </summary>
    
      <category term="nfs" scheme="http://qinghua.github.io/tags/nfs/"/>
    
      <category term="storage" scheme="http://qinghua.github.io/tags/storage/"/>
    
      <category term="linux/unix" scheme="http://qinghua.github.io/categories/linux-unix/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如果有10000台机器，你想怎么玩？（八）网络]]></title>
    <link href="http://qinghua.github.io/kubernetes-in-mesos-8/"/>
    <id>http://qinghua.github.io/kubernetes-in-mesos-8/</id>
    <published>2016-03-02T11:30:29.000Z</published>
    <updated>2016-03-21T11:28:05.000Z</updated>
    <content type="html"><![CDATA[<p>这次聊聊docker、k8s、mesos+k8s的网络，了解一下容器、pod和服务间是怎样通信的。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a><a id="more"></a>
</li>
</ul>
<h2 id="Docker_u7684_u7F51_u7EDC_u6A21_u578B"><a href="#Docker_u7684_u7F51_u7EDC_u6A21_u578B" class="headerlink" title="Docker的网络模型"></a>Docker的网络模型</h2><p>Docker默认使用<a href="/docker-bridge-network">桥接网络</a>。Docker daemon启动时，会在主机上启动一个名为docker0的网桥接口。当容器启动的时候，自动分配一对VETH设备，一个连接到docker0上，另一个连接到容器内部的eth0里，于是同一台主机上的容器便能够跨越网络的命名空间，经由主机相互通信。可是不同的主机上的容器就不是那么简单的了。有一种办法是把容器的端口映射到主机的某个端口上去，这样其他主机上的容器可以通过访问这个主机端口的方式实现跨主机通信。Docker 1.6版本之后发布了一个<a href="/docker-overlay-network">覆盖网络</a>overlay network。当使用这个覆盖网络的时候，它便能实现容器的跨主机通信和隔离。</p>
<h2 id="Kubernetes_u7684_u7F51_u7EDC_u6A21_u578B"><a href="#Kubernetes_u7684_u7F51_u7EDC_u6A21_u578B" class="headerlink" title="Kubernetes的网络模型"></a><a href="http://kubernetes.io/docs/admin/networking/" target="_blank" rel="external">Kubernetes的网络模型</a></h2><p>Kubernetes把每个pod当成是一个节点，在这个pod内的所有容器的网络命名空间是共享的，也就意味着它们共享着一个IP地址。Pod内的容器可以用localhost相互通信，与其他容器通信时，直接使用其他pod的IP地址就可以了。如果把每个pod当成一个虚拟机，这样的设计方式是再正常不过的了。用户无需再去考虑pod间的通信问题，也不用考虑pod和主机端口映射的问题了。从这样的易用性出发，kubernetes对网络有三个要求：</p>
<ul>
<li>所有的容器可以在不使用NAT的情况下相互通信</li>
<li>所有的主机和容器可以在不使用NAT的情况下相互通信</li>
<li>容器自己的IP和外部看它的IP是一样的</li>
</ul>
<p>Kubernetes项目启动的时候，docker还只提供了桥接网络。所以单纯地安装docker和kubernetes并不能够满足kubernetes对网络的要求。常见的公有云如GCE、AWS的基础设施都是默认满足网络要求的。私有云的话，一个方法是直接路由，也就是在所有主机的路由表增加其他主机的docker0网桥。但是，增删主机时所有节点都需要重新配置，非常麻烦。另一种方法使用覆盖网络来实现容器跨主机互通，操作相对容易一些。有不少人使用<a href="https://github.com/coreos/flannel#flannel" target="_blank" rel="external">Flannel</a>，也有人使用<a href="http://kubernetes.io/docs/admin/ovs-networking/" target="_blank" rel="external">Open vSwitch</a>、<a href="https://github.com/zettio/weave" target="_blank" rel="external">Weave</a>、<a href="https://github.com/Metaswitch/calico" target="_blank" rel="external">Calico</a>。就flannel来说，它的网络传输如下图：<br><img src="https://raw.githubusercontent.com/coreos/flannel/master/packet-01.png" alt=""></p>
<p>Flannel会在每个主机上运行一个叫做flanneld的代理，它通过etcd保证所有主机上的容器都不会出现重复IP，并能通过物理网络将数据包投递到目标节点的flanneld去。有兴趣的话可以参考<a href="http://dockone.io/article/618" target="_blank" rel="external">《一篇文章带你了解Flannel》</a>。大部分通过覆盖网络实现跨主机容器互通的方案是工作在L2层，在性能上是有些损耗的，但是Calico略微有点不一样。它直接工作在L3层，没有封包解包的损耗，所以性能上影响很小。所付出的代价就是它仅能支持TCP、UDP、ICMP等协议，不过通常来说也足够了。Open vSwitch功能强大，但是配置也比较麻烦。</p>
<p>虽然docker 1.9版正式宣布内置的覆盖网络可以使用在产品环境了，但是kubernetes并不打算支持docker的覆盖网络，据说技术上最主要的原因是kubernetes并不仅仅是为docker这一种容器技术服务的，kubernetes既不想再引入一个键值存储，也不愿意把自己的键值存储暴露给docker用。当然如果用户自己把docker需要的键值存储管理起来，docker自己的覆盖网络还是能够工作的。非技术上的原因是kubernetes认为docker不够开放，都是因为利益啊。详情可以参考<a href="http://blog.kubernetes.io/2016/01/why-Kubernetes-doesnt-use-libnetwork.html" target="_blank" rel="external">这篇文章</a>。</p>
<h2 id="Kubernetes_u7684_u670D_u52A1"><a href="#Kubernetes_u7684_u670D_u52A1" class="headerlink" title="Kubernetes的服务"></a><a href="http://kubernetes.io/docs/user-guide/services/" target="_blank" rel="external">Kubernetes的服务</a></h2><p>Kubernetes是怎么做到一个pod内的容器只有一个IP地址的呢？假如我们启动一个包含俩容器的pod，然后到启动pod的那台虚拟机查看，就会发现除了这两个容器之外，kubernetes另外还启动了一个叫做<code>gcr.io/google_containers/pause</code>的容器。Pod自身的两个容器都是通过在<code>docker run</code>时，net指定使用pause容器网络的办法，把自己需要的端口由pause容器暴露出来的。如下图所示：<br><img src="/img/pause.png" alt=""></p>
<p>当我们通过replication controller，把pod暴露为服务的时候，kubernetes会通过etcd，为这个服务分配一个唯一的虚拟IP。这样做的好处是：避免了用户自己定义的端口有可能重复的问题。每个kube-proxy都会往iptables里写一条关于service虚拟IP的规则。当内部用户使用这个服务的时候，这个虚拟IP便会把流量导入到kube-proxy监听的某个端口上，由kube-proxy使用轮询或基于客户端IP的会话保持的方式，决定最终来提供服务的pod。外部用户由于没有kube-proxy，是不能访问这个服务的，除非我们把服务通过负载均衡或者NodePort的方式暴露出去，本文就不再赘述了。</p>
<h2 id="Kubernetes-Mesos_u7684_u7F51_u7EDC_u6A21_u578B"><a href="#Kubernetes-Mesos_u7684_u7F51_u7EDC_u6A21_u578B" class="headerlink" title="Kubernetes-Mesos的网络模型"></a><a href="https://github.com/kubernetes/kubernetes/blob/master/contrib/mesos/docs/issues.md#user-content-endpoints" target="_blank" rel="external">Kubernetes-Mesos的网络模型</a></h2><p>Mesos使用最基本的Docker网络模型，也就是主机内共享的桥接网络。这点跟kubernetes的要求是矛盾的。所以当一个pod的端点（endpoint），也就是以<strong>pod的IP:端口</strong>的格式暴露出来的时候，由于pod的IP并不能被其他主机所访问，就会导致通信出问题。于是Kubernetes-Mesos开发组想了一个权宜之计：把端点以<strong>主机IP:端口</strong>的格式暴露出来。由于主机的IP是在集群内是都能访问的，所以只要开放端口，通信就能正常工作。如果用户没有指定主机的端口，那就随机分配一个。这个策略是可以通过指定scheduler和controller-manager的启动参数<code>-host-port-endpoints=false</code>（默认为true）来绕过去的，也就是能恢复到kubernetes默认的网络方案去，不过我们就得自己来想办法实现容器的跨主机通信了。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这次聊聊docker、k8s、mesos+k8s的网络，了解一下容器、pod和服务间是怎样通信的。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a>]]>
    
    </summary>
    
      <category term="kubernetes" scheme="http://qinghua.github.io/tags/kubernetes/"/>
    
      <category term="mesos" scheme="http://qinghua.github.io/tags/mesos/"/>
    
      <category term="devops" scheme="http://qinghua.github.io/categories/devops/"/>
    
  </entry>
  
</feed>
