<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[使用IntelliJ IDEA社区版调试tomcat]]></title>
      <url>http://qinghua.github.io/intellij-idea-community-run-tomcat/</url>
      <content type="html"><![CDATA[<p><a href="https://dzone.com/articles/why-idea-better-eclipse" target="_blank" rel="external">IntelliJ IDEA在重构方面碾压eclipse</a>。但是IDEA的免费社区版是<a href="https://www.jetbrains.com/idea/features/editions_comparison_matrix.html" target="_blank" rel="external">不支持tomcat插件的</a>。如果你正在用IDEA社区版开发一个web应用，可以使用下文的方式来调试运行在tomcat里的web应用。笔者目前使用的IDEA版本为2016.1.1。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u73AF_u5883"><a href="#u51C6_u5907_u73AF_u5883" class="headerlink" title="准备环境"></a>准备环境</h2><p>假设我们有一个项目，已经用IDEA打开了，并且这个项目可以构建成一个<code>ggg-webapp.war</code>的包。在菜单<code>Run</code>-&gt;<code>Edit Configurations</code>里，新增一个<code>Remote</code>，填入下图所示的参数：<br><img src="/img/idea-debug-configurations.jpg" alt=""></p>
<ul>
<li>Name：叫啥都行，这里是tomcat-debug</li>
<li>Host：准备运行tomcat的机器IP，可以是vagrant虚拟机</li>
<li>Port：随便选一个端口就好，别跟其它端口冲突</li>
</ul>
<p>Tomcat的话可以用docker启动，比较方便。这里选用的是tomcat:7。复制<code>Command line arguments for running remote JVM</code>里的值，用它代替下面的<code>JAVA_OPTS</code>参数里的内容，然后运行tomcat并把war包复制到webapps目录里：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=host --name=tomcat <span class="operator">-e</span> JAVA_OPTS=<span class="string">'-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=7007'</span> tomcat:<span class="number">7</span></span><br><span class="line">docker cp ggg-webapp.war tomcat:/usr/<span class="built_in">local</span>/tomcat/webapps/</span><br></pre></td></tr></table></figure></p>
<p>Tomcat启动完成后，选择菜单<code>Run</code>-&gt;<code>Debug &#39;tomcat-debug&#39;</code>，就会显示<code>Connected to the target VM, address: &#39;192.168.33.88:7007&#39;, transport: &#39;socket&#39;</code>，然后就可以顺利debug了。</p>
<h2 id="u5176_u5B83"><a href="#u5176_u5B83" class="headerlink" title="其它"></a>其它</h2><p>附送一些调试快捷键：</p>
<ul>
<li>F7：Step into</li>
<li>F8：Step over</li>
<li>F9：Run</li>
<li>Shift+F7：Smart step into（弹出对话框让你选择进入哪个方法）</li>
<li>Shift+F8：Step out</li>
<li>Ctrl+F8 / Command+F8：Toggle Breakpoint</li>
<li>Alt+F8：Evaluate expression</li>
<li>Alt+F9：Run To Cursor</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[轻松了解Kubernetes认证功能]]></title>
      <url>http://qinghua.github.io/kubernetes-security/</url>
      <content type="html"><![CDATA[<p><a href="http://kubernetes.io/docs/whatisk8s/" target="_blank" rel="external">Kubernetes</a>简称k8s，是谷歌于2014年开始主导的开源项目，提供了以容器为中心的部署、伸缩和运维平台。截止目前它的最新版本为1.2。搭建环境之前建议先了解一下kubernetes的相关知识，可以参考<a href="/kubernetes-in-mesos-1">《如果有10000台机器，你想怎么玩？》</a>系列文章。本文介绍kubernetes的安全性配置。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>首先需要搭建kubernetes集群环境，可以参考<a href="/kubernetes-installation">《轻松搭建Kubernetes 1.2版运行环境》</a>来安装自己的kubernetes集群，运行到flannel配置完成即可。接下来的api server等设置的参数可以参考本文。</p>
<p>结果应该是有三台虚拟机，一台叫做<strong>master</strong>，它的IP是<strong>192.168.33.17</strong>，运行着k8s的api server、controller manager和scheduler；另两台叫做<strong>node1</strong>和<strong>node2</strong>，它们的IP分别是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>，运行着k8s的kubelet和kube-proxy，当做k8s的两个节点。</p>
<h2 id="u90E8_u7F72"><a href="#u90E8_u7F72" class="headerlink" title="部署"></a>部署</h2><p>最简单的方式就是通过基于CSV的基本认证。首先需要创建api server的基本认证文件：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">mkdir security</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="number">123456</span>,admin,qinghua &gt; security/basic_auth.csv                      <span class="comment"># 格式：用户名,密码,用户ID</span></span><br></pre></td></tr></table></figure></p>
<p>然后就可以生成CA和api server的证书了：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> security</span><br><span class="line"></span><br><span class="line">openssl genrsa -out ca.key <span class="number">2048</span></span><br><span class="line">openssl req -x509 -new -nodes -key ca.key -subj <span class="string">"/CN=192.168.33.17"</span> -days <span class="number">10000</span> -out ca.crt</span><br><span class="line">openssl genrsa -out server.key <span class="number">2048</span></span><br><span class="line">openssl req -new -key server.key -subj <span class="string">"/CN=192.168.33.17"</span> -out server.csr</span><br><span class="line">openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br></pre></td></tr></table></figure></p>
<p>上面的命令会生成若干证书相关文件，作用如下：</p>
<ul>
<li>ca.key：自己生成的CA的私钥，用于模拟一个CA</li>
<li>ca.crt：用自己的私钥自签名的CA证书</li>
<li>server.key：api server的私钥，用于配置api server的https</li>
<li>server.csr：api server的证书请求文件，用于请求api server的证书</li>
<li>server.crt：用自己模拟的CA签发的api server的证书，用于配置api server的https</li>
</ul>
<p>接下来启动api server，参数的作用可以参考<a href="http://kubernetes.io/docs/admin/kube-apiserver/" target="_blank" rel="external">kube-apiserver官方文档</a>：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=apiserver \</span><br><span class="line">  --net=host \</span><br><span class="line">  -v /home/vagrant/security:/security \</span><br><span class="line">  gcr.io/google_containers/kube-apiserver:e68c6af15d4672feef7022e94ee4d9af \</span><br><span class="line">  kube-apiserver \</span><br><span class="line">  --advertise-address=<span class="number">192.168</span>.<span class="number">33.17</span> \</span><br><span class="line">  --admission-control=ServiceAccount \</span><br><span class="line">  --insecure-bind-address=<span class="number">0.0</span>.<span class="number">0.0</span> \</span><br><span class="line">  --etcd-servers=http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">4001</span> \</span><br><span class="line">  --service-cluster-ip-range=<span class="number">11.0</span>.<span class="number">0.0</span>/<span class="number">16</span> \</span><br><span class="line">  --tls-cert-file=/security/server.crt \</span><br><span class="line">  --tls-private-key-file=/security/server.key \</span><br><span class="line">  --secure-port=<span class="number">443</span> \</span><br><span class="line">  --basic-auth-file=/security/basic_auth.csv</span><br></pre></td></tr></table></figure></p>
<p>还需要启动controller manager，参数的作用可以参考<a href="http://kubernetes.io/docs/admin/kube-controller-manager/" target="_blank" rel="external">kube-controller-manager官方文档</a>：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=cm \</span><br><span class="line">  -v /home/vagrant/security:/security \</span><br><span class="line">  gcr.io/google_containers/kube-controller-manager:b9107c794e0564bf11719dc554213f7b \</span><br><span class="line">  kube-controller-manager \</span><br><span class="line">  --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> \</span><br><span class="line">  --cluster-cidr=<span class="number">10.245</span>.<span class="number">0.0</span>/<span class="number">16</span> \</span><br><span class="line">  --allocate-node-cidrs=<span class="literal">true</span> \</span><br><span class="line">  --root-ca-file=/security/ca.crt \</span><br><span class="line">  --service-account-private-key-file=/security/server.key</span><br></pre></td></tr></table></figure></p>
<p>最后是scheduler，参数的作用可以参考<a href="http://kubernetes.io/docs/admin/kube-scheduler/" target="_blank" rel="external">kube-scheduler官方文档</a>：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=scheduler \</span><br><span class="line">  gcr.io/google_containers/kube-scheduler:<span class="number">903</span>b34d5ed7367ec4dddf846675613c9 \</span><br><span class="line">  kube-scheduler \</span><br><span class="line">  --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<p>可以运行以下命令来确认安全配置已经生效：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -k -u admin:<span class="number">123456</span> https://<span class="number">127.0</span>.<span class="number">0.1</span>/</span><br><span class="line">curl -k -u admin:<span class="number">123456</span> https://<span class="number">127.0</span>.<span class="number">0.1</span>/api/v1</span><br></pre></td></tr></table></figure></p>
<p>最后启动kubelet和kube-proxy，参数的作用可以参考<a href="http://kubernetes.io/docs/admin/kubelet/" target="_blank" rel="external">kubelet官方文档</a>和<a href="http://kubernetes.io/docs/admin/kube-proxy/" target="_blank" rel="external">kube-proxy官方文档</a>：<br><figure class="highlight sh"><figcaption><span>node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">NODE_IP=`ifconfig eth1 | grep <span class="string">'inet addr:'</span> | cut <span class="operator">-d</span>: <span class="operator">-f</span>2 | cut <span class="operator">-d</span><span class="string">' '</span> <span class="operator">-f</span>1`</span><br><span class="line"></span><br><span class="line">sudo kubernetes/server/bin/kubelet \</span><br><span class="line">  --api-servers=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> \</span><br><span class="line">  --cluster-dns=<span class="number">11.0</span>.<span class="number">0.10</span> \</span><br><span class="line">  --cluster-domain=cluster.local \</span><br><span class="line">  --hostname-override=<span class="variable">$NODE_IP</span> \</span><br><span class="line">  --node-ip=<span class="variable">$NODE_IP</span> &gt; kubelet.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line"></span><br><span class="line">sudo kubernetes/server/bin/kube-proxy \</span><br><span class="line">  --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> \</span><br><span class="line">  --hostname-override=<span class="variable">$NODE_IP</span> &gt; proxy.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></table></figure></p>
<h2 id="u9A8C_u8BC1"><a href="#u9A8C_u8BC1" class="headerlink" title="验证"></a>验证</h2><p>如果需要通过https访问，kubectl的命令就略微有点儿麻烦了，需要用<code>basic_auth.csv</code>里配置的<code>admin/123456</code>来登录：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> https://<span class="number">192.168</span>.<span class="number">33.17</span> --insecure-skip-tls-verify=<span class="literal">true</span> --username=admin --password=<span class="number">123456</span> get po</span><br></pre></td></tr></table></figure></p>
<p>因为8080端口还开着，所以也可以通过http访问：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get po</span><br></pre></td></tr></table></figure></p>
<p>配置完成后，可以看到系统里有TYPE为<code>kubernetes.io/service-account-token</code>的秘密：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get secret</span><br></pre></td></tr></table></figure></p>
<p>里面有三条数据，分别是<code>ca.crt</code>，<code>namespace</code>和<code>token</code>，可以通过以下命令看到：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> describe secret</span><br></pre></td></tr></table></figure></p>
<p>如果你通过kubernetes启动了一个pod，就可以在容器的<code>/var/run/secrets/kubernetes.io/serviceaccount/</code>目录里看到以三个文件的形式看到这三条数据（这是<code>--admission-control=ServiceAccount</code>的功劳），当pod需要访问系统服务的时候，就可以使用它们了。可以使用以下命令看到系统的服务账号：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get serviceAccount</span><br></pre></td></tr></table></figure></p>
<h2 id="u7B80_u5316kubectl"><a href="#u7B80_u5316kubectl" class="headerlink" title="简化kubectl"></a>简化kubectl</h2><p>如果我们通过设置<code>--insecure-port=0</code>把api server的http端口关闭，那它就只能通过https访问了：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> apiserver</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=apiserver \</span><br><span class="line">  --net=host \</span><br><span class="line">  -v /home/vagrant/security:/security \</span><br><span class="line">  gcr.io/google_containers/kube-apiserver:e68c6af15d4672feef7022e94ee4d9af \</span><br><span class="line">  kube-apiserver \</span><br><span class="line">  --advertise-address=<span class="number">192.168</span>.<span class="number">33.17</span> \</span><br><span class="line">  --admission-control=ServiceAccount \</span><br><span class="line">  --insecure-bind-address=<span class="number">0.0</span>.<span class="number">0.0</span> \</span><br><span class="line">  --etcd-servers=http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">4001</span> \</span><br><span class="line">  --service-cluster-ip-range=<span class="number">11.0</span>.<span class="number">0.0</span>/<span class="number">16</span> \</span><br><span class="line">  --tls-cert-file=/security/server.crt \</span><br><span class="line">  --tls-private-key-file=/security/server.key \</span><br><span class="line">  --secure-port=<span class="number">443</span> \</span><br><span class="line">  --basic-auth-file=/security/basic_auth.csv \</span><br><span class="line">  --insecure-port=<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>这样的话，就连取个pod都得这么麻烦：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl <span class="operator">-s</span> https://<span class="number">192.168</span>.<span class="number">33.17</span> --insecure-skip-tls-verify=<span class="literal">true</span> --username=admin --password=<span class="number">123456</span> get po</span><br></pre></td></tr></table></figure></p>
<p>幸运的是，kubernetes提供了一种方式，让我们可以大大简化命令，只用这样就好了：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl get po</span><br></pre></td></tr></table></figure></p>
<p>下面就让我们来试一下吧！首先用<code>kubectl config</code>命令来配置admin用户：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl config <span class="built_in">set</span>-credentials admin --username=admin --password=<span class="number">123456</span></span><br></pre></td></tr></table></figure></p>
<p>然后是api server的访问方式，给集群起个名字叫qinghua：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl config <span class="built_in">set</span>-cluster qinghua --insecure-skip-tls-verify=<span class="literal">true</span> --server=https://<span class="number">192.168</span>.<span class="number">33.17</span></span><br></pre></td></tr></table></figure></p>
<p>接下来创建一个context，它连接用户admin和集群qinghua：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl config <span class="built_in">set</span>-context default/qinghua --user=admin --namespace=default --cluster=qinghua</span><br></pre></td></tr></table></figure></p>
<p>最后设置一下默认的context：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl config use-context default/qinghua</span><br></pre></td></tr></table></figure></p>
<p>然后就可以用我们的简化版啦：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl get po</span><br></pre></td></tr></table></figure></p>
<p>可以通过以下命令来看到当前kubectl的配置：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kubernetes/server/bin/kubectl config view</span><br></pre></td></tr></table></figure></p>
<p>能够看到如下内容：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    insecure-skip-tls-verify: <span class="literal">true</span></span><br><span class="line">    server: https://<span class="number">192.168</span>.<span class="number">33.17</span></span><br><span class="line">  name: qinghua</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: qinghua</span><br><span class="line">    namespace: default</span><br><span class="line">    user: admin</span><br><span class="line">  name: default/qinghua</span><br><span class="line">current-context: default/qinghua</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: admin</span><br><span class="line">  user:</span><br><span class="line">    password: <span class="string">"123456"</span></span><br><span class="line">    username: admin</span><br></pre></td></tr></table></figure></p>
<p>实际上这些配置都存放在<code>~/.kube/config</code>文件里：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.kube/config</span><br></pre></td></tr></table></figure></p>
<p>修改这个文件也可以实时生效。细心的童鞋们可以看到，cluster、context和users都是集合，也就是说如果需要切换用户和集群等，只需要设置默认context就可以了，非常方便。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[轻松了解Kubernetes部署功能]]></title>
      <url>http://qinghua.github.io/kubernetes-deployment/</url>
      <content type="html"><![CDATA[<p><a href="http://kubernetes.io/docs/whatisk8s/" target="_blank" rel="external">Kubernetes</a>简称k8s，是谷歌于2014年开始主导的开源项目，提供了以容器为中心的部署、伸缩和运维平台。截止目前它的最新版本为1.2。搭建环境之前建议先了解一下kubernetes的相关知识，可以参考<a href="/kubernetes-in-mesos-1">《如果有10000台机器，你想怎么玩？》</a>系列文章。本文介绍kubernetes的基本部署功能。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>首先需要搭建kubernetes集群环境，可以参考<a href="/kubernetes-installation">《轻松搭建Kubernetes 1.2版运行环境》</a>来安装自己的kubernetes集群。</p>
<p>结果应该是有三台虚拟机，一台叫做<strong>master</strong>，它的IP是<strong>192.168.33.17</strong>，运行着k8s的api server、controller manager和scheduler；另两台叫做<strong>node1</strong>和<strong>node2</strong>，它们的IP分别是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>，运行着k8s的kubelet和kube-proxy，当做k8s的两个节点。</p>
<h2 id="u90E8_u7F72_uFF08deployment_uFF09"><a href="#u90E8_u7F72_uFF08deployment_uFF09" class="headerlink" title="部署（deployment）"></a>部署（deployment）</h2><p>启动一个容器最简单的方法应该就是使用以下命令了：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> run my-nginx --image=nginx:<span class="number">1.7</span>.<span class="number">9</span></span><br></pre></td></tr></table></figure></p>
<p>如果一切工作正常，应该能看到一条消息<strong>deployment “my-nginx” created</strong>。<a href="http://kubernetes.io/docs/user-guide/deployments/" target="_blank" rel="external">Deployment</a>是kubernetes 1.2的一个新引入的概念，它包含着对<a href="http://kubernetes.io/docs/user-guide/pods/" target="_blank" rel="external">Pod</a>和将要代替<a href="http://kubernetes.io/docs/user-guide/replication-controller/" target="_blank" rel="external">Replication Controller</a>的<a href="http://kubernetes.io/docs/user-guide/replicasets/" target="_blank" rel="external">Replica Set</a>的描述。</p>
<p>为了简化命令，我们设置一个别名：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> kubectl=<span class="string">"kubernetes/server/bin/kubectl -s 192.168.33.17:8080"</span></span><br></pre></td></tr></table></figure></p>
<p>使用以下命令可以看到目前集群里的信息：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubectl get po                          <span class="comment"># 查看目前所有的pod</span></span><br><span class="line">kubectl get rs                          <span class="comment"># 查看目前所有的replica set</span></span><br><span class="line">kubectl get deployment                  <span class="comment"># 查看目前所有的deployment</span></span><br><span class="line">kubectl describe po my-nginx            <span class="comment"># 查看my-nginx pod的详细状态</span></span><br><span class="line">kubectl describe rs my-nginx            <span class="comment"># 查看my-nginx replica set的详细状态</span></span><br><span class="line">kubectl describe deployment my-nginx    <span class="comment"># 查看my-nginx deployment的详细状态</span></span><br></pre></td></tr></table></figure></p>
<p>用<code>kubectl describe po my-nginx</code>可以查看到这个pod被分配到哪台node上去。当<code>kubectl get po</code>显示<strong>1/1 Running</strong>时，说明容器已经启动完成了。SSH到那台虚拟机上，<code>docker ps</code>一下，能够看到有两个容器启动完成了，一个是nginx，另一个就是负责网络的pause。使用<code>docker rm -f</code>将nginx容器删除，再用<code>kubectl get po</code>查看，一会儿便会显示<strong>0/1 ContainerCreating</strong>，随即又变成<strong>1/1 Running</strong>，这是replica set的功劳。当它检测到容器挂掉的时候，便会重新启动一个容器来保证服务不中断。不仅是容器挂掉，停止虚拟机也能使容器被再分配到另一台node上，有兴趣的朋友可以自行尝试，虚拟机重启回来后记得再次运行flannel和kubelet哦。</p>
<p>使用以下命令可以删除my-nginx deployment，my-nginx replica set和my-nginx pod（抵消第一条run命令的作用）：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete deployment my-nginx</span><br></pre></td></tr></table></figure></p>
<p>可以使用<code>kubectl get</code>命令来查看删除后的结果。一般我们会把部署信息写在一个yaml格式的文件里，这样比较容易查看，并且方便写入各种参数：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;nginx.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  replicas: <span class="number">2</span></span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:<span class="number">1.7</span>.<span class="number">9</span></span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: <span class="number">80</span></span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: <span class="number">400</span>m</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create <span class="operator">-f</span> nginx.yaml --record</span><br></pre></td></tr></table></figure></p>
<p>Deployment文件的详细说明可以看<a href="http://kubernetes.io/docs/api-reference/extensions/v1beta1/definitions/" target="_blank" rel="external">Extensions API定义</a>，通用参数的详细信息可以看<a href="http://kubernetes.io/docs/api-reference/v1/definitions/" target="_blank" rel="external">Kubernetes API定义</a>。由于这次的replicas设置为2，kubernetes会帮我们启动并维持两个实例。如果我们想要更新部署的yaml，有两种方法。第一种是使用edit直接修改：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit deployment/nginx-deployment</span><br></pre></td></tr></table></figure></p>
<p>把<code>nginx:1.7.9</code>修改为<code>nginx:1.9.11</code>，保存退出。Kubernetes就会自动帮我们升级镜像。通过<code>kubectl get deployment nginx</code>的Events里可以看到升级的事件。不管是哪一种方法，升级的过程都是这样的：</p>
<ul>
<li>启动一个新容器</li>
<li>停止两个旧容器</li>
<li>启动一个新容器<br>启动一个新的容器，然后停止一个旧的，重复这个过程直到旧的容器全部停止为止。这样可以保证</li>
</ul>
<p>第二种是修改yaml文件，然后apply：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">"s/nginx:1.7.9/nginx:1.91/g"</span> nginx.yaml</span><br><span class="line">kubectl apply <span class="operator">-f</span> nginx.yaml</span><br></pre></td></tr></table></figure></p>
<p>细心的你可能已经发现我在上面的命令里把<code>nginx:1.9.1</code>打成<code>nginx:1.91</code>了。如果此时用<code>kubectl describe po nginx</code>命令，就能看到<strong>Error syncing pod, skipping: failed to “StartContainer” for “nginx” with ErrImagePull: “Tag 1.91 not found in repository docker.io/library/nginx”</strong>的错误。现在我需要停止这次升级。我们可以用这条命令来查看升级历史：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout <span class="built_in">history</span> deployment/nginx-deployment</span><br></pre></td></tr></table></figure></p>
<p>由于<code>kubectl create</code>的时候用了<code>--record</code>的标志，我们能够直接看到命令，方便定位到上一次正确的升级<strong>2 kubectl -s 192.168.33.17:8080 edit deployment/nginx-deployment</strong>，查看详细的deployment内容：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout <span class="built_in">history</span> deployment/nginx-deployment --revision=<span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>有两种方法可以回退到这个版本：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployment/nginx-deployment</span><br><span class="line">kubectl rollout undo deployment/nginx-deployment --to-revision=<span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<h2 id="u670D_u52A1"><a href="#u670D_u52A1" class="headerlink" title="服务"></a>服务</h2><p>前面创建了nginx的部署对象，那么别人如何使用nginx这个服务呢？首先要确定的是，这个nginx服务，是给内部使用的，还是外部。如果是内部使用，那就可以不用设置服务的类型（默认为ClusterIP），否则，可以将服务类型设置为NodePort，通过node的端口暴露出来给外部使用；或者是LoadBalancer，由云服务商提供一个负载均衡直接挂在服务上。这里我们使用NodePort，暴露出30088端口给外部使用。如果不指定nodePort，那么kubernetes会随机生成一个。下面让我们来启动服务：<br><figure class="highlight sh"><figcaption><span>master master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;nginx-svc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">  - port: <span class="number">80</span></span><br><span class="line">    targetPort: <span class="number">80</span></span><br><span class="line">    nodePort: <span class="number">30088</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create <span class="operator">-f</span> nginx-svc.yaml</span><br></pre></td></tr></table></figure></p>
<p>这样便可以通过<a href="http://192.168.33.18:30088" target="_blank" rel="external">http://192.168.33.18:30088</a>或<a href="http://192.168.33.19:30088" target="_blank" rel="external">http://192.168.33.19:30088</a>访问nginx服务了：<br><img src="/img/nginx.jpg" alt=""></p>
<p>可以使用以下命令来删除服务及nginx的部署：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete <span class="operator">-f</span> nginx-svc.yaml</span><br><span class="line">kubectl delete <span class="operator">-f</span> nginx.yaml</span><br></pre></td></tr></table></figure></p>
<h2 id="u4E00_u6B21_u6027_u4EFB_u52A1_uFF08job_uFF09"><a href="#u4E00_u6B21_u6027_u4EFB_u52A1_uFF08job_uFF09" class="headerlink" title="一次性任务（job）"></a>一次性任务（job）</h2><p>Kubernetes 1.1版时将<a href="http://kubernetes.io/docs/user-guide/jobs/" target="_blank" rel="external">Job</a>还是Beta版，1.2之后已经可用于生产环境。Job是包含着若干pod的一次性任务。它与Replication Controller和Replica Set最大的区别就是当容器正常停止后，不会再次重启以维持一定数量的pod提供服务。下面我们用busybox运行一个耗时30s的任务：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;busybox.yaml</span><br><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: busybox</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: busybox</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: busybox</span><br><span class="line">        image: busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">          - sleep</span><br><span class="line">          - <span class="string">"30"</span></span><br><span class="line">      restartPolicy: Never</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create <span class="operator">-f</span> busybox.yaml</span><br></pre></td></tr></table></figure></p>
<p>可以使用以下命令来取得目前的job：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get <span class="built_in">jobs</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到，30s之后，SUCCESSFUL从0变为1了，说明这个job已经顺利完成了。可以使用以下命令来查看所有的pod，包含在busybox的job里正常结束的pod：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods --show-all</span><br></pre></td></tr></table></figure></p>
<p>Job完成之后仍然会在那里。如果需要删除，运行以下两条命令之一：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete job busybox</span><br><span class="line">kubectl delete <span class="operator">-f</span> busybox.yaml</span><br></pre></td></tr></table></figure></p>
<p>相关的pod也会一并被删除。不过容器仍然会留在运行过这个pod的node上。这可以通过设置kubelet的<code>--maximum-dead-containers</code>和<code>--maximum-dead-containers-per-container</code>参数来解决。</p>
<h2 id="Daemon_Sets"><a href="#Daemon_Sets" class="headerlink" title="Daemon Sets"></a>Daemon Sets</h2><p>有时候需要每个node上都运行一个pod，比如监控或是日志收集等。这时候使用Daemon Sets就非常方便。我们用一个tomcat容器来做例子：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;tomcat.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-ds</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: tomcat</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: tomcat</span><br><span class="line">        image: tomcat:<span class="number">8.0</span>.<span class="number">30</span>-jre8</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: <span class="number">8080</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create <span class="operator">-f</span> tomcat.yaml</span><br></pre></td></tr></table></figure></p>
<p>运行完毕后，通过以下命令来取得运行结果：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get ds</span><br><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure></p>
<p>这个yaml文件和一开始的deployment的yaml文件格式很像，虽然我们没有指定replicas，但还是起了两个pod（因为我们有两个node）。可以ssh到这两个node上看看是不是每一个node上都启动了一个tomcat容器。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[VR和其他科技的结合意味着什么]]></title>
      <url>http://qinghua.github.io/vr-mix-with-other-techs/</url>
      <content type="html"><![CDATA[<p>随着VR走进现实，人们的双眼纷纷看到了它在影视、游戏等产业的巨大钱景。借用熊节在<a href="http://gigix.thoughtworkers.org/2015/8/28/driverless-cars/" target="_blank" rel="external">《无人驾驶汽车意味着什么》</a>里的一句话：“然而技术的成熟与投入实用远不是变革的终点，恰恰相反，这只是一场更为深远的变革的起点”。我脑洞大开，畅想一下VR和其他科技结合之后，将会如何影响我们的生活。看上去下一个十年又要迎来一次技术大爆炸。<br><a id="more"></a></p>
<h2 id="u65E0_u4EBA_u673A"><a href="#u65E0_u4EBA_u673A" class="headerlink" title="无人机"></a>无人机</h2><p>曾几何时，拍照、摄像还是摄影师的专利，随着数码相机的普及尤其是手机摄像头的发展，到了现在的自媒体时代，人人都喜欢拍一点什么。数量上去了，大家就开始想到提高质量。无人机给了我们这样一个上帝视角，在某种程度上圆了人类的飞行梦。现在的技术可以做到实时在手机上查看无人机正在拍摄的高清视频。VR结合无人机，会产生什么样的效果呢？看看这幅图：<br><img src="/img/vr-skydiving.jpg" alt=""></p>
<p>跳伞的话，只能下不能上，横向移动范围有限。VR结合无人机，能够带来更高一筹的高空体验，并且大大降低了安全事故发生的概率。当然无人机坏掉砸在路人脑袋上这种事就不在我们讨论范围内了…</p>
<h2 id="u673A_u5668_u4EBA"><a href="#u673A_u5668_u4EBA" class="headerlink" title="机器人"></a>机器人</h2><p>VR结合无人机，无论多么狂拽炫酷吊炸天，仍然是小众玩家的福利。而真正能够影响普通人的，应该算是和机器人结合。想像这样的世界：虽然分身乏术，但是你可以购买一个机器人，让它帮你去远方开会，到时候VR直接连接上这个机器人就好啦。这种方案的话，需要成熟的无人驾驶技术的配合，才能让机器人自动到达目的地。<br><img src="/img/vr-robot.jpg" alt=""></p>
<p>继续往里思考，为什么非要买一个机器人跑到远方去呢？是不是可以直接租赁一个远方的机器人呢？假设有一个提供机器人租赁的公司，直接在开会地点附近有一个机器人租赁点，与会人员就没必要派遣机器人过去了，直接每个人租一个机器人开会就好啦。</p>
<h2 id="3D_u6253_u5370"><a href="#3D_u6253_u5370" class="headerlink" title="3D打印"></a>3D打印</h2><p>虽然VR结合机器人听起来令人心动，但是有一个问题就是，大家都是机器人了，那我在VR里看出去都是一堆的机器人有什么用？根本不知道谁是谁，干脆直接电话会议或者视频会议不就好了嘛，反正表情动作眼神都很难感觉出来。这时候3D打印就派上用场了。假设到时候的3D打印技术非常成熟，你在家里就能很轻易地将你现在的模型传送给机器人租赁公司，而公司可以在较短的时间内打印一个像你一样的机器人，于是你在别人的眼里并不是一个机器人，而是真正的“你”。每个人都是自己的样子，开会就像是在现场一样。到时候的材料工艺也应该有很大进步，可以实现材料回收再利用，这样可以大大降低租用机器人的成本。<br><img src="/img/vr-humanoid.jpg" alt=""></p>
<p>想象一下在一个会议里，里面所有的“人”都只是躯壳，而真实的人都在其它地方，是不是有点儿感觉怪怪的了？</p>
<h2 id="u4EBA_u5DE5_u667A_u80FD"><a href="#u4EBA_u5DE5_u667A_u80FD" class="headerlink" title="人工智能"></a>人工智能</h2><p>VR结合3D打印，开始带来一丝怪异的感觉了。再加上人工智能的混入，就已经不是“怪异”两个字就能形容的了。想象你面对一个“人”，音容笑貌都是某个你熟悉的人，但“ta”可能并不是你熟悉的“ta”…“ta”可能是个具有人工智能的机器人！在良性的环境下，“ta”只可能是真人戴着VR在跟你交流，或者由于真人在忙其它事情，“ta”现在是个尽力在模仿真人的善意人工智能；在恶性的环境下，“它”可能被其他人所控制，要对你做出一些诈骗等恶意行为；最恶劣的是“ta”可能发展了自己的意识，你根本不知道“ta”在想什么，“ta”想干什么。这里引用霍金的预言：“成功制造出一台人工智能机器人将是人类历史上的里程碑。但不幸的是，它也可能会成为我们历史上最后的一个里程碑”。黑客帝国里的场景，也有可能变成现实。</p>
<h2 id="u5176_u4ED6_u7545_u60F3"><a href="#u5176_u4ED6_u7545_u60F3" class="headerlink" title="其他畅想"></a>其他畅想</h2><p>在上文的模型里，可以预见的是，信息安全将会是未来的重中之重。传输自己的模型给机器人公司，需要小心传错对象或被黑客拦截；VR连接远程机器人，需要小心机器人可能被其他人捷足先登；对人工智能，也许也需要必要的防范。虽然有许多安全上的顾虑，但也正是这些技术，可以让我们更加安全。矿工附身在机器人上，用自己的娴熟技巧采矿；消防员附身在机器人上，可以身冲火海而不会收到任何损伤；对于户外极限运动爱好者来说，这真的是加了无数条命呀。话说回来，如果要实现这样的效果，光自动驾驶是远远不够的。可能还需要如下的几点：</p>
<ul>
<li>高带宽+低延迟的移动网络：没有高带宽的支持，收到的VR场景数据可能分辨率或刷新率低，降低沉浸式体验。没有低延迟的支持，机器人反应慢就不那么适合争分夺秒型任务。</li>
<li><p>移动：现在大多数的VR都不支持移动。即使像HTC Vive那样，由于受到空间制约，也只是有限支持。如果要实用化，下图的平面移动能够满足一定的需求，不过还需考虑包含上下的三维空间。<br><img src="/img/vr-move.jpg" alt=""></p>
</li>
<li><p>材料和传感器：为了完美体验触感，也许还需要能够快速打印出具有一定质量、密度、硬度的物质，需要极高的材料工艺，并且需要机器人身上安装高精度传感器实时传回需要的材料数据。</p>
</li>
<li>环境：微风轻抚和狂风暴雨是完全不一样的感受。还要考虑温度、湿度等感觉。</li>
<li>嗅觉支持：虽然人类的鼻子没有狗鼻子那么灵敏，但是现实生活中，有时候气味是决定下一步行动的原因。比如闻到了焦糊味就去厨房看看。相比起来，对于味觉的支持就可以缓一缓了。</li>
<li>梦境或意识：除了上面的移动、材料和传感器的硬派支持，还有一种方式是以类似催眠、梦境或者存意识的方式来支持这种感觉的传递。阿凡达！！</li>
</ul>
<p>由于有了机器人替身，战争的形式也应该会发生变化。士兵们不需要搏命了，也许打战变成了一场游戏。开战前拔掉对方的VR中心可能变成了一种战略选择。毕竟如果存在由人控制的机器人对人类的战争，那结果应该像是现代兵器对战冷兵器时代，是一边倒的。希望不要有战争。即使有战争也别培育出作战型人工智能。</p>
<p>再想点儿其他贴近生活的方面，可能人们就不太需要出行了。现在的上班族大多经历过天天挤公交地铁或者开车堵在路上的罪。以后不需要了。公司摆一个你的机器人躯壳，家里来副VR眼镜，一切搞定。还有什么场景需要出行？旅游？现在的VR都已经快要解决这个问题了，各大VR视频资源库里已经有不少全景漫游的视频了。风景变化不大，甚至不需要机器人的代劳，直接数字化就好了。什么九寨沟一年四季各有各的看点？巴尔夏明神庙被炸毁？华南虎灭绝了？统统都能看到。购物也是同理，送货上门应该算是未来商店最基本的服务之一了。探亲？这个确实有些不好回答，也许有些家庭无所谓真的假的，有些家庭会更倾向于真人探亲。对了，吃饭和看病机器人就没法代劳了吧？何不反过来想想这个问题，虽然机器人不能代替你去饭店吃饭，但是大厨可以附身在你家的机器人上，给你做出美味佳肴！没有趁手的工具？3D打印！看病也是同理，就是可能会由于医疗器械比较昂贵或者各种许可证、专利问题，也许最终还是得你亲自跑一趟。上厕所？想必你每天上厕所不至于跑个几公里吧…不管怎样，VR和其他技术的结合确实能够大大减少出行的次数。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用RADOSGW联盟网关跨域同步]]></title>
      <url>http://qinghua.github.io/ceph-radosgw-replication/</url>
      <content type="html"><![CDATA[<p><a href="http://ceph.com/" target="_blank" rel="external">Ceph</a>是一个高性能的PB级分布式文件系统。它能够在一个系统中提供对象存储、块存储和文件存储。本文的主要内容是如何<a href="http://docs.ceph.com/docs/master/radosgw/federated-config/" target="_blank" rel="external">通过RADOSGW备份一个可用区</a>。</p>
<ul>
<li>对如何加载使用块存储和文件存储感兴趣可以参考<a href="/ceph-demo">《用容器轻松搭建ceph实验环境》</a></li>
<li>对RADOSGW如何暴露S3和Swift接口感兴趣可以参考<a href="/ceph-radosgw">《通过RADOSGW提供ceph的S3和Swift接口》</a><a id="more"></a>
</li>
</ul>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配三台虚拟机，一台叫做<strong>us-east</strong>，它的IP是<strong>192.168.33.15</strong>；另一台叫做<strong>us-west</strong>，它的IP是<strong>192.168.33.16</strong>；第三台叫做<strong>eu-east</strong>，它的IP是<strong>192.168.33.17</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"us-east"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"us-east"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.15"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"us-west"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"us-west"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.16"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"eu-east"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"eu-east"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>然后分别在三个终端运行以下命令启动并连接三台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh us-east</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh us-west</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh eu-east</span><br></pre></td></tr></table></figure>
<h2 id="u80CC_u666F_u4ECB_u7ECD"><a href="#u80CC_u666F_u4ECB_u7ECD" class="headerlink" title="背景介绍"></a>背景介绍</h2><p><a href="http://docs.ceph.com/docs/master/radosgw/" target="_blank" rel="external">Ceph对象网关（Ceph Object Gateway，radosgw）</a>提供了S3和Swift的API，同时也支持S3的一些概念。<a href="http://docs.ceph.com/docs/master/radosgw/config-ref/#regions" target="_blank" rel="external">辖区（region）</a>表明了一个地理位置，比如us。在这个辖区里可以有多个<a href="http://docs.ceph.com/docs/master/radosgw/config-ref/#zones" target="_blank" rel="external">域（zone）</a>，比如east和west。一个域里可以有多个实例，一个实例可以有多个<a href="http://docs.ceph.com/docs/master/glossary/#term-ceph-node" target="_blank" rel="external">节点（node）</a>。同时，配置一个域需要一系列的<a href="http://docs.ceph.com/docs/master/radosgw/config-ref/#pools" target="_blank" rel="external">存储池（pool）</a>。如下图：<br><img src="/img/ceph-radosgw-replication-config.png" alt=""></p>
<p>对于这次练习，我们使用下图的架构：<br><img src="/img/ceph-radosgw-replication-example.png" alt=""></p>
<p>一个us辖区里有us-east和us-west两个域，每个域里各有一个实例，分别为us-east-1和us-west-1。还有一个eu的辖区，里面有一个eu-east的域，一个为eu-east-1的实例。我们将会首先实现同一个辖区（us）里的同步，然后是不同辖区的同步。相同辖区可以同步元数据和数据对象，而不同的辖区只能同步元数据而不能同步数据对象。元数据包括网关用户和存储桶（bucket）。</p>
<h2 id="u76F8_u540C_u8F96_u533A_u7684_u540C_u6B65"><a href="#u76F8_u540C_u8F96_u533A_u7684_u540C_u6B65" class="headerlink" title="相同辖区的同步"></a>相同辖区的同步</h2><p>首先需要安装一些ceph、radosgw的依赖包：<br><figure class="highlight sh"><figcaption><span>us-east us-west eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y --force-yes install ceph-common radosgw radosgw-agent</span><br></pre></td></tr></table></figure></p>
<p>为了提供HTTP服务，还需要所有虚拟机都安装apache2和FastCGI：<br><figure class="highlight sh"><figcaption><span>us-east us-west eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get -y --force-yes install apache2 libapache2-mod-fastcgi</span><br></pre></td></tr></table></figure></p>
<p>然后就可以分别启动ceph/demo这个容器来轻松提供ceph服务了：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=host -v /etc/ceph:/etc/ceph <span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">33.15</span> <span class="operator">-e</span> CEPH_NETWORK=<span class="number">192.168</span>.<span class="number">33.15</span>/<span class="number">24</span> --name=ceph ceph/demo</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=host -v /etc/ceph:/etc/ceph <span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">33.16</span> <span class="operator">-e</span> CEPH_NETWORK=<span class="number">192.168</span>.<span class="number">33.16</span>/<span class="number">24</span> --name=ceph ceph/demo</span><br></pre></td></tr></table></figure>
<p>然后手动在各自虚拟机上创建一些提供给域使用的存储池，这一步不是必须的，因为我们创建的网关用户是有权限自动创建存储池的：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">ceph osd pool create .us-east.rgw.root <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.rgw.control <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.rgw.gc <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.rgw.buckets <span class="number">512</span> <span class="number">512</span></span><br><span class="line">ceph osd pool create .us-east.rgw.buckets.index <span class="number">32</span> <span class="number">32</span></span><br><span class="line">ceph osd pool create .us-east.rgw.buckets.extra <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.intent-log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.usage <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.users <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.users.email <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.users.swift <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-east.users.uid <span class="number">16</span> <span class="number">16</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">ceph osd pool create .us-west.rgw.root <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.rgw.control <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.rgw.gc <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.rgw.buckets <span class="number">512</span> <span class="number">512</span></span><br><span class="line">ceph osd pool create .us-west.rgw.buckets.index <span class="number">32</span> <span class="number">32</span></span><br><span class="line">ceph osd pool create .us-west.rgw.buckets.extra <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.intent-log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.usage <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.users <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.users.email <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.users.swift <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .us-west.users.uid <span class="number">16</span> <span class="number">16</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<p>两台虚拟机对应两个实例，接下来为这两个实例分别创建密钥环（keyring），用它生成网关的用户和密钥（key），增加密钥的rwx权限并让其有权限访问Ceph对象集群：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph auth del client.radosgw.gateway</span><br><span class="line">sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.us-east-<span class="number">1</span> --gen-key</span><br><span class="line">sudo ceph-authtool -n client.radosgw.us-east-<span class="number">1</span> --cap osd <span class="string">'allow rwx'</span> --cap mon <span class="string">'allow rw'</span> /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.us-east-<span class="number">1</span> -i /etc/ceph/ceph.client.radosgw.keyring</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph auth del client.radosgw.gateway</span><br><span class="line">sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.us-west-<span class="number">1</span> --gen-key</span><br><span class="line">sudo ceph-authtool -n client.radosgw.us-west-<span class="number">1</span> --cap osd <span class="string">'allow rwx'</span> --cap mon <span class="string">'allow rw'</span> /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.us-west-<span class="number">1</span> -i /etc/ceph/ceph.client.radosgw.keyring</span><br></pre></td></tr></table></figure>
<p>接下来创建一个apache2的配置文件，监听80端口并把请求转发到radosgw提供的FastCGI 9000端口（稍后将会配置）上：<br><figure class="highlight sh"><figcaption><span>us-east us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; rgw.conf</span><br><span class="line">FastCgiExternalServer /var/www/s3gw.fcgi -host localhost:<span class="number">9000</span></span><br><span class="line"></span><br><span class="line">&lt;VirtualHost *:<span class="number">80</span>&gt;</span><br><span class="line">    ServerName localhost</span><br><span class="line">    ServerAlias *.localhost</span><br><span class="line">    ServerAdmin qinghua@ggg.com</span><br><span class="line">    DocumentRoot /var/www</span><br><span class="line">    RewriteEngine On</span><br><span class="line">    RewriteRule  ^/(.*) /s3gw.fcgi?%&#123;QUERY_STRING&#125; [E=HTTP_AUTHORIZATION:%&#123;HTTP:Authorization&#125;,L]</span><br><span class="line"></span><br><span class="line">    &lt;IfModule mod_fastcgi.c&gt;</span><br><span class="line">           &lt;Directory /var/www&gt;</span><br><span class="line">            Options +ExecCGI</span><br><span class="line">            AllowOverride All</span><br><span class="line">            SetHandler fastcgi-script</span><br><span class="line">            Order allow,deny</span><br><span class="line">            Allow from all</span><br><span class="line">            AuthBasicAuthoritative Off</span><br><span class="line">        &lt;/Directory&gt;</span><br><span class="line">    &lt;/IfModule&gt;</span><br><span class="line"></span><br><span class="line">    AllowEncodedSlashes On</span><br><span class="line">    ErrorLog /var/<span class="built_in">log</span>/apache2/error.log</span><br><span class="line">    CustomLog /var/<span class="built_in">log</span>/apache2/access.log combined</span><br><span class="line">    ServerSignature Off</span><br><span class="line">&lt;/VirtualHost&gt;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv rgw.conf /etc/apache2/conf-enabled/rgw.conf</span><br></pre></td></tr></table></figure></p>
<p>由于上述配置需要用到apache2默认未加载的<a href="http://httpd.apache.org/docs/current/mod/mod_rewrite.html" target="_blank" rel="external">rewrite模块</a>，所以需要加载并重新启动apache2：<br><figure class="highlight sh"><figcaption><span>us-east us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo a2enmod rewrite</span><br><span class="line">sudo service apache2 restart</span><br></pre></td></tr></table></figure></p>
<p>FastCGI需要一个脚本来启用兼容S3的接口，同样也是所有虚拟机都要配，但是实例名略有区别：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; s3gw.fcgi</span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line"><span class="built_in">exec</span> /usr/bin/radosgw -c /etc/ceph/ceph.conf -n client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv s3gw.fcgi /var/www/s3gw.fcgi</span><br><span class="line">sudo chmod +x /var/www/s3gw.fcgi</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; s3gw.fcgi</span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line"><span class="built_in">exec</span> /usr/bin/radosgw -c /etc/ceph/ceph.conf -n client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv s3gw.fcgi /var/www/s3gw.fcgi</span><br><span class="line">sudo chmod +x /var/www/s3gw.fcgi</span><br></pre></td></tr></table></figure>
<p>现在到了在<code>ceph.conf</code>配置实例的时候了：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'$a rgw region root pool = .us.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zonegroup root pool = .us.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a [client.radosgw.us-east-1]'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw region = us'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone = us-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone root pool = .us-east.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw dns name = us-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a host = us-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a keyring = /etc/ceph/ceph.client.radosgw.keyring'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw socket path = ""'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a log file = /var/log/radosgw/client.radosgw.us-east-1.log'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw frontends = fastcgi socket_port=9000 socket_host=0.0.0.0'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw print continue = false'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'$a rgw region root pool = .us.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zonegroup root pool = .us.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a [client.radosgw.us-west-1]'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw region = us'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone = us-west'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone root pool = .us-west.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw dns name = us-west'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a host = us-west'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a keyring = /etc/ceph/ceph.client.radosgw.keyring'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw socket path = ""'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a log file = /var/log/radosgw/client.radosgw.us-west-1.log'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw frontends = fastcgi socket_port=9000 socket_host=0.0.0.0'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw print continue = false'</span> /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure>
<p>配置参数及其作用可以在<a href="http://docs.ceph.com/docs/master/radosgw/config-ref/" target="_blank" rel="external">这里</a>查到，下面列出了一部分与辖区和域有关的参数：</p>
<ul>
<li><strong>rgw region root pool</strong>：v.67版本中指定辖区所使用的存储池</li>
<li><strong>rgw zonegroup root pool</strong>：Jewel版本中指定辖区所使用的存储池</li>
<li><strong>rgw region</strong>：指定该实例的辖区名</li>
<li><strong>rgw zone</strong>：指定该实例的域名</li>
<li><strong>rgw zone root pool</strong>：指定域所使用的存储池</li>
</ul>
<p>接下来在各自实例上生成一个相同的json格式的辖区文件：<br><figure class="highlight sh"><figcaption><span>us-east us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">cat &lt;&lt; EOF &gt; us.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"us"</span>,</span><br><span class="line">  <span class="string">"api_name"</span>: <span class="string">"us"</span>,</span><br><span class="line">  <span class="string">"is_master"</span>: <span class="string">"true"</span>,</span><br><span class="line">  <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.15:80\/"</span>],</span><br><span class="line">  <span class="string">"master_zone"</span>: <span class="string">"us-east"</span>,</span><br><span class="line">  <span class="string">"zones"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"us-east"</span>,</span><br><span class="line">      <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.15:80\/"</span>],</span><br><span class="line">      <span class="string">"log_meta"</span>: <span class="string">"true"</span>,</span><br><span class="line">      <span class="string">"log_data"</span>: <span class="string">"true"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"us-west"</span>,</span><br><span class="line">      <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.16:80\/"</span>],</span><br><span class="line">      <span class="string">"log_meta"</span>: <span class="string">"true"</span>,</span><br><span class="line">      <span class="string">"log_data"</span>: <span class="string">"true"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"placement_targets"</span>: [</span><br><span class="line">   &#123;</span><br><span class="line">     <span class="string">"name"</span>: <span class="string">"default-placement"</span>,</span><br><span class="line">     <span class="string">"tags"</span>: []</span><br><span class="line">   &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"default_placement"</span>: <span class="string">"default-placement"</span></span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>然后通过辖区文件生成us辖区并设置其为默认辖区：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin region <span class="built_in">set</span> --infile us.json --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">radosgw-admin region default --rgw-region=us --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin region <span class="built_in">set</span> --infile us.json --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">radosgw-admin region default --rgw-region=us --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<p>辖区搞定之后，就轮到域啦。在各自实例上生成两个相同的json格式的域文件：<br><figure class="highlight sh"><figcaption><span>us-east us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">cat &lt;&lt; EOF &gt; us-east.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"domain_root"</span>: <span class="string">".us-east.domain.rgw"</span>,</span><br><span class="line">  <span class="string">"control_pool"</span>: <span class="string">".us-east.rgw.control"</span>,</span><br><span class="line">  <span class="string">"gc_pool"</span>: <span class="string">".us-east.rgw.gc"</span>,</span><br><span class="line">  <span class="string">"log_pool"</span>: <span class="string">".us-east.log"</span>,</span><br><span class="line">  <span class="string">"intent_log_pool"</span>: <span class="string">".us-east.intent-log"</span>,</span><br><span class="line">  <span class="string">"usage_log_pool"</span>: <span class="string">".us-east.usage"</span>,</span><br><span class="line">  <span class="string">"user_keys_pool"</span>: <span class="string">".us-east.users"</span>,</span><br><span class="line">  <span class="string">"user_email_pool"</span>: <span class="string">".us-east.users.email"</span>,</span><br><span class="line">  <span class="string">"user_swift_pool"</span>: <span class="string">".us-east.users.swift"</span>,</span><br><span class="line">  <span class="string">"user_uid_pool"</span>: <span class="string">".us-east.users.uid"</span>,</span><br><span class="line">  <span class="string">"system_key"</span>: &#123;</span><br><span class="line">    <span class="string">"access_key"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"secret_key"</span>: <span class="string">""</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"placement_pools"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"key"</span>: <span class="string">"default-placement"</span>,</span><br><span class="line">      <span class="string">"val"</span>: &#123;</span><br><span class="line">        <span class="string">"index_pool"</span>: <span class="string">".us-east.rgw.buckets.index"</span>,</span><br><span class="line">        <span class="string">"data_pool"</span>: <span class="string">".us-east.rgw.buckets"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed <span class="string">'s/east/west/g'</span> us-east.json &gt; us-west.json</span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>然后通过域文件生成两个域并更新辖区图（region map）：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-east --infile us-east.json --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-west --infile us-west.json --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-east --infile us-east.json --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-west --infile us-west.json --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<p>在<code>us-east-1</code>实例上，生成<code>us-east</code>的用户，并用生成的<code>access_key</code>和<code>secret_key</code>填充刚才为空的<code>us-east.json</code>文件，并将其复制到<code>us-west</code>虚拟机上：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin user create --uid=<span class="string">"us-east"</span> --display-name=<span class="string">"Region-US Zone-East"</span> --name client.radosgw.us-east-<span class="number">1</span> --system | tee eastuser.txt</span><br><span class="line"><span class="built_in">export</span> SRC_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> eastuser.txt`</span><br><span class="line"><span class="built_in">export</span> SRC_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> eastuser.txt`</span><br><span class="line">sed -i <span class="string">"s/access_key\": \"/access_key\": \"<span class="variable">$SRC_ACCESS_KEY</span>/g"</span> us-east.json</span><br><span class="line">sed -i <span class="string">"s/secret_key\": \"/secret_key\": \"<span class="variable">$SRC_SECRET_KEY</span>/g"</span> us-east.json</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line">docker cp ceph:us-east.json us-east.json</span><br><span class="line">scp us-east.json vagrant@<span class="number">192.168</span>.<span class="number">33.16</span>:/home/vagrant    <span class="comment"># vagrant的密码也是vagrant</span></span><br></pre></td></tr></table></figure></p>
<p>在<code>us-west-1</code>实例上，生成<code>us-west</code>的用户，也用生成的<code>access_key</code>和<code>secret_key</code>填充刚才为空的<code>us-west.json</code>文件，并将其复制到<code>us-east</code>虚拟机上：<br><figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">radosgw-admin user create --uid=<span class="string">"us-west"</span> --display-name=<span class="string">"Region-US Zone-West"</span> --name client.radosgw.us-west-<span class="number">1</span> --system | tee westuser.txt</span><br><span class="line"><span class="built_in">export</span> DEST_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> westuser.txt`</span><br><span class="line"><span class="built_in">export</span> DEST_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> westuser.txt`</span><br><span class="line">sed -i <span class="string">"s/access_key\": \"/access_key\": \"<span class="variable">$DEST_ACCESS_KEY</span>/g"</span> us-west.json</span><br><span class="line">sed -i <span class="string">"s/secret_key\": \"/secret_key\": \"<span class="variable">$DEST_SECRET_KEY</span>/g"</span> us-west.json</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line">docker cp ceph:us-west.json us-west.json</span><br><span class="line">scp us-west.json vagrant@<span class="number">192.168</span>.<span class="number">33.15</span>:/home/vagrant    <span class="comment"># vagrant的密码也是vagrant</span></span><br></pre></td></tr></table></figure></p>
<p>现在两台虚拟机的用户主文件夹里都有对方的json文件，分别复制进ceph容器里：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker cp us-west.json ceph:/us-west.json</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker cp us-east.json ceph:/us-east.json</span><br></pre></td></tr></table></figure>
<p>接下来分别在两个实例里更新带了<code>access_key</code>和<code>secret_key</code>的各两个域：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-east --infile us-east.json --name client.radosgw.us-east-<span class="number">1</span></span><br><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-west --infile us-west.json --name client.radosgw.us-east-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-east --infile us-east.json --name client.radosgw.us-west-<span class="number">1</span></span><br><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin zone <span class="built_in">set</span> --rgw-zone=us-west --infile us-west.json --name client.radosgw.us-west-<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>都完成了以后，就可以重启ceph服务和apache2啦：<br><figure class="highlight sh"><figcaption><span>us-east us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker restart ceph</span><br><span class="line">sudo /etc/init.d/radosgw start</span><br><span class="line">sudo service apache2 restart</span><br></pre></td></tr></table></figure></p>
<p>Apache2启动完成后，在浏览器打开<a href="http://192.168.33.15/" target="_blank" rel="external">http://192.168.33.15/</a>或<a href="http://192.168.33.16/" target="_blank" rel="external">http://192.168.33.16/</a>应该能看到下面的xml：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">ListAllMyBucketsResult</span> <span class="attribute">xmlns</span>=<span class="value">"http://s3.amazonaws.com/doc/2006-03-01/"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">Owner</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">ID</span>&gt;</span>anonymous<span class="tag">&lt;/<span class="title">ID</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">DisplayName</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">Owner</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">Buckets</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">ListAllMyBucketsResult</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>如果只看到了500的错误，等一会儿再刷新一次即可。如果遇到麻烦，可以这样调试：输入命令<code>sudo lsof -i :9000</code>看看是否radosgw启动了这个端口。如果没有，输入命令<code>ps -ef | grep radosgw</code>看看radosgw是否正常启动。若是正常启动，应该会有一个<code>/usr/bin/radosgw -n client.radosgw.us-east-1</code>的进程。若是没有正常启动，可以检查<code>/ect/ceph/ceph.conf</code>的内容，一般都是配置有问题。</p>
<p>搞定ceph和apache2后，在<code>us-east</code>里用python的boto库给<code>us-east-1</code>实例创建一个名为<code>my-new-bucket</code>的存储桶，并给<code>ggg</code>的key上传一句<strong>Hello world</strong>：<br><figure class="highlight sh"><figcaption><span>us-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SRC_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> us-east.json`</span><br><span class="line"><span class="built_in">export</span> SRC_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> us-east.json`</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; s3test.py</span><br><span class="line">import boto</span><br><span class="line">import boto.s3</span><br><span class="line">import boto.s3.connection</span><br><span class="line">import os</span><br><span class="line">from boto.s3.key import Key</span><br><span class="line"></span><br><span class="line">access_key = os.environ[<span class="string">"SRC_ACCESS_KEY"</span>]</span><br><span class="line">secret_key = os.environ[<span class="string">"SRC_SECRET_KEY"</span>]</span><br><span class="line">conn = boto.connect_s3(</span><br><span class="line">  aws_access_key_id = access_key,</span><br><span class="line">  aws_secret_access_key = secret_key,</span><br><span class="line">  host = <span class="string">'192.168.33.15'</span>,</span><br><span class="line">  is_secure=False,</span><br><span class="line">  calling_format = boto.s3.connection.OrdinaryCallingFormat(),</span><br><span class="line">)</span><br><span class="line">bucket = conn.create_bucket(<span class="string">'my-new-bucket'</span>)</span><br><span class="line"></span><br><span class="line">k = Key(bucket)</span><br><span class="line">k.key = <span class="string">'ggg'</span></span><br><span class="line">k.set_contents_from_string(<span class="string">'Hello world'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> bucket <span class="keyword">in</span> conn.get_all_buckets():</span><br><span class="line">  <span class="built_in">print</span> <span class="string">"&#123;name&#125;\t&#123;created&#125;"</span>.format(</span><br><span class="line">    name = bucket.name,</span><br><span class="line">    created = bucket.creation_date,</span><br><span class="line">)</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">python s3test.py</span><br></pre></td></tr></table></figure></p>
<p>现在就该启动<code>radosgw-agent</code>来同步数据啦：<br><figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SRC_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> us-east.json`</span><br><span class="line"><span class="built_in">export</span> SRC_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> us-east.json`</span><br><span class="line"><span class="built_in">export</span> DEST_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> us-west.json`</span><br><span class="line"><span class="built_in">export</span> DEST_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> us-west.json`</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; cluster-data-sync.conf</span><br><span class="line">src_zone: us-east</span><br><span class="line"><span class="built_in">source</span>: http://<span class="number">192.168</span>.<span class="number">33.15</span></span><br><span class="line">src_access_key: <span class="variable">$SRC_ACCESS_KEY</span></span><br><span class="line">src_secret_key: <span class="variable">$SRC_SECRET_KEY</span></span><br><span class="line">dest_zone: us-west</span><br><span class="line">destination: http://<span class="number">192.168</span>.<span class="number">33.16</span></span><br><span class="line">dest_access_key: <span class="variable">$DEST_ACCESS_KEY</span></span><br><span class="line">dest_secret_key: <span class="variable">$DEST_SECRET_KEY</span></span><br><span class="line"><span class="built_in">log</span>_file: /var/<span class="built_in">log</span>/radosgw/radosgw-sync-us-east-west.log</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo radosgw-agent -c cluster-data-sync.conf</span><br></pre></td></tr></table></figure></p>
<p>再打开一个终端窗口，用以下命令查看<code>us-west-1</code>实例是不是已经把<code>my-new-bucket</code>同步过来啦：<br><figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin metadata bucket list --name client.radosgw.us-west-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>可能是由于单机<code>ceph/demo</code>容器的性能极差，在同步对象的时候基本上就一直停在<strong>INFO:radosgw_agent.worker:syncing bucket “my-new-bucket”</strong>上。如果有真实环境的ceph应该能够很快同步过来。如果同步成功，可以用以下命令来得到刚才的<strong>Hello world</strong>：<br><figure class="highlight sh"><figcaption><span>us-west</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; s3download.py</span><br><span class="line">import boto</span><br><span class="line">import boto.s3</span><br><span class="line">import boto.s3.connection</span><br><span class="line">import os</span><br><span class="line">from boto.s3.key import Key</span><br><span class="line"></span><br><span class="line">access_key = os.environ[<span class="string">"SRC_ACCESS_KEY"</span>]</span><br><span class="line">secret_key = os.environ[<span class="string">"SRC_SECRET_KEY"</span>]</span><br><span class="line">conn = boto.connect_s3(</span><br><span class="line">  aws_access_key_id = access_key,</span><br><span class="line">  aws_secret_access_key = secret_key,</span><br><span class="line">  host = <span class="string">'192.168.33.16'</span>,</span><br><span class="line">  is_secure=False,</span><br><span class="line">  calling_format = boto.s3.connection.OrdinaryCallingFormat(),</span><br><span class="line">)</span><br><span class="line">bucket = conn.get_bucket(<span class="string">'my-new-bucket'</span>)</span><br><span class="line"></span><br><span class="line">key = bucket.get_key(<span class="string">'ggg'</span>)</span><br><span class="line"><span class="built_in">print</span> key.get_contents_as_string()</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">python s3download.py</span><br></pre></td></tr></table></figure></p>
<p>下面是一张同辖区同步示意图：<br><img src="http://docs.ceph.com/docs/master/_images/zone-sync.png" alt=""></p>
<h2 id="u4E0D_u540C_u8F96_u533A_u7684_u540C_u6B65"><a href="#u4E0D_u540C_u8F96_u533A_u7684_u540C_u6B65" class="headerlink" title="不同辖区的同步"></a>不同辖区的同步</h2><p>不同的辖区只能同步元数据而不能同步数据对象。接下来我们在eu-east上，尝试同步us-east的元数据。有了<a href="/ceph-radosgw-replication/#u76F8_u540C_u8F96_u533A_u7684_u540C_u6B65">相同辖区的同步</a>的经验，这回就不详细介绍下面的命令了：<br><figure class="highlight sh"><figcaption><span>eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=host -v /etc/ceph:/etc/ceph <span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">33.17</span> <span class="operator">-e</span> CEPH_NETWORK=<span class="number">192.168</span>.<span class="number">33.17</span>/<span class="number">24</span> --name=ceph ceph/demo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建存储池这一步也同上面一样是可选</span></span><br><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line">ceph osd pool create .eu-east.rgw.root <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.rgw.control <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.rgw.gc <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.rgw.buckets <span class="number">512</span> <span class="number">512</span></span><br><span class="line">ceph osd pool create .eu-east.rgw.buckets.index <span class="number">32</span> <span class="number">32</span></span><br><span class="line">ceph osd pool create .eu-east.rgw.buckets.extra <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.intent-log <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.usage <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.users <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.users.email <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.users.swift <span class="number">16</span> <span class="number">16</span></span><br><span class="line">ceph osd pool create .eu-east.users.uid <span class="number">16</span> <span class="number">16</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建密钥环和网关用户</span></span><br><span class="line">sudo ceph auth del client.radosgw.gateway</span><br><span class="line">sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.eu-east-<span class="number">1</span> --gen-key</span><br><span class="line">sudo ceph-authtool -n client.radosgw.eu-east-<span class="number">1</span> --cap osd <span class="string">'allow rwx'</span> --cap mon <span class="string">'allow rw'</span> /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.eu-east-<span class="number">1</span> -i /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置apache2</span></span><br><span class="line">cat &lt;&lt; EOF &gt; rgw.conf</span><br><span class="line">FastCgiExternalServer /var/www/s3gw.fcgi -host localhost:<span class="number">9000</span></span><br><span class="line"></span><br><span class="line">&lt;VirtualHost *:<span class="number">80</span>&gt;</span><br><span class="line"></span><br><span class="line">    ServerName localhost</span><br><span class="line">    ServerAlias *.localhost</span><br><span class="line">    ServerAdmin qinghua@ggg.com</span><br><span class="line">    DocumentRoot /var/www</span><br><span class="line">    RewriteEngine On</span><br><span class="line">    RewriteRule  ^/(.*) /s3gw.fcgi?%&#123;QUERY_STRING&#125; [E=HTTP_AUTHORIZATION:%&#123;HTTP:Authorization&#125;,L]</span><br><span class="line"></span><br><span class="line">    &lt;IfModule mod_fastcgi.c&gt;</span><br><span class="line">           &lt;Directory /var/www&gt;</span><br><span class="line">            Options +ExecCGI</span><br><span class="line">            AllowOverride All</span><br><span class="line">            SetHandler fastcgi-script</span><br><span class="line">            Order allow,deny</span><br><span class="line">            Allow from all</span><br><span class="line">            AuthBasicAuthoritative Off</span><br><span class="line">        &lt;/Directory&gt;</span><br><span class="line">    &lt;/IfModule&gt;</span><br><span class="line"></span><br><span class="line">    AllowEncodedSlashes On</span><br><span class="line">    ErrorLog /var/<span class="built_in">log</span>/apache2/error.log</span><br><span class="line">    CustomLog /var/<span class="built_in">log</span>/apache2/access.log combined</span><br><span class="line">    ServerSignature Off</span><br><span class="line"></span><br><span class="line">&lt;/VirtualHost&gt;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv rgw.conf /etc/apache2/conf-enabled/rgw.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置FastCGI</span></span><br><span class="line">cat &lt;&lt; EOF &gt; s3gw.fcgi</span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line"><span class="built_in">exec</span> /usr/bin/radosgw -c /etc/ceph/ceph.conf -n client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv s3gw.fcgi /var/www/s3gw.fcgi</span><br><span class="line">sudo chmod +x /var/www/s3gw.fcgi</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置ceph</span></span><br><span class="line">sudo sed -i <span class="string">'$a rgw region root pool = .eu.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zonegroup root pool = .eu.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a [client.radosgw.eu-east-1]'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw region = eu'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone = eu-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw zone root pool = .eu-east.rgw.root'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw dns name = eu-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a host = eu-east'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a keyring = /etc/ceph/ceph.client.radosgw.keyring'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw socket path = ""'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a log file = /var/log/radosgw/client.radosgw.eu-east-1.log'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw frontends = fastcgi socket_port=9000 socket_host=0.0.0.0'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw print continue = false'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure></p>
<p>接下来就是辖区和域的配置了。需要设置us的辖区和eu自己的辖区，否则会报错：<strong>AssertionError: No master zone found for region default</strong>。但是域只用设置eu自己的就好：<br><figure class="highlight sh"><figcaption><span>eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ceph bash</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; us.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"us"</span>,</span><br><span class="line">  <span class="string">"api_name"</span>: <span class="string">"us"</span>,</span><br><span class="line">  <span class="string">"is_master"</span>: <span class="string">"true"</span>,</span><br><span class="line">  <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.15:80\/"</span>],</span><br><span class="line">  <span class="string">"master_zone"</span>: <span class="string">"us-east"</span>,</span><br><span class="line">  <span class="string">"zones"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"us-east"</span>,</span><br><span class="line">      <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.15:80\/"</span>],</span><br><span class="line">      <span class="string">"log_meta"</span>: <span class="string">"true"</span>,</span><br><span class="line">      <span class="string">"log_data"</span>: <span class="string">"true"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"us-west"</span>,</span><br><span class="line">      <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.16:80\/"</span>],</span><br><span class="line">      <span class="string">"log_meta"</span>: <span class="string">"true"</span>,</span><br><span class="line">      <span class="string">"log_data"</span>: <span class="string">"true"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"placement_targets"</span>: [</span><br><span class="line">   &#123;</span><br><span class="line">     <span class="string">"name"</span>: <span class="string">"default-placement"</span>,</span><br><span class="line">     <span class="string">"tags"</span>: []</span><br><span class="line">   &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"default_placement"</span>: <span class="string">"default-placement"</span></span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; eu.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"eu"</span>,</span><br><span class="line">  <span class="string">"api_name"</span>: <span class="string">"eu"</span>,</span><br><span class="line">  <span class="string">"is_master"</span>: <span class="string">"false"</span>,</span><br><span class="line">  <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.15:80\/"</span>],</span><br><span class="line">  <span class="string">"master_zone"</span>: <span class="string">"eu-east"</span>,</span><br><span class="line">  <span class="string">"zones"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"eu-east"</span>,</span><br><span class="line">      <span class="string">"endpoints"</span>: [<span class="string">"http:\/\/192.168.33.17:80\/"</span>],</span><br><span class="line">      <span class="string">"log_meta"</span>: <span class="string">"true"</span>,</span><br><span class="line">      <span class="string">"log_data"</span>: <span class="string">"true"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"placement_targets"</span>: [</span><br><span class="line">   &#123;</span><br><span class="line">     <span class="string">"name"</span>: <span class="string">"default-placement"</span>,</span><br><span class="line">     <span class="string">"tags"</span>: []</span><br><span class="line">   &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"default_placement"</span>: <span class="string">"default-placement"</span></span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置辖区</span></span><br><span class="line">radosgw-admin region <span class="built_in">set</span> --infile us.json --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">radosgw-admin region <span class="built_in">set</span> --infile eu.json --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line">radosgw-admin region default --rgw-region=eu --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; eu-east.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"domain_root"</span>: <span class="string">".eu-east.domain.rgw"</span>,</span><br><span class="line">  <span class="string">"control_pool"</span>: <span class="string">".eu-east.rgw.control"</span>,</span><br><span class="line">  <span class="string">"gc_pool"</span>: <span class="string">".eu-east.rgw.gc"</span>,</span><br><span class="line">  <span class="string">"log_pool"</span>: <span class="string">".eu-east.log"</span>,</span><br><span class="line">  <span class="string">"intent_log_pool"</span>: <span class="string">".eu-east.intent-log"</span>,</span><br><span class="line">  <span class="string">"usage_log_pool"</span>: <span class="string">".eu-east.usage"</span>,</span><br><span class="line">  <span class="string">"user_keys_pool"</span>: <span class="string">".eu-east.users"</span>,</span><br><span class="line">  <span class="string">"user_email_pool"</span>: <span class="string">".eu-east.users.email"</span>,</span><br><span class="line">  <span class="string">"user_swift_pool"</span>: <span class="string">".eu-east.users.swift"</span>,</span><br><span class="line">  <span class="string">"user_uid_pool"</span>: <span class="string">".eu-east.users.uid"</span>,</span><br><span class="line">  <span class="string">"system_key"</span>: &#123;</span><br><span class="line">    <span class="string">"access_key"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"secret_key"</span>: <span class="string">""</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"placement_pools"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"key"</span>: <span class="string">"default-placement"</span>,</span><br><span class="line">      <span class="string">"val"</span>: &#123;</span><br><span class="line">        <span class="string">"index_pool"</span>: <span class="string">".eu-east.rgw.buckets.index"</span>,</span><br><span class="line">        <span class="string">"data_pool"</span>: <span class="string">".eu-east.rgw.buckets"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置域</span></span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=eu-east --infile eu-east.json --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line">radosgw-admin regionmap update --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建用户</span></span><br><span class="line">radosgw-admin user create --uid=<span class="string">"eu-east"</span> --display-name=<span class="string">"Region-EU Zone-East"</span> --name client.radosgw.eu-east-<span class="number">1</span> --system | tee eastuser.txt</span><br><span class="line"><span class="built_in">export</span> SRC_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> eastuser.txt`</span><br><span class="line"><span class="built_in">export</span> SRC_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> eastuser.txt`</span><br><span class="line">sed -i <span class="string">"s/access_key\": \"/access_key\": \"<span class="variable">$SRC_ACCESS_KEY</span>/g"</span> eu-east.json</span><br><span class="line">sed -i <span class="string">"s/secret_key\": \"/secret_key\": \"<span class="variable">$SRC_SECRET_KEY</span>/g"</span> eu-east.json</span><br><span class="line"></span><br><span class="line">radosgw-admin zone <span class="built_in">set</span> --rgw-zone=eu-east --infile eu-east.json --name client.radosgw.eu-east-<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>都完成了以后，就可以重启ceph服务和apache2啦：<br><figure class="highlight sh"><figcaption><span>eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker restart ceph</span><br><span class="line">sudo /etc/init.d/radosgw start</span><br><span class="line">sudo a2enmod rewrite</span><br><span class="line">sudo service apache2 restart</span><br></pre></td></tr></table></figure></p>
<p>最后同步元数据：<br><figure class="highlight sh"><figcaption><span>eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">scp vagrant@<span class="number">192.168</span>.<span class="number">33.15</span>:/home/vagrant/us-east.json .    <span class="comment"># vagrant的密码也是vagrant</span></span><br><span class="line"></span><br><span class="line">docker cp ceph:eu-east.json eu-east.json</span><br><span class="line"><span class="built_in">export</span> SRC_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> us-east.json`</span><br><span class="line"><span class="built_in">export</span> SRC_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> us-east.json`</span><br><span class="line"><span class="built_in">export</span> DEST_ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> eu-east.json`</span><br><span class="line"><span class="built_in">export</span> DEST_SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> eu-east.json`</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; cluster-data-sync.conf</span><br><span class="line">src_zone: us-east</span><br><span class="line"><span class="built_in">source</span>: http://<span class="number">192.168</span>.<span class="number">33.15</span></span><br><span class="line">src_access_key: <span class="variable">$SRC_ACCESS_KEY</span></span><br><span class="line">src_secret_key: <span class="variable">$SRC_SECRET_KEY</span></span><br><span class="line">dest_zone: eu-east</span><br><span class="line">destination: http://<span class="number">192.168</span>.<span class="number">33.17</span></span><br><span class="line">dest_access_key: <span class="variable">$DEST_ACCESS_KEY</span></span><br><span class="line">dest_secret_key: <span class="variable">$DEST_SECRET_KEY</span></span><br><span class="line"><span class="built_in">log</span>_file: /var/<span class="built_in">log</span>/radosgw/radosgw-sync-eu-east-west.log</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo radosgw-agent -c cluster-data-sync.conf --metadata-only</span><br></pre></td></tr></table></figure></p>
<p>如果不加<code>--metadata-only</code>，则会报错：<strong>ERROR:root:data sync can only occur between zones in the same region</strong>。同步完成后，我们运行以下命令查看现在<code>eu-east-1</code>实例里的存储桶：<br><figure class="highlight sh"><figcaption><span>eu-east</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin metadata bucket list --name client.radosgw.eu-east-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>应该能够看到先前在<code>us-east-1</code>创建的<code>my-new-bucket</code>。下面是一张不同辖区同步示意图：<br><img src="http://docs.ceph.com/docs/master/_images/region-sync.png" alt=""></p>
<h2 id="u5E38_u89C1_u547D_u4EE4"><a href="#u5E38_u89C1_u547D_u4EE4" class="headerlink" title="常见命令"></a>常见命令</h2><p>再介绍一些ceph的常见命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rados lspools                                                <span class="comment"># 列出所有的存储池</span></span><br><span class="line">rados ls -p .rgw.root                                        <span class="comment"># 列出.rgw.root存储池的所有对象</span></span><br><span class="line">rados get zone_info.default obj.txt -p .rgw.root             <span class="comment"># 将.rgw.root存储池的zone_info.default对象内容保存到obj.txt文件</span></span><br><span class="line">rados rm region_info.default -p .us.rgw.root                 <span class="comment"># 删除.us.rgw.root存储池的region_info.default对象</span></span><br><span class="line">radosgw-admin region list --name client.radosgw.us-east-<span class="number">1</span>    <span class="comment"># 列出client.radosgw.us-east-1实例的所有辖区</span></span><br><span class="line">radosgw-admin region get --name client.radosgw.us-east-<span class="number">1</span>     <span class="comment"># 查看client.radosgw.us-east-1实例的主辖区</span></span><br><span class="line">radosgw-admin zone list --name client.radosgw.us-east-<span class="number">1</span>      <span class="comment"># 列出client.radosgw.us-east-1实例的所有域</span></span><br><span class="line">radosgw-admin zone get --name client.radosgw.us-east-<span class="number">1</span>       <span class="comment"># 查看client.radosgw.us-east-1实例的主域</span></span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[光学字符识别软件tesseract-ocr]]></title>
      <url>http://qinghua.github.io/tesseract/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/tesseract-ocr/tesseract/wiki" target="_blank" rel="external">Tesseract</a>是一个可以将图片转换成文字的<a href="https://zh.wikipedia.org/wiki/%E5%85%89%E5%AD%A6%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB" target="_blank" rel="external">OCR</a>（Optical Character Recognition）软件，支持包括中文简繁体的<a href="https://github.com/tesseract-ocr/tesseract/blob/master/doc/tesseract.1.asc#languages" target="_blank" rel="external">多种语言</a>，简单易用，可以用来识别验证码。让我们来看一看吧。<br><a id="more"></a></p>
<h2 id="u5B89_u88C5"><a href="#u5B89_u88C5" class="headerlink" title="安装"></a>安装</h2><p>Tesseract只是一个小应用程序，在mac里直接安装就好啦：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install tesseract</span><br></pre></td></tr></table></figure></p>
<p>下面是张包含了一些英文的图片：<br><img src="/img/test.png" alt=""></p>
<p>把图片保存到本地之后，使用以下命令将其转成文字：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tesseract ~/Downloads/test.png out</span><br><span class="line">cat out.txt</span><br></pre></td></tr></table></figure></p>
<p>可以看到，对于正常字体来说，粗体、斜体、大小字号等的识别率还是很不错的。如果是手写体的字体，识别率将会严重下降。</p>
<h2 id="u4E2D_u6587"><a href="#u4E2D_u6587" class="headerlink" title="中文"></a>中文</h2><p>Tesseract支持多种语言，不过除了英语以外，都必须先下载语言数据：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget -c https://github.com/tesseract-ocr/tessdata/blob/master/chi_sim.traineddata?raw=<span class="literal">true</span></span><br><span class="line">mv chi_sim.traineddata\?raw\=<span class="literal">true</span> /usr/<span class="built_in">local</span>/Cellar/tesseract/<span class="number">3.04</span>.<span class="number">00</span>/share/tessdata/chi_sim.traineddata</span><br></pre></td></tr></table></figure></p>
<p>从后缀名traineddata可以看出来，tesseract是可以通过训练来提高识别率的。网上有许多教程，有兴趣的朋友可以自行尝试。下面是张包含了一些中文的图片：<br><img src="/img/testcn.png" alt=""></p>
<p>把图片保存到本地之后，使用以下命令将其转成文字：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tesseract ~/Downloads/testcn.png out <span class="operator">-l</span> chi_sim</span><br><span class="line">cat out.txt</span><br><span class="line">tesseract ~/Downloads/testcn.png out <span class="operator">-l</span> eng+chi_sim</span><br><span class="line">cat out.txt</span><br></pre></td></tr></table></figure></p>
<p>毕竟汉字内容多，这回没有英文识别率那么高了，想要更加实用可能需要更多训练和校对。</p>
<h2 id="PDF"><a href="#PDF" class="headerlink" title="PDF"></a>PDF</h2><h3 id="u8F93_u51FA"><a href="#u8F93_u51FA" class="headerlink" title="输出"></a>输出</h3><p>很简单，在<code>tesseract</code>命令的最后面加上<code>pdf</code>就好了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tesseract ~/Downloads/test.png out pdf</span><br><span class="line">open out.pdf</span><br></pre></td></tr></table></figure></p>
<h3 id="u8F93_u5165"><a href="#u8F93_u5165" class="headerlink" title="输入"></a>输入</h3><p>虽然tesseract不能直接处理PDF，但是借助<a href="https://www.imagemagick.org/script/index.php" target="_blank" rel="external">ImageMagick</a>和<a href="http://www.ghostscript.com/" target="_blank" rel="external">Ghostscript</a>可以轻松地把PDF转换成图片文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brew install imagemagick</span><br><span class="line">brew install ghostscript</span><br><span class="line">convert -density <span class="number">100</span> -trim input.pdf output%<span class="number">04</span>d.jpg</span><br></pre></td></tr></table></figure></p>
<p>这里的100表示DPI，<code>%04d</code>表示分页储存。有了图片之后就可以用tesseract随意操作啦。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[轻松搭建OpenStack Swift存储测试环境]]></title>
      <url>http://qinghua.github.io/openstack-swift/</url>
      <content type="html"><![CDATA[<p><a href="http://docs.openstack.org/developer/swift/" target="_blank" rel="external">Swift</a>（OpenStack Object Storage）是Rackspace开发的高可用分布式对象存储，贡献给了<a href="http://www.openstack.org/" target="_blank" rel="external">OpenStack</a>。上次在<a href="/ceph-radosgw">《通过RADOSGW提供ceph的S3和Swift接口》</a>一文里介绍了ceph RADOSGW的Swift接口，这次让我们直接来试试原生的swift吧！<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配一台IP为<strong>192.168.33.17</strong>，内存为1G的虚拟机。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line">config.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">  v.memory = <span class="number">1024</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后终端运行以下命令启动并连接虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>这里参考了Swift的官方文档<a href="http://docs.openstack.org/developer/swift/development_saio.html" target="_blank" rel="external">Swift All In One</a>来搭建一个swift测试环境。首先需要安装各种依赖包：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y install curl gcc memcached rsync sqlite3 xfsprogs \</span><br><span class="line">                        git-core libffi-dev python-setuptools \</span><br><span class="line">                        liberasurecode-dev</span><br><span class="line">sudo apt-get -y install python-coverage python-dev python-nose \</span><br><span class="line">                        python-xattr python-eventlet \</span><br><span class="line">                        python-greenlet python-pastedeploy \</span><br><span class="line">                        python-netifaces python-pip python-dnspython \</span><br><span class="line">                        python-mock</span><br><span class="line">sudo pip install --upgrade pip</span><br></pre></td></tr></table></figure></p>
<p>这里就不用<code>fdisk</code>而使用较简单的环回设备来当做我们的存储：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /srv</span><br><span class="line">sudo truncate <span class="operator">-s</span> <span class="number">1</span>GB /srv/swift-disk</span><br><span class="line">sudo mkfs.xfs /srv/swift-disk</span><br><span class="line">sudo sh -c <span class="string">'echo "/srv/swift-disk /mnt/sdb1 xfs loop,noatime,nodiratime,nobarrier,logbufs=8 0 0" &gt;&gt; /etc/fstab'</span></span><br></pre></td></tr></table></figure></p>
<p>生成挂载点和link：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /mnt/sdb1</span><br><span class="line">sudo mount /mnt/sdb1</span><br><span class="line">sudo mkdir /mnt/sdb1/<span class="number">1</span> /mnt/sdb1/<span class="number">2</span> /mnt/sdb1/<span class="number">3</span> /mnt/sdb1/<span class="number">4</span></span><br><span class="line">sudo chown vagrant:vagrant /mnt/sdb1/*</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> &#123;<span class="number">1</span>..<span class="number">4</span>&#125;; <span class="keyword">do</span> sudo ln <span class="operator">-s</span> /mnt/sdb1/<span class="variable">$x</span> /srv/<span class="variable">$x</span>; <span class="keyword">done</span></span><br><span class="line">sudo mkdir -p /srv/<span class="number">1</span>/node/sdb1 /srv/<span class="number">1</span>/node/sdb5 \</span><br><span class="line">              /srv/<span class="number">2</span>/node/sdb2 /srv/<span class="number">2</span>/node/sdb6 \</span><br><span class="line">              /srv/<span class="number">3</span>/node/sdb3 /srv/<span class="number">3</span>/node/sdb7 \</span><br><span class="line">              /srv/<span class="number">4</span>/node/sdb4 /srv/<span class="number">4</span>/node/sdb8 \</span><br><span class="line">              /var/run/swift</span><br><span class="line">sudo chown -R vagrant:vagrant /var/run/swift</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> &#123;<span class="number">1</span>..<span class="number">4</span>&#125;; <span class="keyword">do</span> sudo chown -R vagrant:vagrant /srv/<span class="variable">$x</span>/; <span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>在<code>rc.local</code>里增加几条创建文件夹和授权的命令，使之能够被开机执行：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'$i mkdir -p /var/cache/swift /var/cache/swift2 /var/cache/swift3 /var/cache/swift4'</span> /etc/rc.local</span><br><span class="line">sudo sed -i <span class="string">'$i chown vagrant:vagrant /var/cache/swift*'</span> /etc/rc.local</span><br><span class="line">sudo sed -i <span class="string">'$i mkdir -p /var/run/swift'</span> /etc/rc.local</span><br><span class="line">sudo sed -i <span class="string">'$i chown vagrant:vagrant /var/run/swift'</span> /etc/rc.local</span><br></pre></td></tr></table></figure></p>
<p>接下来需要安装swift和它的客户端：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>; git <span class="built_in">clone</span> https://github.com/openstack/python-swiftclient.git</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/python-swiftclient; git checkout <span class="number">2.7</span>.<span class="number">0</span>; sudo python setup.py develop; <span class="built_in">cd</span> -</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openstack/swift.git</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/swift; git checkout <span class="number">2.7</span>.<span class="number">0</span></span><br><span class="line">sed -i <span class="string">"s/;python_version&lt;'3.0'//"</span> requirements.txt</span><br><span class="line">sed -i <span class="string">"/dnspython3&gt;=1.12.0;python_version&gt;='3.0'/d"</span> requirements.txt</span><br><span class="line">sudo pip install -r requirements.txt; sudo python setup.py develop; sudo pip install -r <span class="built_in">test</span>-requirements.txt</span><br></pre></td></tr></table></figure></p>
<p>然后需要配置rsync：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp <span class="variable">$HOME</span>/swift/doc/saio/rsyncd.conf /etc/</span><br><span class="line">sudo sed -i <span class="string">"s/&lt;your-user-name&gt;/vagrant/"</span> /etc/rsyncd.conf</span><br><span class="line">sudo sed -i <span class="string">"s/RSYNC_ENABLE=false/RSYNC_ENABLE=true/"</span> /etc/default/rsync</span><br><span class="line">sudo service rsync restart</span><br></pre></td></tr></table></figure></p>
<p>使用以下命令来验证rsync，应该能看到一堆的account、container和object：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync rsync://pub@localhost/</span><br></pre></td></tr></table></figure></p>
<p>我们前面已经安装了memcached，验证一下服务是可用的：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service memcached status</span><br></pre></td></tr></table></figure></p>
<p>接下来需要配置各个节点：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/swift/doc; sudo cp -r saio/swift /etc/swift; <span class="built_in">cd</span> -</span><br><span class="line">sudo chown -R <span class="variable">$&#123;USER&#125;</span>:<span class="variable">$&#123;USER&#125;</span> /etc/swift</span><br><span class="line">find /etc/swift/ -name \*.conf | xargs sudo sed -i <span class="string">"s/&lt;your-user-name&gt;/<span class="variable">$&#123;USER&#125;</span>/"</span></span><br></pre></td></tr></table></figure></p>
<p>然后配置swift脚本，<code>/etc/swift/test.conf</code>为我们添加了三个测试账户：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="variable">$HOME</span>/bin</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/swift/doc; cp saio/bin/* <span class="variable">$HOME</span>/bin; <span class="built_in">cd</span> -</span><br><span class="line">chmod +x <span class="variable">$HOME</span>/bin/*</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"export SAIO_BLOCK_DEVICE=/srv/swift-disk"</span> &gt;&gt; <span class="variable">$HOME</span>/.bashrc</span><br><span class="line">sed -i <span class="string">"/^find/d"</span> <span class="variable">$HOME</span>/bin/resetswift</span><br><span class="line">cp <span class="variable">$HOME</span>/swift/<span class="built_in">test</span>/sample.conf /etc/swift/test.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"export SWIFT_TEST_CONFIG_FILE=/etc/swift/test.conf"</span> &gt;&gt; <span class="variable">$HOME</span>/.bashrc</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"export PATH=<span class="variable">$&#123;PATH&#125;</span>:<span class="variable">$HOME</span>/bin"</span> &gt;&gt; <span class="variable">$HOME</span>/.bashrc</span><br><span class="line">. <span class="variable">$HOME</span>/.bashrc</span><br></pre></td></tr></table></figure></p>
<p>Swift里有一个非常重要的概念，<a href="http://docs.openstack.org/developer/swift/overview_ring.html?highlight=ring" target="_blank" rel="external">ring</a>。通过它可以找到数据的物理位置。它的存储模型是这样的：一个账号（account）里可以有多个容器（container），容器里可以有许多个键值对，字典里的值称为对象（object）。账号和容器被存储在SQLite数据库里，而对象是以文件方式存储的。账号数据库、容器数据库和每个单独对象都有自己的ring。下面我们来构建一些ring：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">remakerings</span><br></pre></td></tr></table></figure></p>
<h2 id="u542F_u52A8_u73AF_u5883"><a href="#u542F_u52A8_u73AF_u5883" class="headerlink" title="启动环境"></a>启动环境</h2><p>现在我们就可以用<code>startmain</code>启动swift啦：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'s/bind_ip = 127.0.0.1/bind_ip = 192.168.33.17/'</span> /etc/swift/proxy-server.conf</span><br><span class="line">startmain</span><br></pre></td></tr></table></figure></p>
<p>然后用<code>test:tester/testing</code>这个预先创建好的测试账户登录：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -i -H <span class="string">'X-Storage-User: test:tester'</span> -H <span class="string">'X-Storage-Pass: testing'</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span>/auth/v1.<span class="number">0</span> | tee auth.txt</span><br><span class="line">sudo apt-get -y install dos2unix</span><br><span class="line">dos2unix auth.txt</span><br><span class="line"><span class="built_in">export</span> X_AUTH_TOKEN=`cat auth.txt | sed -n <span class="string">'s/X-Auth-Token: \(.*\)/\1/p'</span>`</span><br><span class="line"><span class="built_in">export</span> X_STORAGE_URL=`cat auth.txt | sed -n <span class="string">'s/X-Storage-Url: \(.*\)/\1/p'</span>`</span><br><span class="line">curl -v -H <span class="string">"X-Auth-Token: <span class="variable">$X_AUTH_TOKEN</span>"</span> <span class="variable">$X_STORAGE_URL</span></span><br></pre></td></tr></table></figure></p>
<p>上面用到<code>dos2unix</code>是因为取到的<code>X-Storage-Url</code>最后面带着<code>^M$</code>的特殊字符。直接管道的话，下一个<code>curl</code>会报错：<code>Illegal characters found in URL</code>。可以保存成文件之后使用<code>cat -A</code>来查看这些特殊字符。<br>登录完成后，就能看到swift的状态和所有容器啦：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span>/auth/v1.<span class="number">0</span> -U <span class="built_in">test</span>:tester -K testing <span class="built_in">stat</span></span><br><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span>/auth/v1.<span class="number">0</span> -U <span class="built_in">test</span>:tester -K testing list</span><br></pre></td></tr></table></figure></p>
<h2 id="u6D4B_u8BD5_u73AF_u5883"><a href="#u6D4B_u8BD5_u73AF_u5883" class="headerlink" title="测试环境"></a>测试环境</h2><h3 id="Swift_u6D4B_u8BD5"><a href="#Swift_u6D4B_u8BD5" class="headerlink" title="Swift测试"></a>Swift测试</h3><p>有兴趣的话，还可以运行下面的单元测试、功能测试和探索性测试：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HOME</span>/swift/.unittests</span><br><span class="line"><span class="variable">$HOME</span>/swift/.functests</span><br><span class="line"><span class="variable">$HOME</span>/swift/.probetests</span><br></pre></td></tr></table></figure></p>
<h3 id="Docker_Registry_u6D4B_u8BD5"><a href="#Docker_Registry_u6D4B_u8BD5" class="headerlink" title="Docker Registry测试"></a>Docker Registry测试</h3><p>在Docker Registry的<code>config.yml</code>里使用以下配置：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">storage:&#10;  swift:&#10;    username: test:tester &#10;    password: testing &#10;    authurl: http://192.168.33.17:8080/auth/v1.0&#10;    container: swift</span><br></pre></td></tr></table></figure></p>
<p>可以测试<code>docker push</code>啦。不过记得需要先登录用户哦。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用RADOSGW提供ceph的S3和Swift接口]]></title>
      <url>http://qinghua.github.io/ceph-radosgw/</url>
      <content type="html"><![CDATA[<p><a href="http://ceph.com/" target="_blank" rel="external">Ceph</a>是一个高性能的PB级分布式文件系统。它能够在一个系统中提供对象存储、块存储和文件存储。对如何加载使用这些存储感兴趣的话可以参考<a href="/ceph-demo">《用容器轻松搭建ceph实验环境》</a>。它还可以通过RADOSGW来实现S3和OpenStack Swift存储接口。不管RADOSGW还是块存储或文件存储都是基于对象存储来提供服务。本文的主要内容是如何通过RADOSGW来暴露S3和SWIFT接口。由于Docker Registry在2.4版本<a href="https://github.com/docker/distribution/commit/5967d333425a8dd5d36c5bb456098839654d38af" target="_blank" rel="external">移除了对rados的支持</a>，所以如果使用ceph作为后端存储就需要利用RADOSGW了。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配一台IP为<strong>192.168.33.111</strong>，内存为1G的虚拟机。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.111"</span></span><br><span class="line">config.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">  v.memory = <span class="number">1024</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后终端运行以下命令启动并连接虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>首先需要安装一些ceph、radosgw的依赖包，还有python-boto、swift客户端等工具可以用于测试。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y --force-yes install ceph-common radosgw python-boto</span><br><span class="line">sudo pip install --upgrade setuptools</span><br><span class="line">sudo pip install --upgrade python-swiftclient</span><br></pre></td></tr></table></figure></p>
<p>然后就可以启动ceph/demo这个容器来轻松提供ceph服务了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=host -v /etc/ceph:/etc/ceph <span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">33.111</span> <span class="operator">-e</span> CEPH_NETWORK=<span class="number">192.168</span>.<span class="number">33.111</span>/<span class="number">24</span> --name=ceph ceph/demo</span><br></pre></td></tr></table></figure></p>
<p>接下来的步骤主要参考的是有一点坑的<a href="http://docs.ceph.com/docs/master/radosgw/config/" target="_blank" rel="external">官方教程</a>。需要为radosgw生成一个名为<code>gateway</code>的用户：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph auth del client.radosgw.gateway</span><br><span class="line">sudo ceph auth get-or-create client.radosgw.gateway osd <span class="string">'allow rwx'</span> mon <span class="string">'allow rwx'</span> -o /etc/ceph/ceph.client.radosgw.keyring</span><br></pre></td></tr></table></figure></p>
<p>然后需要把这个用户加到<code>ceph.conf</code>配置里，提供端口为9000的<a href="https://en.wikipedia.org/wiki/FastCGI" target="_blank" rel="external">FastCGI</a>服务：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i $<span class="string">'$a \\\n'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a [client.radosgw.gateway]'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a host = vagrant-ubuntu-trusty-64'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a keyring = /etc/ceph/ceph.client.radosgw.keyring'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw socket path = ""'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a log file = /var/log/radosgw/client.radosgw.gateway.log'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw frontends = fastcgi socket_port=9000 socket_host=0.0.0.0'</span> /etc/ceph/ceph.conf</span><br><span class="line">sudo sed -i <span class="string">'$a rgw print continue = false'</span> /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure></p>
<p>其中第二行的<code>vagrant-ubuntu-trusty-64</code>，必须使用<code>hostname -s</code>得出的结果。如果是按照<a href="/ceph-radosgw/#u51C6_u5907_u5DE5_u4F5C">准备工作</a>的做法，是不需要变的。另外<a href="https://segmentfault.com/q/1010000000256516" target="_blank" rel="external">这里</a>的第一个回答非常清晰地解释了CGI和FastCGI。<br>配置完成后就可以重启ceph容器并启动radosgw：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker restart ceph</span><br><span class="line">sudo /etc/init.d/radosgw start</span><br></pre></td></tr></table></figure></p>
<p>为了提供HTTP服务，需要安装apache2（Red Hat系是httpd）：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get -y --force-yes install apache2</span><br></pre></td></tr></table></figure></p>
<p>接下来创建一个apache2的配置文件，监听80端口并把请求转发到radosgw提供的FastCGI 9000端口上：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; rgw.conf</span><br><span class="line">&lt;VirtualHost *:<span class="number">80</span>&gt;</span><br><span class="line">ServerName localhost</span><br><span class="line">DocumentRoot /var/www/html</span><br><span class="line"></span><br><span class="line">ErrorLog /var/<span class="built_in">log</span>/apache2/rgw_error.log</span><br><span class="line">CustomLog /var/<span class="built_in">log</span>/apache2/rgw_access.log combined</span><br><span class="line"></span><br><span class="line"><span class="comment"># LogLevel debug</span></span><br><span class="line"></span><br><span class="line">RewriteEngine On</span><br><span class="line"></span><br><span class="line">RewriteRule .* - [E=HTTP_AUTHORIZATION:%&#123;HTTP:Authorization&#125;,L]</span><br><span class="line"></span><br><span class="line">SetEnv proxy-nokeepalive <span class="number">1</span></span><br><span class="line"></span><br><span class="line">ProxyPass / fcgi://localhost:<span class="number">9000</span>/</span><br><span class="line"></span><br><span class="line">&lt;/VirtualHost&gt;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mv rgw.conf /etc/apache2/conf-enabled/rgw.conf</span><br></pre></td></tr></table></figure></p>
<p>由于上述配置需要用到一些apache2默认未加载的模块，所以需要加载并重新启动apache2：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo a2enmod rewrite</span><br><span class="line">sudo a2enmod proxy_http</span><br><span class="line">sudo a2enmod proxy_fcgi</span><br><span class="line">sudo service apache2 restart</span><br></pre></td></tr></table></figure></p>
<h2 id="u6D4B_u8BD5_u670D_u52A1"><a href="#u6D4B_u8BD5_u670D_u52A1" class="headerlink" title="测试服务"></a>测试服务</h2><h3 id="S3"><a href="#S3" class="headerlink" title="S3"></a>S3</h3><p>RADOSGW的基本配置已经完成，现在我们测试一下s3接口。它的存储模型是这样的：用户可以创建和管理多个<a href="http://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/UsingBucket.html" target="_blank" rel="external">存储桶（bucket）</a>，每个存储桶里可以存放无限多个对象（object），每个对象是一个键值对。存储桶的名称与区域无关，全球唯一。</p>
<p>接下来先创建一个s3用户：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin user create --uid=<span class="string">"testuser"</span> --display-name=<span class="string">"First User"</span> | tee user.txt</span><br><span class="line"><span class="built_in">export</span> ACCESS_KEY=`sed -n <span class="string">'s/ *"access_key": "\(.*\)",/\1/p'</span> user.txt`</span><br><span class="line"><span class="built_in">export</span> SECRET_KEY=`sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span> user.txt`</span><br></pre></td></tr></table></figure></p>
<p>使用以下python代码来测试我们的s3接口是否已经可用：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; s3test.py</span><br><span class="line">import boto</span><br><span class="line">import boto.s3.connection</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">access_key = os.environ[<span class="string">"ACCESS_KEY"</span>]</span><br><span class="line">secret_key = os.environ[<span class="string">"SECRET_KEY"</span>]</span><br><span class="line">conn = boto.connect_s3(</span><br><span class="line">aws_access_key_id = access_key,</span><br><span class="line">aws_secret_access_key = secret_key,</span><br><span class="line">host = <span class="string">'192.168.33.111'</span>,</span><br><span class="line">is_secure=False,</span><br><span class="line">calling_format = boto.s3.connection.OrdinaryCallingFormat(),</span><br><span class="line">)</span><br><span class="line">bucket = conn.create_bucket(<span class="string">'my-new-bucket'</span>)</span><br><span class="line"><span class="keyword">for</span> bucket <span class="keyword">in</span> conn.get_all_buckets():</span><br><span class="line">    <span class="built_in">print</span> <span class="string">"&#123;name&#125;\t&#123;created&#125;"</span>.format(</span><br><span class="line">        name = bucket.name,</span><br><span class="line">        created = bucket.creation_date,</span><br><span class="line">)</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">python s3test.py</span><br></pre></td></tr></table></figure></p>
<p>如果显示了<code>my-new-bucket</code>，那就说明测试成功地通过s3接口创建了一个存储桶。可以使用以下命令来获取这个存储桶和实例的信息：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin metadata bucket list</span><br><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin metadata get bucket:my-new-bucket  | tee bucket.txt</span><br><span class="line"><span class="built_in">export</span> BUCKET_ID=`cat bucket.txt | sed -n <span class="string">'s/ *"bucket_id": "\(.*\)"/\1/p'</span>`</span><br><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin metadata get bucket.instance:my-new-bucket:<span class="variable">$BUCKET_ID</span></span><br></pre></td></tr></table></figure></p>
<p>还可以修改实例的信息并PUT回去，具体做法可参见<a href="http://blog.widodh.nl/2013/11/changing-the-region-of-a-rgw-bucket/" target="_blank" rel="external">《Changing the region of a RGW bucket》</a>。</p>
<h3 id="Swift"><a href="#Swift" class="headerlink" title="Swift"></a>Swift</h3><p>接下来测试swift。对于swift来说，它的存储模型是这样的：一个账号（account）里可以有多个容器（container），容器里可以有许多个键值对，字典里的值称为对象（object）。账号和容器被存储在SQLite数据库里，而对象是以文件方式存储的。</p>
<p>首先需要创建swift用户并生成secret：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin subuser create --uid=testuser --subuser=testuser:swift --access=full</span><br><span class="line">docker <span class="built_in">exec</span> ceph radosgw-admin key create --subuser=testuser:swift --key-type=swift --gen-secret | tee subuser.txt</span><br><span class="line"><span class="built_in">export</span> PASSWORD=`sed -n <span class="string">'/testuser:swift/&#123;N;p;&#125;'</span> subuser.txt | sed -n <span class="string">'s/ *"secret_key": "\(.*\)"/\1/p'</span>`</span><br></pre></td></tr></table></figure></p>
<p>然后就可以用以下命令查看swift里所有的容器：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> list</span><br></pre></td></tr></table></figure></p>
<p>应该能看到刚才测试s3接口时创建的<code>my-new-bucket</code>，在这里s3的存储桶和swift的容器是同一个概念。接下来我们自己创建容器：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> post qinghua</span><br><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> list qinghua</span><br></pre></td></tr></table></figure></p>
<p>创建成功，里面没有文件。现在可以上传、下载文件试试：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World &gt; hw.txt</span><br><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> upload qinghua hw.txt</span><br><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> list qinghua</span><br><span class="line">mv hw.txt hw.bak</span><br><span class="line">swift -A http://<span class="number">192.168</span>.<span class="number">33.111</span>/auth/v1.<span class="number">0</span> -U testuser:swift -K <span class="variable">$PASSWORD</span> download qinghua hw.txt</span><br><span class="line">cat hw.txt</span><br></pre></td></tr></table></figure></p>
<p>搞定！</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[用容器轻松搭建Rancher运行环境]]></title>
      <url>http://qinghua.github.io/rancher/</url>
      <content type="html"><![CDATA[<p><a href="http://rancher.com/" target="_blank" rel="external">Rancher</a>是开源的容器平台，功能齐全，部署简单，支持Kubernets和Docker Swarm。它把自己定位在持续交付流水线上的后半段上，如下图所示:<br><img src="/img/rancher-feature.png" alt=""></p>
<p>2016年3月底刚刚发布了1.0正式版。借着这个契机，下面就让我们用容器来部署一套Rancher环境试试它的功能吧！<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配三台虚拟机，一台叫做<strong>server</strong>，它的IP是<strong>192.168.33.17</strong>；另两台分别是<strong>agent1</strong>和<strong>agent2</strong>，它们的IP是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"server"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"server"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">1024</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"agent1"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"agent1"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">1024</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"agent2"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"agent2"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">1024</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后分别在三个终端运行以下命令启动并连接三台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh server</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh agent1</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh agent2</span><br></pre></td></tr></table></figure>
<p>如果想要在接下来的步骤中获得良好体验，建议先下载以下镜像：<br><figure class="highlight sh"><figcaption><span>server agent1 agent2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker pull rancher/server:v1.<span class="number">0.0</span></span><br><span class="line">docker pull rancher/agent:v0.<span class="number">11.0</span></span><br><span class="line">docker pull rancher/agent-instance:v0.<span class="number">8.1</span></span><br><span class="line">docker pull tomcat:<span class="number">8.0</span>.<span class="number">30</span>-jre8</span><br><span class="line">docker pull busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br><span class="line">docker pull mysql:<span class="number">5.7</span>.<span class="number">10</span></span><br><span class="line">docker pull wordpress:<span class="number">4.4</span>.<span class="number">2</span></span><br><span class="line">docker pull rancher/etcd:v2.<span class="number">3.0</span></span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>启动Rancher服务器相当简单，一条命令而已：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --name=rs \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -p <span class="number">8080</span>:<span class="number">8080</span> \</span><br><span class="line">    rancher/server:v1.<span class="number">0.0</span></span><br></pre></td></tr></table></figure></p>
<p>稍待片刻，就可以访问Rancher主页<a href="http://192.168.33.17:8080" target="_blank" rel="external">http://192.168.33.17:8080</a>了：<br><img src="/img/rancher-applications.jpg" alt=""></p>
<p>不像其他的web应用一开始没有数据时都是显示一片空白，Rancher展示了非常丰富的信息来帮助我们尽快上手。菜单上的<strong>ADMIN</strong>有个红色的感叹号，这是因为我们刚启动服务器，还没有配置认证信息。点击这个感叹号就可以开始配置，除了本地设置用户名密码以外，还支持与AD、GitHub和LDAP的集成。这里我们更加关注容器管理部分，对鉴权有兴趣的朋友可以自行尝试认证信息的配置。点击菜单上的<strong>INFRASTRUCTURE</strong>并点击<strong>Add Host</strong>按钮，可以增加一个agent host。由于现在我们用的是内部IP<strong>192.168.33.17</strong>，Rancher会提示我们是否真的连接到这里，不用管它直接点击<strong>Save</strong>按钮就可以了。复制下一个页面中第5步的命令，在agent上1运行即可。在我的虚拟机上是这样子的：<br><figure class="highlight sh"><figcaption><span>agent1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run <span class="operator">-d</span> --privileged -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/rancher:/var/lib/rancher rancher/agent:v0.<span class="number">11.0</span> http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span>/v1/scripts/B9EAC6780C8126FB739E:<span class="number">1460016000000</span>:Adj9D4Qp3smSmIscdUVT0JSCPdM</span><br></pre></td></tr></table></figure></p>
<p>然后就可以点击<strong>Close</strong>，稍待片刻，就能看到agent1已经被加入到Hosts里了：<br><img src="/img/rancher-infrastructure.jpg" alt=""></p>
<p>在agent2上重复执行一遍命令，把agent2也加入到Hosts里。Rancher的server和agent都是设置为<code>restart=true</code>的，所以重启虚拟机之类的行为也不会影响Rancher正常工作。现在看到的Hosts应该是这样的：<br><img src="/img/rancher-hosts.jpg" alt=""></p>
<h2 id="u8FD0_u884C_u5BB9_u5668"><a href="#u8FD0_u884C_u5BB9_u5668" class="headerlink" title="运行容器"></a>运行容器</h2><p>接下来运行一个tomcat容器试试。点击agent1上的<strong>Add Container</strong>按钮，如下填入参数：</p>
<ul>
<li><strong>Name</strong>：tomcat</li>
<li><strong>Select Image</strong>：tomcat:8.0.30-jre8</li>
<li><strong>Public (on Host) IP/Port</strong>：8080</li>
<li><strong>Private (in Container) Port</strong>：8080</li>
</ul>
<p>然后点击最下方的<strong>Create</strong>按钮：<br><img src="/img/rancher-add-container.jpg" alt=""></p>
<p>过一段时间，便能看到如下的容器已经启动完成了：<br><img src="/img/rancher-standalone-container.jpg" alt=""></p>
<p>之所以需要等一段时间，是因为它像kubernetes一样，需要给容器配一个网络代理Network Agent，不过功能要复杂得多，拥有跨网络通信、健康检查等功能。当前版本下使用的网络代理镜像为<code>rancher/agent-instance:v0.8.1</code>。在agent1上运行<code>docker ps</code>便能看到这两个容器。还可以通过<a href="http://192.168.33.18:8080" target="_blank" rel="external">http://192.168.33.18:8080</a>来访问tomcat服务。在页面上点击某个容器比如tomcat，可以看到容器的基本信息和一些基本监控数据。如图：<br><img src="/img/rancher-tomcat-container.jpg" alt=""></p>
<p>自行启动的容器也能被Rancher监控到。我们来启动一个小容器：<br><figure class="highlight sh"><figcaption><span>agent1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --name=bb busybox:<span class="number">1.24</span>.<span class="number">1</span> sleep <span class="number">3600</span></span><br></pre></td></tr></table></figure></p>
<p>在界面上便能看到这个bb容器已经启动完成了：<br><img src="/img/rancher-self-container.jpg" alt=""></p>
<p>通过Rancher启动的容器IP是在<code>10.42.*.*</code>区间的，自行启动的bb容器的IP是在它之外的。如果想用相同IP段，可以使用以下命令：<br><figure class="highlight sh"><figcaption><span>agent1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --name=bb2 --label io.rancher.container.network=<span class="literal">true</span> busybox:<span class="number">1.24</span>.<span class="number">1</span> sleep <span class="number">3600</span></span><br></pre></td></tr></table></figure></p>
<p>在界面上可以看到bb2容器的IP已经落入区间了：<br><img src="/img/rancher-self-container2.jpg" alt=""></p>
<h2 id="u8FD0_u884C_u5E94_u7528"><a href="#u8FD0_u884C_u5E94_u7528" class="headerlink" title="运行应用"></a>运行应用</h2><p>上面我们在指定的虚拟机上创建容器。不过对于一个真实的网络应用，我们并不关心它运行在哪里，只关心服务地址罢了。下面我们来创建一个这样的WordPress应用。它包含一个MySQL数据库，两个WordPress实例和一套负载均衡。首先点击<strong>APPLICATIONS</strong>，然后点击Default的<strong>Add Service</strong>。填入：</p>
<ul>
<li><strong>Name</strong>：database</li>
<li><strong>Select Image</strong>：mysql:5.7.10</li>
<li><strong>Always pull image before creating</strong>：false</li>
<li><strong>Environment Vars</strong>：MYSQL_ROOT_PASSWORD=pass1</li>
</ul>
<p>然后点击<strong>Create</strong>来创建这个MySQL服务。接下来是WordPress，还是像MySQL那样新建服务。填入：</p>
<ul>
<li><strong>Scale</strong>：2</li>
<li><strong>Name</strong>：mywordpress</li>
<li><strong>Select Image</strong>：wordpress:4.4.2</li>
<li><strong>Always pull image before creating</strong>：false</li>
<li><strong>Service Links</strong>：database &gt; mysql</li>
</ul>
<p>然后点击<strong>Create</strong>来创建这个WordPress服务。最后是负载均衡，点击<strong>Add Service</strong>旁边的向下箭头，选择<strong>Add Load Balancer</strong>。填入：</p>
<ul>
<li><strong>Scale</strong>：Always run one instance of this container on every host</li>
<li><strong>Name</strong>：wordpresslb</li>
<li><strong>Source IP/Port</strong>：80</li>
<li><strong>Default Target Port</strong>：80</li>
<li><strong>Target Service</strong>：mywordpress</li>
</ul>
<p>点击<strong>Save</strong>来创建这个负载均衡。稍待片刻，就可以看到wordpresslb变为Active状态了，然后就可以访问<a href="http://192.168.33.18" target="_blank" rel="external">http://192.168.33.18</a>或<a href="http://192.168.33.19" target="_blank" rel="external">http://192.168.33.19</a>来使用WordPress服务了：<br><img src="/img/wordpress.jpg" alt=""></p>
<p>Rancher负载均衡使用和网络代理一样的<code>rancher/agent-instance</code>镜像。它内置了HAProxy，默认使用轮询。</p>
<h2 id="u9884_u7F6E_u6A21_u677F"><a href="#u9884_u7F6E_u6A21_u677F" class="headerlink" title="预置模板"></a>预置模板</h2><p>点击<strong>CATALOG</strong>，便能看到Rancher为我们预置了一系列的应用模板。我们用个小镜像Etcd试试。首先找到Etcd的图标：<br><img src="/img/rancher-etcd.jpg" alt=""></p>
<p>点击<strong>View Details</strong>进入etcd详细页面，滚动到最下方。由于我们只有两个agent，在<strong>Number of Nodes</strong>里填入1，然后点击<strong>Launch</strong>按钮。很快，一个etcd服务就启动起来了。按如下参数给这个服务增加一套负载均衡：</p>
<ul>
<li><strong>Scale</strong>：Always run one instance of this container on every host</li>
<li><strong>Name</strong>：etcdlb</li>
<li><strong>Source IP/Port</strong>：2379</li>
<li><strong>Protocol</strong>：tcp</li>
<li><strong>Default Target Port</strong>：2379</li>
<li><strong>Target Service</strong>：etcd</li>
</ul>
<p>还可以点击<strong>Preview</strong>来查看<code>docker-compose.yml</code>和<code>rancher-compose.yml</code>文件，里面也有比较详细的注释。<code>docker-compose.yml</code>不必多说，<code>rancher-compose.yml</code>类似于它但更小一些。可以在任何Rancher页面的右下方点击<strong>Download CLI</strong>来下载rancher compose命令行工具，这样就可以通过命令行而非在网页上点来点去来管理容器和服务了。最后点击<strong>Save</strong>并等待负载均衡启动完成，就可以访问啦：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -L http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2379</span>/version</span><br><span class="line">curl -L http://<span class="number">192.168</span>.<span class="number">33.19</span>:<span class="number">2379</span>/version</span><br></pre></td></tr></table></figure></p>
<p>太方便了，简直是爽得不能不能的。最后送上全家福大图一张：<br><img src="/img/rancher-applications-stack.jpg" alt=""></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Mac上的git图形工具GitUp]]></title>
      <url>http://qinghua.github.io/gitup/</url>
      <content type="html"><![CDATA[<p>已经11岁的<a href="https://git-scm.com/" target="_blank" rel="external">Git</a>现在应该算是最流行的版本管理系统了。不过它的上手过程略令人感伤：为什么要用<code>git reset HEAD</code>而不是<code>git unadd/unstage</code>？Mac的朋友们有福了，<a href="http://gitup.co/" target="_blank" rel="external">GitUp</a>来拯救懒程序员们啦。它提供了一个简约而不简单的界面，让我们可以凭直觉轻松地打出git组合拳来处理各种状况。在2016年4月的<a href="https://www.thoughtworks.com/radar/tools/gitup" target="_blank" rel="external">ThoughtWorks技术雷达</a>上，它处于试验阶段，也就是值得追求，建议尝试。让我们来看看它有什么能力吧。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u73AF_u5883"><a href="#u51C6_u5907_u73AF_u5883" class="headerlink" title="准备环境"></a>准备环境</h2><p>GitUp只是一个小应用程序，下载下来就能用啦：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir gitup</span><br><span class="line"><span class="built_in">cd</span> gitup</span><br><span class="line">wget -c https://s3-us-west-<span class="number">2</span>.amazonaws.com/gitup-builds/stable/GitUp.zip</span><br><span class="line">unzip GitUp.zip</span><br><span class="line">open GitUp.app</span><br></pre></td></tr></table></figure></p>
<p>打开GitUp就能看见下面的界面：<br><img src="/img/gitup-welcome.jpg" alt=""></p>
<p>然后我们新建一个git repo：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">git init</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> a &gt; a.txt</span><br><span class="line"><span class="built_in">echo</span> b &gt; b.txt</span><br><span class="line"><span class="built_in">echo</span> c &gt; c.txt</span><br><span class="line">git add a.txt</span><br><span class="line">git commit -m <span class="string">"a"</span></span><br><span class="line">git add b.txt</span><br><span class="line">git commit -m <span class="string">"bb"</span></span><br><span class="line">git add c.txt</span><br><span class="line">git commit -m <span class="string">"c"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="u67E5_u770B_u5386_u53F2"><a href="#u67E5_u770B_u5386_u53F2" class="headerlink" title="查看历史"></a>查看历史</h2><p>在刚才的GitUp欢迎界面上选择新建的test文件夹，就能看到简洁的版本历史图，有三个commit，其中两个是小圆点，一个是现在所处的HEAD。随便单击选择一个commit：<br><img src="/img/gitup-map.jpg" alt=""></p>
<p>可以看到这个commit的信息，按上下键可以选择其它commit，按空格切换commit详细页面。在commit上右击，便能看到所有支持的操作。我们可以先右击中间的commit，选择<strong>Edit Message</strong>把先前的提交消息”bb”改成”b”。很简单吧！比输命令易用多了。之后在HEAD上右击并选择<strong>Create Branch</strong>来新建一个分支，分支名为temp。然后就能看到下图：<br><img src="/img/gitup-new-branch.jpg" alt=""></p>
<p>可以在终端中运行<code>git branch</code>来确认自己在temp分支上。然后加点代码：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> d &gt;&gt; c.txt</span><br><span class="line">cat c.txt</span><br></pre></td></tr></table></figure></p>
<p>这时切回GitUp的界面，选择中间的视图，如下图所示：<br><img src="/img/gitup-commit.jpg" alt=""></p>
<p>看起来很像<code>git gui</code>吧。双击<code>c.txt</code>就可以切换文件的状态。输入提交消息<code>cd</code>，然后点击<strong>Commit</strong>按钮来提交。于是就能看到下图：<br><img src="/img/gitup-new-branch-commit.jpg" alt=""></p>
<p>双击master的小黄点就可以切换到master分支上了。然后也加点代码：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> e &gt;&gt; c.txt</span><br><span class="line">cat c.txt</span><br><span class="line">git commit -am <span class="string">"ce"</span></span><br></pre></td></tr></table></figure></p>
<p>这回的图变成这样了：<br><img src="/img/gitup-master-commit.jpg" alt=""></p>
<h2 id="u5408_u5E76_u548C_u884D_u5408"><a href="#u5408_u5E76_u548C_u884D_u5408" class="headerlink" title="合并和衍合"></a>合并和衍合</h2><p>我们来试一下合并分支。右击temp上的小圆圈，选择<strong>Merge into Current Branch</strong>，然后点击<strong>Merge</strong>按钮就能看到冲突了。可以使用<strong>Open with Default Editor</strong>来自己解决冲突，也可以使用<strong>Resolve in Merge Tool</strong>来解决。如果是前者，可以注意一个小细节：这里的冲突提示是ours和theirs，看起来人性化了不少。合并完成后，点击<strong>Mark as Resolved</strong>，然后<strong>Commit</strong>，就可以看到图变成这样了：<br><img src="/img/gitup-merge.jpg" alt=""></p>
<p>衍合也是类似。GitUp提供了一个逆天功能Command+Z，可以快速回退到上一次操作（再次前进是Command+Shift+Z）。这样我们很轻松就能再来一次衍合。右击temp上的小圆圈，选择<strong>Rebase Current Branch onto Here</strong>，剩下的和合并分支类似。提交之后，就可以看到图变成这样了：<br><img src="/img/gitup-rebase.jpg" alt=""></p>
<p>GitUp还提供了强大的快照功能。我们可以点击右上方的时钟按钮来选择自己想要的快照，就像Time Machine似的。如下图：<br><img src="/img/gitup-snapshot.jpg" alt=""></p>
<h2 id="u67E5_u770Bstash"><a href="#u67E5_u770Bstash" class="headerlink" title="查看stash"></a>查看stash</h2><p>那么第三个视图是干什么的呢？我们先stash一段代码：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> f &gt;&gt; c.txt</span><br><span class="line">cat c.txt</span><br><span class="line">git stash</span><br><span class="line"><span class="built_in">echo</span> g &gt;&gt; c.txt</span><br></pre></td></tr></table></figure></p>
<p>打开第三个视图，原来是stash列表，这回可以很容易看清楚了。也可以在这里stash：点击左下方的加号按钮，输入一个消息然后<strong>Save Stash</strong>，就可以看到下图：<br><img src="/img/gitup-stashes.jpg" alt=""></p>
<p>还可以在这里轻松地<strong>Apply</strong>想要的stash，这个可视化可以有。可惜还是不支持选择特定文件stash。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[用容器轻松搭建Prometheus运行环境]]></title>
      <url>http://qinghua.github.io/prometheus/</url>
      <content type="html"><![CDATA[<p><a href="https://prometheus.io/" target="_blank" rel="external">Prometheus</a>是一个开源的监控解决方案，包括数据采集、汇聚、存储、可视化、监控、告警等。除了基本的监控数据，也支持通过自定义exporter来获取自己想要的数据。本文从零开始用容器搭建一个prometheus环境，并介绍一些基本功能。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下代码，相当于给它分配一台IP是<strong>192.168.33.18</strong>的虚拟机。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后在终端运行以下命令启动并连接虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>Prometheus的环境搭建起来非常简单，只要一个docker镜像即可。绿色的压缩包安装方式可以参考<a href="https://prometheus.io/docs/introduction/getting_started/" target="_blank" rel="external">官方文档</a>。此外还需要一个配置文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;prometheus.yml</span><br><span class="line">global:</span><br><span class="line">  scrape_interval: <span class="number">15</span>s</span><br><span class="line">  external_labels:</span><br><span class="line">    monitor: <span class="string">'codelab-monitor'</span></span><br><span class="line">scrape_configs:</span><br><span class="line">  - job_name: <span class="string">'prometheus'</span></span><br><span class="line">    scrape_interval: <span class="number">5</span>s</span><br><span class="line">    target_groups:</span><br><span class="line">      - targets: [<span class="string">'localhost:9090'</span>]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mkdir /etc/prometheus</span><br><span class="line">sudo mv prometheus.yml /etc/prometheus</span><br></pre></td></tr></table></figure></p>
<p>配置文件中，<code>scrape_interval</code>指的是数据获取间隔，<code>prometheus</code>这个任务里的<code>scrape_interval</code>将会在这个任务里覆盖掉默认的<code>global</code>全局值，也就是这个任务每5秒钟获取一次数据，其它任务则是每15秒钟。完整的配置文件格式，请参考<a href="http://prometheus.io/docs/operating/configuration/" target="_blank" rel="external">官方文档</a>。接下来启动Prometheus：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/docker run <span class="operator">-d</span> \</span><br><span class="line">    --name=prometheus \</span><br><span class="line">    --publish=<span class="number">9090</span>:<span class="number">9090</span> \</span><br><span class="line">    -v /etc/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml \</span><br><span class="line">    -v /var/prometheus/storage:/prometheus \</span><br><span class="line">    prom/prometheus:<span class="number">0.17</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>启动完成后，将会在<a href="http://192.168.33.18:9090" target="_blank" rel="external">http://192.168.33.18:9090</a>看到prometheus的首页：<br><img src="/img/prometheus-home.jpg" alt=""></p>
<h2 id="u6570_u636E_u6536_u96C6"><a href="#u6570_u636E_u6536_u96C6" class="headerlink" title="数据收集"></a>数据收集</h2><p>在<a href="http://192.168.33.18:9090/metrics" target="_blank" rel="external">http://192.168.33.18:9090/metrics</a>可以看到prometheus收集到的数据。其中有一个<code>prometheus_target_interval_length_seconds</code>，表示真实的数据获取间隔。在prometheus首页输入它并回车，就可以看到一系列的数据，它们有着不同的quantile，从0.01至0.99不等。0.99的意思是有99%的数据都在这个值以内。如果我们只关心这个数，我们可以输入<code>prometheus_target_interval_length_seconds{quantile=&quot;0.99&quot;}</code>来查看。查询还支持函数，比如<code>count(prometheus_target_interval_length_seconds)</code>可以查询数量。完整的表达式可以参考<a href="https://prometheus.io/docs/querying/basics/" target="_blank" rel="external">官方文档</a>。</p>
<p>点击<strong>Console</strong>旁边的<strong>Graph</strong>标签就可以看见时序图了：<br><img src="/img/prometheus-graph.jpg" alt=""></p>
<p>可以随意选择指标和函数试一试，比如<code>rate(prometheus_local_storage_chunk_ops_total[1m])</code>。</p>
<h2 id="Exporter"><a href="#Exporter" class="headerlink" title="Exporter"></a>Exporter</h2><p>Prometheus支持官方/非官方的许多种<a href="https://prometheus.io/docs/instrumenting/exporters/" target="_blank" rel="external">exporter</a>，如HAProxy，Jenkins，MySQL等，也有一些软件直接支持Prometheus而无需exporter，如Etcd，Kubernetes等。我们试一下node exporter：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=ne \</span><br><span class="line">  -p <span class="number">9100</span>:<span class="number">9100</span> \</span><br><span class="line">  prom/node-exporter</span><br></pre></td></tr></table></figure></p>
<p>Node exporter暴露的端口是9100，所以我们需要修改一下prometheus的配置文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;prometheus.yml</span><br><span class="line">global:</span><br><span class="line">  scrape_interval: <span class="number">15</span>s</span><br><span class="line">  external_labels:</span><br><span class="line">    monitor: <span class="string">'codelab-monitor'</span></span><br><span class="line">scrape_configs:</span><br><span class="line">  - job_name: <span class="string">'node'</span></span><br><span class="line">    scrape_interval: <span class="number">5</span>s</span><br><span class="line">    target_groups:</span><br><span class="line">      - targets: [<span class="string">'192.168.33.18:9100'</span>]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo cp prometheus.yml /etc/prometheus</span><br></pre></td></tr></table></figure></p>
<p>重启prometheus：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker stop prometheus</span><br><span class="line">sudo rm -rf /var/prometheus/storage</span><br><span class="line">docker start prometheus</span><br></pre></td></tr></table></figure></p>
<p>这样在页面上就可以选择节点的一些指标了。也可以访问<a href="http://192.168.33.18:9100/" target="_blank" rel="external">http://192.168.33.18:9100/</a>来直接查看Exporter的指标。</p>
<h2 id="Push_Gateway"><a href="#Push_Gateway" class="headerlink" title="Push Gateway"></a>Push Gateway</h2><p>Prometheus采集数据是用的pull也就是拉模型，这从我们刚才设置的5秒参数就能看出来。但是有些数据并不适合采用这样的方式，对这样的数据可以使用Push Gateway服务。它就相当于一个缓存，当数据采集完成之后，就上传到这里，由Prometheus稍后再pull过来。我们来试一下，首先启动Push Gateway：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=pg \</span><br><span class="line">  -p <span class="number">9091</span>:<span class="number">9091</span> \</span><br><span class="line">  prom/pushgateway</span><br></pre></td></tr></table></figure></p>
<p>可以访问<a href="http://192.168.33.18:9091/" target="_blank" rel="external">http://192.168.33.18:9091/</a>来查看它的页面。下个命令将会往Push Gateway上传数据：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"some_metric 3.14"</span> | curl --data-binary @- http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">9091</span>/metrics/job/some_job</span><br></pre></td></tr></table></figure></p>
<p>效果是酱紫滴：<br><img src="/img/prometheus-push-gateway.jpg" alt=""></p>
<p>而在Prometheus的配置文件里，只要把端口换成<code>9100</code>便能采集到Push Gateway的数据了。</p>
<h2 id="Grafana"><a href="#Grafana" class="headerlink" title="Grafana"></a>Grafana</h2><p><a href="https://prometheus.io/docs/visualization/grafana/" target="_blank" rel="external">Grafana</a>是目前比较流行的监控可视化UI，它从2.5.0版开始直接支持Prometheus的数据。我们来试一下。首先启动grafana：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name grafana \</span><br><span class="line">  -p <span class="number">3000</span>:<span class="number">3000</span> \</span><br><span class="line">  grafana/grafana:<span class="number">2.6</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>打开<a href="http://192.168.33.18:3000/" target="_blank" rel="external">http://192.168.33.18:3000/</a>，就能看到grafana的登录页面了。输入默认的admin/admin登录grafana。选择左侧的<strong>Data Sources</strong>，然后点击上面的<strong>Add new</strong>按钮，便可以把prometheus作为数据源导入grafana：<br><img src="/img/grafana-prometheus-data-source.jpg" alt=""></p>
<p>输入下面的值：</p>
<ul>
<li>Name：prometheus</li>
<li>Default：true</li>
<li>Type：Prometheus</li>
<li>Url：<a href="http://192.168.33.18:9090/" target="_blank" rel="external">http://192.168.33.18:9090/</a></li>
</ul>
<p>然后点击<strong>Add</strong>按钮。之后会出来一个<strong>Test Connection</strong>的按钮，点击它便可以收到<strong>Data source is working</strong>的消息。点击左边的<strong>Dashboards</strong>回到主页，点击上面的<strong>Home</strong>，选择<strong>+ New</strong>，会出来一个绿色的小竖条，点击它便会弹出来一个菜单：<br><img src="/img/grafana-dashboard-menu.jpg" alt=""></p>
<p>选择<strong>Add Panel</strong>和<strong>Graph</strong>，便会出来一个图。然后就可以在<strong>Query</strong>里输入prometheus支持的查询了：<br><img src="/img/grafana-prometheus-graph.jpg" alt=""></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TOSCA简介]]></title>
      <url>http://qinghua.github.io/tosca/</url>
      <content type="html"><![CDATA[<p><a href="https://www.oasis-open.org/committees/tosca/" target="_blank" rel="external">TOSCA</a>（Topology and Orchestration Specification for Cloud Applications）是由OASIS组织制定的云应用拓扑编排规范。通俗地说，就是制定了一个标准，用来描述云平台上应用的拓扑结构。目前支持XML和YAML，Cloudiy的蓝图就是基于这个规范而来。这个规范比较庞大，本文尽量浓缩了<a href="http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html" target="_blank" rel="external">TOSCA的YAML版</a>前两章，以便用尽量少的时间了解尽量多的规范内容。<br><a id="more"></a></p>
<h2 id="u7B80_u4ECB"><a href="#u7B80_u4ECB" class="headerlink" title="简介"></a>简介</h2><p>TOSCA的基本概念只有两个：节点（node）和关系（relationship）。节点有许多类型，可以是一台服务器，一个网络，一个计算节点等等。关系描述了节点之间是如何连接的。举个栗子：一个nodejs应用（节点）部署在（关系）名为host的主机（节点）上。节点和关系都可以通过程序来扩展和实现。</p>
<p>目前它的开源实现有OpenStack (Heat-Translator，Tacker，Senlin)，Alien4Cloud，Cloudify等。</p>
<h2 id="u793A_u4F8B"><a href="#u793A_u4F8B" class="headerlink" title="示例"></a>示例</h2><h3 id="Hello_World"><a href="#Hello_World" class="headerlink" title="Hello World"></a>Hello World</h3><p>首先登场的是广大程序猿和攻城狮们都喜闻乐见的Hello World，但是其实里面并没有Hello World，只是比较简单而已。先看下面这段描述文件：</p>
<pre>
tosca_definitions_version: tosca_simple_yaml_1_0

description: Template for deploying a single server with predefined properties.

topology_template:
  node_templates:
    my_server:
      type: tosca.nodes.Compute
      capabilities:
        host:
          properties:
            num_cpus: 1
            disk_size: 10 GB
            mem_size: 4096 MB
        os:
          properties:
            architecture: x86_64
            type: linux 
            distribution: rhel 
            version: 6.5 
</pre>

<p>除了TOSCA的版本<code>tosca_definitions_version</code>和描述信息<code>description</code>以外，就是这个<code>topology_template</code>了。这里我们看到有一个名为<code>my_server</code>的节点，它的类型是<code>tosca.nodes.Compute</code>。这个类型预置了两个<code>capabilities</code>信息，一个是<code>host</code>，定义了硬件信息；另一个是<code>os</code>，定义了操作系统信息。</p>
<h3 id="u8F93_u5165_u8F93_u51FA"><a href="#u8F93_u5165_u8F93_u51FA" class="headerlink" title="输入输出"></a>输入输出</h3><p>再看看下面这个描述文件：</p>
<pre>
topology_template:
  <b style="color:magenta">inputs</b>:
    cpus:
      type: integer
      description: Number of CPUs for the server.
      constraints:
        - valid_values: [ 1, 2, 4, 8 ]

  node_templates:
    my_server:
      type: tosca.nodes.Compute
      capabilities:
        host:
          properties:
            num_cpus: { get_input: cpus }
            mem_size: 2048  MB
            disk_size: 10 GB

  <b style="color:magenta">outputs</b>:
    server_ip:
      description: The private IP address of the provisioned server.
      value: { get_attribute: [ my_server, private_address ] }
</pre>

<p>这里的<code>inputs</code>和<code>outputs</code>分别定义了输入和输出。输入的<code>cpus</code>是在1，2，4和8中的一个整数，而输出的<code>server_ip</code>就是<code>my_server</code>这个节点的<code>private_address</code>也就是私有IP地址。另外一点是TOSCA提供了一些内置函数，在上面这个文件中使用了<code>get_input</code>和<code>get_attribute</code>。输入参数可以通过<code>get_input</code>被使用。</p>
<h3 id="u5B89_u88C5_u8F6F_u4EF6"><a href="#u5B89_u88C5_u8F6F_u4EF6" class="headerlink" title="安装软件"></a>安装软件</h3><p>第三个描述文件如下：</p>
<pre>
topology_template:
  inputs:
    # 略

  node_templates:
    mysql:
      type: <b style="color:magenta">tosca.nodes.DBMS.MySQL</b>
      properties:
        root_password: { get_input: my_mysql_rootpw }
        port: { get_input: my_mysql_port }
      <b style="color:magenta">requirements</b>:
        - host: db_server

    db_server:
      type: tosca.nodes.Compute
      capabilities:
        # 略
</pre>

<p>我们看到了一个新的节点类型：<code>tosca.nodes.DBMS.MySQL</code>。这个类型允许接收<code>root_password</code>和<code>port</code>的参数。在<code>requirements</code>里定义了<code>mysql</code>这个节点需要被安装到<code>db_server</code>这个节点上，这就是“关系”。如果只想表明依赖，比如说<code>service_a</code>依赖于<code>service_b</code>，也可以直接用<code>- dependency: service_b</code>来描述。上面文件的拓扑结构如下图：<br><img src="http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/csprd02/TOSCA-Simple-Profile-YAML-v1.0-csprd02_files/image003.png" alt=""></p>
<h3 id="u521D_u59CB_u5316_u6570_u636E_u5E93"><a href="#u521D_u59CB_u5316_u6570_u636E_u5E93" class="headerlink" title="初始化数据库"></a>初始化数据库</h3><p>第四个描述文件如下：</p>
<pre>
  node_templates:
    my_db:
      type: <b style="color:magenta">tosca.nodes.Database.MySQL</b>
      properties:
        name: { get_input: database_name }
        user: { get_input: database_user }
        password: { get_input: database_password }
        port: { get_input: database_port }
      <b style="color:magenta">artifacts</b>:
        db_content:
          file: files/my_db_content.txt
          type: tosca.artifacts.File
      requirements:
        - host: mysql
      interfaces:
        <b style="color:magenta">Standard:
          create:
            implementation: db_create.sh</b>
            inputs:
              db_data: { get_artifact: [ SELF, db_content ] }

    mysql:
      type: tosca.nodes.DBMS.MySQL
      properties:
        root_password: { get_input: mysql_rootpw }
        port: { get_input: mysql_port }
      requirements:
        - host: db_server

    db_server:
      # 略
</pre>

<p>这里的<code>tosca.nodes.Database.MySQL</code>表示一个MySQL数据库的实例。在<code>artifacts</code>的<code>db_content</code>里指定了一个文本文件，而这个文件将被<code>interfaces</code>里的<code>Create</code>所用，为<code>db_create.sh</code>脚本提供数据。<code>Standard</code>表示生命周期，可能会包含<code>configure</code>、<code>start</code>、<code>stop</code>等各种操作，而<code>db_create.sh</code>本身是对<code>tosca.nodes.Database.MySQL</code>提供的默认<code>create</code>操作的一个重写。如下图：<br><img src="http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/csprd02/TOSCA-Simple-Profile-YAML-v1.0-csprd02_files/image004.png" alt=""></p>
<h3 id="u4E24_u5C42_u5E94_u7528"><a href="#u4E24_u5C42_u5E94_u7528" class="headerlink" title="两层应用"></a>两层应用</h3><p>再来看看第五个描述文件：</p>
<pre>
  node_templates:
    wordpress:
      type: tosca.nodes.WebApplication.WordPress
      properties:
        context_root: { get_input: context_root }
        admin_user: { get_input: wp_admin_username }
        admin_password: { get_input: wp_admin_password }
        db_host: { get_attribute: [ db_server, private_address ] }
      <b style="color:magenta">requirements:
        - host: apache
        - database_endpoint: wordpress_db</b>
      interfaces:
        Standard:
          inputs:
            db_host: { get_attribute: [ db_server, private_address ] }
            db_port: { get_property: [ wordpress_db, port ] }
            db_name: { get_property: [ wordpress_db, name ] }
            db_user: { get_property: [ wordpress_db, user ] }
            db_password: { get_property: [ wordpress_db, password ] }  
    apache:
      type: tosca.nodes.WebServer.Apache
      properties:
        # 略
      <b style="color:magenta">requirements:
        - host: web_server</b>
    web_server:
      type: tosca.nodes.Compute
      # 略

    wordpress_db:
      type: tosca.nodes.Database.MySQL
      # 略
    mysql:
      type: tosca.nodes.DBMS.MySQL
      # 略
    db_server:
      type: tosca.nodes.Compute
      # 略
</pre>

<p>这个文件描述了一个很常见的拓扑结构：<code>mysql</code>里有一个<code>wordpress_db</code>，运行在<code>db_server</code>上；<code>apache</code>部署了一个<code>wordpress</code>，运行在<code>web_server</code>上。<code>wordpress</code>需要<code>wordpress_db</code>。</p>
<h3 id="u5173_u7CFB_u5B9A_u5236_u5316"><a href="#u5173_u7CFB_u5B9A_u5236_u5316" class="headerlink" title="关系定制化"></a>关系定制化</h3><p>第六个描述文件：</p>
<pre>
  node_templates:
    wordpress:
      type: tosca.nodes.WebApplication.WordPress
      properties:
        # 略
      requirements:
        - host: apache
        - database_endpoint:
            node: wordpress_db
            <b style="color:magenta">relationship: my.types.WordpressDbConnection</b>
    wordpress_db:
      type: tosca.nodes.Database.MySQL
      properties:
        # 略
      requirements:
        - host: mysql
  <b style="color:magenta">relationship_templates:
    my.types.WordpressDbConnection:</b>
      type: ConnectsTo
      interfaces:
        Configure:
          pre_configure_source: scripts/wp_db_configure.sh
</pre>

<p>这里的关注点是<code>relationship</code>里的<code>my.types.WordpressDbConnection</code>。这是一个自定义的关系，在文件的下半部分描述了详细定义。它实际上是一个<code>ConnectsTo</code>类型，为<code>pre_configure_source</code>操作提供了一个自定义脚本。这个定义也可以单独提出一个文件，就像下面这样：</p>
<pre>
tosca_definitions_version: tosca_simple_yaml_1_0

description: Definition of custom WordpressDbConnection relationship type

<b style="color:magenta">relationship_types:
  my.types.WordpressDbConnection:</b>
    derived_from: tosca.relationships.ConnectsTo
    interfaces:
      Configure:
        pre_configure_source: scripts/wp_db_configure.sh
</pre>

<h3 id="u9650_u5B9A_u9700_u6C42_u8D44_u6E90"><a href="#u9650_u5B9A_u9700_u6C42_u8D44_u6E90" class="headerlink" title="限定需求资源"></a>限定需求资源</h3><p>再看一个描述文件：</p>
<pre>
  node_templates:
    mysql:
      type: tosca.nodes.DBMS.MySQL
      properties:
        # 略
      requirements:
        - host:
            <b style="color:magenta">node_filter</b>:
              capabilities:
                - host:
                    properties:
                      - num_cpus: { <b style="color:magenta">in_range</b>: [ 1, 4 ] }
                      - mem_size: { <b style="color:magenta">greater_or_equal</b>: 2 GB }
                - os:
                    properties:
                      - architecture: { <b style="color:magenta">equal</b>: x86_64 }
                      - type: linux
                      - distribution: ubuntu
</pre>

<p>需要关注的是<code>node_filter</code>。这里并没有指定mysql在哪个节点上启动，但是指定了一些节点信息，只有符合的节点才能够启动它。也可以抽出来做个模板：</p>
<pre>
  node_templates:
    mysql:
      type: tosca.nodes.DBMS.MySQL
      properties:
        # 略
      requirements:
        - host: <b style="color:magenta">mysql_compute</b>

    <b style="color:magenta">mysql_compute</b>:
      type: Compute
      node_filter:
        capabilities:
          - host:
              properties:
                num_cpus: { equal: 2 }
                mem_size: { greater_or_equal: 2 GB }
          - os:
              properties:
                architecture: { equal: x86_64 }
                type: linux
                distribution: ubuntu
</pre>

<p>数据库也可以使用：</p>
<pre>
  node_templates:
    my_app:
      type: my.types.MyApplication
      properties:
        admin_user: { get_input: admin_username }
        admin_password: { get_input: admin_password }
        db_endpoint_url: { get_property: [SELF, <b style="color:magenta">database_endpoint</b>, url_path ] }         
      requirements:
        - <b style="color:magenta">database_endpoint</b>:
            node: my.types.nodes.MyDatabase
            <b style="color:magenta">node_filter</b>:
              properties:
                - db_version: { greater_or_equal: 5.5 }
</pre>

<p>上面指定了数据库的版本。也可以抽出来做个模板：</p>
<pre>
  node_templates:
    my_app:
      type: my.types.MyApplication
      properties:
        admin_user: { get_input: admin_username }
        admin_password: { get_input: admin_password }
        db_endpoint_url: { get_property: [SELF, database_endpoint, url_path ] }         
      requirements:
        - database_endpoint: <b style="color:magenta">my_abstract_database</b>
    <b style="color:magenta">my_abstract_database</b>:
      type: my.types.nodes.MyDatabase
      properties:
        - db_version: { greater_or_equal: 5.5 }
</pre>

<h3 id="u8282_u70B9_u6A21_u677F_u66FF_u6362"><a href="#u8282_u70B9_u6A21_u677F_u66FF_u6362" class="headerlink" title="节点模板替换"></a>节点模板替换</h3><p>再看一个描述文件：</p>
<pre>
  node_templates:
    web_app:
      type: tosca.nodes.WebApplication.MyWebApp
      requirements:
        - host: web_server
        - database_endpoint: <b style="color:magenta">db</b>

    web_server:
      type: tosca.nodes.WebServer
      requirements:
        - host: server

    server:
      type: tosca.nodes.Compute
      # 略

    <b style="color:magenta">db</b>:
      # 这是一个抽象节点
      type: tosca.nodes.Database
      properties:
        user: my_db_user
        password: secret
        name: my_db_name
</pre>

<p>这里的<code>db</code>是一个抽象节点，可以被下面的描述文件所替换：</p>
<pre>
topology_template:
  inputs:
    db_user:
      type: string
    # 略
  <b style="color:magenta">substitution_mappings:
    node_type: tosca.nodes.Database
    capabilities:
      database_endpoint: [ database, database_endpoint ]</b>
  node_templates:
    database:
      type: tosca.nodes.Database
      properties:
        user: { get_input: db_user }
        # 略
      requirements:
        - host: dbms
    dbms:
      type: tosca.nodes.DBMS
      # 略
    server:
      type: tosca.nodes.Compute
      # 略
</pre>

<p>这里的<code>database_endpoint</code>是由<code>database</code>节点提供的<code>database_endpoint</code>。两个文件联系起来看，表明了上面的<code>web_app</code>不需要管<code>db</code>是什么样子的，有什么拓扑结构，它关心的只是<code>database_endpoint</code>。而下面由<code>database</code>、<code>dbms</code>和<code>server</code>三个节点组成的模板正好可以提供<code>database_endpoint</code>，从而替换掉<code>db</code>这个抽象节点。另外，这样的替换也支持嵌套。</p>
<h3 id="u8282_u70B9_u6A21_u677F_u7EC4"><a href="#u8282_u70B9_u6A21_u677F_u7EC4" class="headerlink" title="节点模板组"></a>节点模板组</h3><p>再看一个描述文件：</p>
<pre>
  node_templates:
    apache:
      type: tosca.nodes.WebServer.Apache
      properties:
        # 略
      requirements:
        - host: server
    server:
      type: tosca.nodes.Compute
        # 略
  <b style="color:magenta">groups</b>:
    <b style="color:magenta">webserver_group</b>:
      type: tosca.groups.Root
      members: [ apache, server ]

  <b style="color:magenta">policies</b>:
    - my_anti_collocation_policy:
        type: my.policies.anticolocateion
        targets: [ <b style="color:magenta">webserver_group</b> ]
        # 可以一起处理
</pre>

<p>这个例子表明了<code>apache</code>和<code>server</code>应该是一组的关系。这样它们就可以一起被处理，比如说伸缩。</p>
<h3 id="YAML_u5B8F"><a href="#YAML_u5B8F" class="headerlink" title="YAML宏"></a>YAML宏</h3><p>下面这个描述文件使用了宏来避免重复：</p>
<pre>
<b style="color:magenta">dsl_definitions:
  my_compute_node_props: &my_compute_node_props</b>
    disk_size: 10 GB
    num_cpus: 1
    mem_size: 2 GB

topology_template:
  node_templates:
    my_server:
      type: Compute
      capabilities:
        - host:
            properties: <b style="color:magenta">*my_compute_node_props</b>

    my_database:
      type: Compute
      capabilities:
        - host:
            properties: <b style="color:magenta">*my_compute_node_props</b>
</pre>

<h3 id="u4F20_u53C2"><a href="#u4F20_u53C2" class="headerlink" title="传参"></a>传参</h3><p>先看一个描述文件：</p>
<pre>
  node_templates: 
    wordpress:
      type: tosca.nodes.WebApplication.WordPress
      requirements:
        - database_endpoint: mysql_database
      interfaces:
        Standard:
          <b style="color:magenta">inputs</b>:
            wp_db_port: { get_property: [ SELF, database_endpoint, port ] }
          configure:
            implementation: wordpress_configure.sh           
            <b style="color:magenta">inputs</b>:
              wp_db_port: { get_property: [ SELF, database_endpoint, port ] }
</pre>

<p>这个例子有两个<code>inputs</code>，前者指的是为所有操作都声明一个变量，后者指的是为<code>configure</code>这个操作声明一个变量。再看下一个文件：</p>
<pre>
  node_templates: 
    frontend: 
      type: MyTypes.SomeNodeType    
      attributes: 
        url: { <b style="color:magenta">get_operation_output</b>: [ SELF, Standard, create, generated_url ] } 
      interfaces: 
        Standard: 
          create: 
            implementation: scripts/frontend/create.sh
          configure: 
            implementation: scripts/frontend/configure.sh 
            inputs: 
              data_dir: { <b style="color:magenta">get_operation_output</b>: [ SELF, Standard, create, data_dir ] }
</pre>

<p>在这个例子里有两个<code>get_operation_output</code>，前者指的是将<code>create</code>操作的环境变量<code>generated_url</code>设置到<code>url</code>里，后者是将<code>data_dir</code>传递给<code>configure</code>操作。</p>
<h3 id="u53D6_u52A8_u6001_u503C"><a href="#u53D6_u52A8_u6001_u503C" class="headerlink" title="取动态值"></a>取动态值</h3><p>最后一个描述文件：</p>
<pre>
node_types:
  ServerNode:
    derived_from: SoftwareComponent
    properties:
      <b style="color:magenta">notification_port</b>:
        type: integer
    capabilities:
      # 略
  ClientNode:
    derived_from: SoftwareComponent
    properties:
      # 略
    requirements:
      - server:
          capability: Endpoint
          node: ServerNode 
          relationship: ConnectsTo
topology_template:          
  node_templates:
    my_server:
      type: ServerNode 
      properties:
        notification_port: 8000
    my_client:
      type: ClientNode
      requirements:
        - server:
            node: my_server
            relationship: <b style="color:magenta">my_connection</b>
  relationship_templates:
    <b style="color:magenta">my_connection</b>:
      type: ConnectsTo
      interfaces:
        Configure:
          inputs:
            <b style="color:magenta">targ_notify_port: { get_attribute: [ TARGET, notification_port ] }</b>
            # 略
</pre>

<p>这个例子里，类型为<code>ClientNode</code>的<code>my_client</code>在<code>my_connection</code>关系的<code>Configure</code>操作上需要<code>notification_port</code>变量。这样的话，当类型为<code>ServerNode</code>的<code>my_server</code>连接过来时，就能取到它的<code>notification_port</code>变量，并设置到<code>targ_notify_port</code>环境变量里。有一点值得注意的是，真实的<code>notification_port</code>可能是8000，也可能不是。所以在这种情况下，不用<code>get_property</code>，而用<code>get_attribute</code>函数。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[轻松搭建Cloudify运行环境]]></title>
      <url>http://qinghua.github.io/cloudify/</url>
      <content type="html"><![CDATA[<p><a href="http://getcloudify.org/" target="_blank" rel="external">Cloudify</a>是一个开源的云应用编排系统，它允许使用DSL来描述应用的拓扑结构，并部署到任意环境中。本文大量参考了<a href="http://docs.getcloudify.org/3.3.1/intro/what-is-cloudify/" target="_blank" rel="external">官方教程</a>从零开始搭建并管理一个cloudify 3.3.1集群。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，注释掉并在它的下面添加如下几行代码，相当于给它分配两台虚拟机，一台叫做<strong>manager</strong>，它的IP是<strong>192.168.33.17</strong>；另一台叫做<strong>agent</strong>，它们的IP是<strong>192.168.33.18</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"manager"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.box = <span class="string">"cloudify-virtualbox_3.3.0-ga-b300.box"</span></span><br><span class="line">  host.vm.box_url = <span class="string">"http://repository.cloudifysource.org/org/cloudify3/3.3.0/ga-RELEASE/cloudify-virtualbox_3.3.0-ga-b300.box"</span></span><br><span class="line">  host.vm.hostname = <span class="string">"manager"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">2048</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"agent"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.box = <span class="string">"minimum/ubuntu-trusty64-docker"</span></span><br><span class="line">  host.vm.hostname = <span class="string">"agent"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>虚拟机agent所用的vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。虚拟机manager用的远程镜像是cloudify官方镜像，提供了cloudify manager功能。然后分别在两个终端运行以下命令启动并连接两台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh manager</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh agent</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>在Ubuntu上安装cloudify很简单，在agent上运行以下命令即可：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget -c http://repository.cloudifysource.org/org/cloudify3/get-cloudify.py</span><br><span class="line">sudo python get-cloudify.py</span><br></pre></td></tr></table></figure></p>
<p>安装完了之后，运行以下命令可以看到cloudify命令行的版本及帮助文档：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cfy --version</span><br><span class="line">cfy -h</span><br></pre></td></tr></table></figure></p>
<h2 id="u90E8_u7F72_u5E94_u7528"><a href="#u90E8_u7F72_u5E94_u7528" class="headerlink" title="部署应用"></a>部署应用</h2><p>Cloudify的应用被称为<a href="http://docs.getcloudify.org/3.3.1/intro/blueprints/" target="_blank" rel="external">蓝图</a>（blueprint），这个名字很好地诠释了它在主页上声称的“从蓝图到生产环境（From Blueprint to Production）”。官方已经为我们的第一次使用准备了一个Hello World，让我们先下载下来：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -c https://github.com/cloudify-examples/simple-python-webserver-blueprint/archive/master.zip</span><br><span class="line">sudo apt-get install <span class="operator">-f</span> unzip</span><br><span class="line">unzip master.zip</span><br><span class="line"><span class="built_in">cd</span> simple-python-webserver-blueprint-master/</span><br></pre></td></tr></table></figure></p>
<p>接下来初始化下载的蓝图并传入端口等参数：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> init --blueprint-path blueprint.yaml --inputs <span class="string">'&#123;"webserver_port": "8000", "host_ip":"localhost"&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>Cloudify使用工作流（workflow）来管理应用程序。现在启动install工作流来部署一个python的web服务器：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> execute --workflow install</span><br><span class="line">curl localhost:<span class="number">8000</span></span><br></pre></td></tr></table></figure></p>
<p>也可以在启动vagrant虚拟机的主机上访问：<a href="http://192.168.33.18:8000" target="_blank" rel="external">http://192.168.33.18:8000</a>：<br><img src="/img/cloudify-hello-world.jpg" alt=""></p>
<p>短短几步，我们便顺利部署了一个应用。通过以下命令可以看到一些运行的参数：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> outputs</span><br></pre></td></tr></table></figure></p>
<p>我们看到的内容称之为模型（model）。蓝图是应用的模板，蓝图的实例称为部署（deployment），部署就是模型的内容之一。蓝图里的每个实体称之为节点（node），节点在部署里称为节点实例（node-instances），它们是一对多的关系。但是在这个例子里，我们有两个节点，每个节点各有一个节点实例。可以用以下命令查看节点实例：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> instances</span><br></pre></td></tr></table></figure></p>
<p>我们能看到这两个节点实例分别是host和http_web_server，其中http_web_server运行在host之上。可以用以下命令来结束部署：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> execute -w uninstall</span><br></pre></td></tr></table></figure></p>
<h2 id="u84DD_u56FE_u89E3_u6790"><a href="#u84DD_u56FE_u89E3_u6790" class="headerlink" title="蓝图解析"></a>蓝图解析</h2><p>现在让我们看一看刚才所用的蓝图的结构：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat blueprint.yaml</span><br></pre></td></tr></table></figure></p>
<p>这就是一个yaml格式的文件，里面都是cloudify的DSL。文件分为以下五个部分：</p>
<ul>
<li>tosca_definitions_version：蓝图的DSL版本，这里是cloudify_dsl_1_2</li>
<li>imports：引用yaml文件的地址</li>
<li>inputs：蓝图的配置信息，也就是一开始初始化蓝图时传入的参数</li>
<li>node_templates：描述了应用的资源以及应用是如何被部署的，可以跟刚才看到的节点实例相对应起来</li>
<li>outputs：输出信息，也就是刚才看到的模型里的内容</li>
</ul>
<p>其中包括了三个内置函数（Intrinsic Functions），分别是<code>get_input</code>，<code>get_property</code>和<code>concat</code>，只能在蓝图里使用。它们的意思也都比较明显，可以从函数名推断出来。所有的内置函数可以在<a href="http://docs.getcloudify.org/3.3.1/blueprints/spec-intrinsic-functions/" target="_blank" rel="external">这里</a>查看到。</p>
<h2 id="u90E8_u7F72_u5BB9_u5668"><a href="#u90E8_u7F72_u5BB9_u5668" class="headerlink" title="部署容器"></a>部署容器</h2><p>Cloudify通过<a href="http://docs.getcloudify.org/3.3.1/plugins/docker/" target="_blank" rel="external">docker插件</a>来支持docker。这个插件依赖于Docker Python API库，而不是Docker CLI，所以体验上有所不同。比如说，<code>docker run</code>将会被分解为<code>docker create</code>和<code>docker start</code>。接下来让我们来尝试部署一个tomcat容器。首先需要生成一个tomcat容器的蓝图：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">mkdir ../docker</span><br><span class="line"><span class="built_in">cd</span> ../docker</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; blueprint.yaml</span><br><span class="line">tosca_definitions_version: cloudify_dsl_1_2</span><br><span class="line">imports:</span><br><span class="line">  - http://www.getcloudify.org/spec/cloudify/<span class="number">3.4</span>m3/types.yaml</span><br><span class="line">  - http://www.getcloudify.org/spec/docker-plugin/<span class="number">1.3</span>.<span class="number">1</span>/plugin.yaml</span><br><span class="line">inputs:</span><br><span class="line">  host_ip:</span><br><span class="line">      description: &gt;</span><br><span class="line">        The ip of the host the application will be deployed on</span><br><span class="line">      default: <span class="number">127.0</span>.<span class="number">0.1</span></span><br><span class="line">  tomcat_container_port_bindings:</span><br><span class="line">    description: &gt;</span><br><span class="line">      A dict of port bindings <span class="keyword">for</span> the node container.</span><br><span class="line">    default:</span><br><span class="line">      <span class="number">8080</span>: <span class="number">8080</span></span><br><span class="line">node_templates:</span><br><span class="line">  host:</span><br><span class="line">    <span class="built_in">type</span>: cloudify.nodes.Compute</span><br><span class="line">    properties:</span><br><span class="line">      install_agent: <span class="literal">false</span></span><br><span class="line">      ip: &#123; get_input: host_ip &#125;</span><br><span class="line">  tomcat_container:</span><br><span class="line">    <span class="built_in">type</span>: cloudify.docker.Container</span><br><span class="line">    properties:</span><br><span class="line">      name: tomcat</span><br><span class="line">      image:</span><br><span class="line">        repository: tomcat</span><br><span class="line">        tag: <span class="number">8.0</span>.<span class="number">30</span>-jre8</span><br><span class="line">    interfaces:</span><br><span class="line">      cloudify.interfaces.lifecycle:</span><br><span class="line">        create:</span><br><span class="line">          implementation: docker.docker_plugin.tasks.create_container</span><br><span class="line">          inputs:</span><br><span class="line">            params:</span><br><span class="line">              stdin_open: <span class="literal">true</span></span><br><span class="line">              tty: <span class="literal">true</span></span><br><span class="line">        start:</span><br><span class="line">          implementation: docker.docker_plugin.tasks.start</span><br><span class="line">          inputs:</span><br><span class="line">            params:</span><br><span class="line">              port_bindings: &#123; get_input: tomcat_container_port_bindings &#125;</span><br><span class="line">    relationships:</span><br><span class="line">      - <span class="built_in">type</span>: cloudify.relationships.contained_<span class="keyword">in</span></span><br><span class="line">        target: host</span><br><span class="line">outputs:</span><br><span class="line">  http_endpoint:</span><br><span class="line">    description: Tomcat web server endpoint</span><br><span class="line">    value: &#123; <span class="string">'http://localhost:8080'</span> &#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>这个蓝图需要docker的插件，所以必须先安装一下，然后就可以初始化蓝图（这次不传参数，使用默认值）：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> create-requirements -o requirements.txt -p blueprint.yaml</span><br><span class="line">sudo pip install -r requirements.txt</span><br><span class="line">cfy <span class="built_in">local</span> init -p blueprint.yaml</span><br></pre></td></tr></table></figure></p>
<p>现在可以运行啦。由于第一次运行需要下载镜像，可能会比较慢：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> execute -w install</span><br><span class="line">docker ps</span><br><span class="line">docker images</span><br></pre></td></tr></table></figure></p>
<p>总算是可以访问了：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl localhost:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<p>也可以在启动vagrant虚拟机的主机上访问：<a href="http://192.168.33.18:8080" target="_blank" rel="external">http://192.168.33.18:8080</a>。查看运行参数和节点实例：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> outputs</span><br><span class="line">cfy <span class="built_in">local</span> instances</span><br></pre></td></tr></table></figure></p>
<p>可以用以下命令来结束部署：<br><figure class="highlight sh"><figcaption><span>agent</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cfy <span class="built_in">local</span> execute -w uninstall</span><br><span class="line">docker ps <span class="operator">-a</span></span><br></pre></td></tr></table></figure></p>
<p>看起来容器会被删除。感觉怎么样？你愿意天天这样来部署docker么？</p>
<h2 id="Cloudify_u7BA1_u7406_u5668"><a href="#Cloudify_u7BA1_u7406_u5668" class="headerlink" title="Cloudify管理器"></a>Cloudify管理器</h2><p>除了命令行以外，cloudify也支持使用管理器来部署应用。Cloudify管理器有自己的用户界面，提供历史记录、授权和鉴权等功能，并且支持并行运行工作流。下面我们来试着安装一个cloudify管理器。启动cloudify管理器就像是启动一个普通的蓝图一样。可是安装需要下载一大堆的依赖，比较繁琐，有兴趣的童鞋可以参考<a href="http://docs.getcloudify.org/3.3.1/manager/bootstrapping/" target="_blank" rel="external">官方教程</a>。官方另外还提供了一个<a href="http://docs.getcloudify.org/3.3.1/manager/getting-started/" target="_blank" rel="external">vagrant镜像</a>，里面已经配置好了整个Cloudify管理器，因为我们启动vagrant的时候就已经导入了，直接用它更方便。只要虚拟机启动起来（按照本教程的话，现在是起来的状态），可以直接访问<a href="http://192.168.33.17/" target="_blank" rel="external">http://192.168.33.17/</a>来打开cloudify管理器的页面了：<br><img src="/img/cloudify-manager-blueprint.jpg" alt=""></p>
<p>接下来我们来上传一个官方的蓝图，这是nodejs调用mongodb的应用：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> blueprints</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/cloudify-cosmo/cloudify-nodecellar-example</span><br><span class="line"><span class="built_in">cd</span> cloudify-nodecellar-example/</span><br><span class="line">git checkout tags/<span class="number">3.3</span></span><br><span class="line">cfy blueprints upload -b nodecellar -p simple-blueprint.yaml</span><br></pre></td></tr></table></figure></p>
<p><code>-b</code>参数的nodecellar是这个蓝图的名字。刷新蓝图的界面，我们就能看到一个名为nodecellar的蓝图。点击进去，还能看到更详细的拓扑结构、节点信息等。甚至还可以点击图上的各个组件查看详细信息：<br><img src="/img/cloudify-blueprint-topology.jpg" alt=""></p>
<p>这里有4个节点：</p>
<ol>
<li>host：部署的主机</li>
<li>mongod：mongoDB，运行在host上</li>
<li>nodejs：nodejs服务器，运行在host上</li>
<li>nodec…：显示不下的nodecellar，也就是这个酒窖应用，运行在nodejs服务器上，它会去访问mongoDB</li>
</ol>
<p>接下来让我们生成一个部署对象：<br><figure class="highlight"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &#39;s/host_ip: .*/host_ip: 192.168.33.17/&#39; ../inputs/nodecellar-singlehost.yaml&#10;cfy deployments create -b nodecellar -d nodecellar --inputs ../inputs/nodecellar-singlehost.yaml</span><br></pre></td></tr></table></figure></p>
<p><code>-d</code>参数的nodecellar是这个部署的ID。页面上点击左边的Deployments，我们就能看到ID为nodecellar的部署了。而Logs &amp; Events里面也生成了好几页日志和事件。与此同时，最左下的Nodes也出现了4条记录。接下来，真正地开始部署：<br><figure class="highlight"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy executions start -w install -d nodecellar</span><br></pre></td></tr></table></figure></p>
<p>部署需要一些时间，在笔者的mac上大约5分钟。这时如果刷新部署页面，就能看到Action显示Install，旁边还有一个<code>×</code>号，可以通过点击它来取消本次部署。点击部署页面上nodecellar的ID，就能看到一系列详细信息，甚至还有监控：<br><img src="/img/cloudify-deployments-monitoring.jpg" alt=""></p>
<p>部署完成后，就可以直接访问<a href="http://192.168.33.17:8080/" target="_blank" rel="external">http://192.168.33.17:8080/</a>来打开这个nodejs酒窖的网站了：<br><img src="/img/cloudify-node-cellar.jpg" alt=""></p>
<p>还可以用以下命令来停止nodecellar的部署，并删除这个部署：<br><figure class="highlight"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy executions start -w uninstall -d nodecellar&#10;cfy deployments delete -d nodecellar</span><br></pre></td></tr></table></figure></p>
<p>值得一提的是，刚才我们输入的命令，都可以通过cloudify manager的界面来操作。如果需要停止cloudify manager，可以用以下命令：<br><figure class="highlight"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfy teardown -f</span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[轻松搭建Kubernetes 1.2版运行环境]]></title>
      <url>http://qinghua.github.io/kubernetes-installation/</url>
      <content type="html"><![CDATA[<p><a href="http://kubernetes.io/docs/whatisk8s/" target="_blank" rel="external">Kubernetes</a>简称k8s，是谷歌于2014年开始主导的开源项目，提供了以容器为中心的部署、伸缩和运维平台。截止目前它的最新版本为1.2。搭建环境之前建议先了解一下kubernetes的相关知识，可以参考<a href="/kubernetes-in-mesos-1">《如果有10000台机器，你想怎么玩？》</a>系列文章。本文从零开始搭建一个kubernetes集群。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配三台虚拟机，一台叫做<strong>master</strong>，它的IP是<strong>192.168.33.17</strong>；另两台叫做<strong>node1</strong>和<strong>node2</strong>，它们的IP是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"master"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"master"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">1024</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"node1"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"node1"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">2048</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"node2"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"node2"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">2048</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后分别在三个终端运行以下命令启动并连接三台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh master</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh node1</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh node2</span><br></pre></td></tr></table></figure>
<p>这个vagrant镜像默认的docker版本为1.9.0，如果你愿意，可以用下面的命令将其升级为1.10.3，但这不是必须的：<br><figure class="highlight sh"><figcaption><span>all or none</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:<span class="number">80</span> --recv-keys <span class="number">58118</span>E89F3A912897C070ADBF76221572C52609D</span><br><span class="line">sudo sh -c <span class="string">"echo deb https://apt.dockerproject.org/repo ubuntu-trusty main &gt; /etc/apt/sources.list.d/docker.list"</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get purge lxc-docker</span><br><span class="line">sudo apt-cache policy docker-engine</span><br><span class="line">sudo apt-get install docker-engine</span><br><span class="line">sudo service docker restart</span><br><span class="line">docker -v</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u7F51_u7EDC_u73AF_u5883"><a href="#u642D_u5EFA_u7F51_u7EDC_u73AF_u5883" class="headerlink" title="搭建网络环境"></a>搭建网络环境</h2><p>为了打通不同主机上的容器的网络连接，最简单的方法是安装一个覆盖网络，这里我们使用flannel。它使用etcd来配置，所以我们需要先运行一个etcd实例。下面在master虚拟机上用容器运行一个etcd实例：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --net=host \</span><br><span class="line">  --restart=always \</span><br><span class="line">  --name=etcd \</span><br><span class="line">  -v /var/etcd/data:/var/etcd/data \</span><br><span class="line">  kubernetes/etcd:<span class="number">2.0</span>.<span class="number">5</span> \</span><br><span class="line">  /usr/<span class="built_in">local</span>/bin/etcd \</span><br><span class="line">  --addr=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">4001</span> \</span><br><span class="line">  --bind-addr=<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">4001</span> \</span><br><span class="line">  --data-dir=/var/etcd/data</span><br></pre></td></tr></table></figure></p>
<p>接下来往etcd里插入flannel的配置数据。这里指定flannel可以使用的IP地址为<code>10.0.0.0/8</code>区间：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it etcd etcdctl <span class="built_in">set</span> /qinghua.github.io/network/config <span class="string">'&#123;"Network": "10.0.0.0/8"&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>然后安装并在后台运行flannel：<br><figure class="highlight sh"><figcaption><span>master node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -c https://github.com/coreos/flannel/releases/download/v0.<span class="number">5.5</span>/flannel-<span class="number">0.5</span>.<span class="number">5</span>-linux-amd64.tar.gz</span><br><span class="line">tar zxvf flannel-<span class="number">0.5</span>.<span class="number">5</span>-linux-amd64.tar.gz</span><br><span class="line">sudo flannel-<span class="number">0.5</span>.<span class="number">5</span>/flanneld --etcd-endpoints=http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">4001</span> --etcd-prefix=/qinghua.github.io/network --iface=eth1 &gt; flannel.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line">cat flannel.log</span><br></pre></td></tr></table></figure></p>
<p>Flannel启动完成后，会获得一个可用于分配的IP集合，并存放到<code>/run/flannel/subnet.env</code>里。我们需要配置一下docker的可用IP为可用于分配的IP：<br><figure class="highlight sh"><figcaption><span>master node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /run/flannel/subnet.env</span><br><span class="line">sudo sh -c <span class="string">"echo DOCKER_OPTS=\\\"--bip=<span class="variable">$FLANNEL_SUBNET</span> --mtu=<span class="variable">$FLANNEL_MTU</span>\\\" &gt;&gt; /etc/default/docker"</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFAk8s_u73AF_u5883"><a href="#u642D_u5EFAk8s_u73AF_u5883" class="headerlink" title="搭建k8s环境"></a>搭建k8s环境</h2><p>终于轮到k8s啦。首先需要下载并解压kubernetes 1.2.0版：<br><figure class="highlight sh"><figcaption><span>master node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget -c https://github.com/kubernetes/kubernetes/releases/download/v1.<span class="number">2.0</span>/kubernetes.tar.gz</span><br><span class="line">tar zxvf kubernetes.tar.gz</span><br><span class="line">tar zxvf kubernetes/server/kubernetes-server-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>解压出来的文件里面含了一些启动master需要的docker镜像文件，将它们导入：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker load -i kubernetes/server/bin/kube-apiserver.tar</span><br><span class="line">docker load -i kubernetes/server/bin/kube-controller-manager.tar </span><br><span class="line">docker load -i kubernetes/server/bin/kube-scheduler.tar</span><br><span class="line">docker images</span><br></pre></td></tr></table></figure></p>
<p>有条件科学上网的童鞋可以自行准备<code>gcr.io/google_containers/etcd:2.2.1</code>这个镜像，否则就凑合着使用先前的<code>kubernetes/etcd:2.0.5</code>。注意，这里为了简单起见，使用同一套etcd。真实环境里，flannel和kubernetes使用的etcd是分开的。接下来开始启动api server：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=apiserver \</span><br><span class="line">  --net=host \</span><br><span class="line">  gcr.io/google_containers/kube-apiserver:e68c6af15d4672feef7022e94ee4d9af \</span><br><span class="line">  kube-apiserver \</span><br><span class="line">  --insecure-bind-address=<span class="number">192.168</span>.<span class="number">33.17</span> \</span><br><span class="line">  --service-cluster-ip-range=<span class="number">11.0</span>.<span class="number">0.0</span>/<span class="number">16</span> \</span><br><span class="line">  --etcd-servers=http://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">4001</span></span><br></pre></td></tr></table></figure></p>
<p>然后是controller manager：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=cm \</span><br><span class="line">  gcr.io/google_containers/kube-controller-manager:b9107c794e0564bf11719dc554213f7b \</span><br><span class="line">  kube-controller-manager \</span><br><span class="line">  --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<p>最后是scheduler：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=scheduler \</span><br><span class="line">  gcr.io/google_containers/kube-scheduler:<span class="number">903</span>b34d5ed7367ec4dddf846675613c9 \</span><br><span class="line">  kube-scheduler \</span><br><span class="line">  --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<p>服务器启动完毕，可以运行以下命令来查看版本，咱们用的是1.2：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> version</span><br></pre></td></tr></table></figure></p>
<p>接下来该客户端了。首先启动kubelet：<br><figure class="highlight sh"><figcaption><span>node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">NODE_IP=`ifconfig eth1 | grep <span class="string">'inet addr:'</span> | cut <span class="operator">-d</span>: <span class="operator">-f</span>2 | cut <span class="operator">-d</span><span class="string">' '</span> <span class="operator">-f</span>1`</span><br><span class="line">sudo kubernetes/server/bin/kubelet --api-servers=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> --node-ip=<span class="variable">$NODE_IP</span> &gt; kubelet.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line">cat kubelet.log</span><br></pre></td></tr></table></figure></p>
<p>Kubelet启动完成后，在master上就可以看到了：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get no</span><br></pre></td></tr></table></figure></p>
<p>最后启动kube-proxy：<br><figure class="highlight sh"><figcaption><span>node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo kubernetes/server/bin/kube-proxy --master=<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> &gt; proxy.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line">cat proxy.log</span><br></pre></td></tr></table></figure></p>
<h2 id="u6D4B_u8BD5k8s_u73AF_u5883"><a href="#u6D4B_u8BD5k8s_u73AF_u5883" class="headerlink" title="测试k8s环境"></a>测试k8s环境</h2><p>环境安装好了，接下来试着启动一个pod。启动之前，由于kubernetes需要通过gcr.io/google_containers/pause:2.0的小镜像来管理pod的网络。它会自动下载，如果没有科学上网导致下载不到，那么可以先用docker hub上的kubernetes/pause来代替：<br><figure class="highlight sh"><figcaption><span>node1 node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker pull kubernetes/pause</span><br><span class="line">docker tag kubernetes/pause gcr.io/google_containers/pause:<span class="number">2.0</span></span><br></pre></td></tr></table></figure></p>
<p>然后就可以用命令行在任意一台虚拟机上运行一个tomcat，并生成服务：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;tomcat.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ReplicationController</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat</span><br><span class="line">spec:</span><br><span class="line">  replicas: <span class="number">1</span></span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: tomcat</span><br><span class="line">      labels:</span><br><span class="line">        app: tomcat</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: tomcat</span><br><span class="line">        image: tomcat:<span class="number">8.0</span>.<span class="number">30</span>-jre8</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: <span class="number">8080</span></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat</span><br><span class="line">  labels: </span><br><span class="line">    app: tomcat</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat</span><br><span class="line">  ports:</span><br><span class="line">  - port: <span class="number">80</span></span><br><span class="line">    targetPort: <span class="number">8080</span></span><br><span class="line">    nodePort: <span class="number">30088</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> create <span class="operator">-f</span> tomcat.yaml</span><br></pre></td></tr></table></figure></p>
<p>一开始由于需要下载tomcat镜像可能会慢点，随时可以用下面的命令来查看进度：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> describe po tomcat</span><br></pre></td></tr></table></figure></p>
<p>可以用下面的命令来查看pod、replication controller、service和endpoint：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get po</span><br><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get rc</span><br><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get svc</span><br><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get ep</span><br></pre></td></tr></table></figure></p>
<p>我们看到的endpoint里，应该有一个tomcat。在我的虚拟机上它的ENDPOINTS是<code>10.0.8.3:8080</code>，访问一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">POD_IP=`kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> get ep tomcat -o jsonpath=&#123;.subsets[*].addresses[*].ip&#125;`</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$POD_IP</span></span><br><span class="line">curl <span class="variable">$POD_IP</span>:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<p>顺利的话，这三台虚拟机任意一台都可以访问这个tomcat的endpoint。由于启动这三台vagrant虚拟机的主机上并没有安装flannel，所以目前就别想用主机的浏览器打开这个网址啦。但是，由于我们创建服务的时候类型设置为NodePort，这样外部是可以通过任意node的特定端口访问这个服务的。也就是说，下面这两个url都是可以在集群外部访问的，并且效果一样：</p>
<ul>
<li><a href="http://192.168.33.18:30088/" target="_blank" rel="external">http://192.168.33.18:30088/</a></li>
<li><a href="http://192.168.33.19:30088/" target="_blank" rel="external">http://192.168.33.19:30088/</a></li>
</ul>
<p>初步测试完毕，可以使用以下命令来删除刚才创建的tomcat系列对象：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/server/bin/kubectl <span class="operator">-s</span> <span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">8080</span> delete <span class="operator">-f</span> tomcat.yaml</span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如果有10000台机器，你想怎么玩？（九）安全性]]></title>
      <url>http://qinghua.github.io/kubernetes-in-mesos-9/</url>
      <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的安全性，还有多租户。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a><a id="more"></a>
</li>
</ul>
<h2 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h2><h3 id="u9274_u6743_u548C_u6388_u6743"><a href="#u9274_u6743_u548C_u6388_u6743" class="headerlink" title="鉴权和授权"></a>鉴权和授权</h3><p>身份认证分为鉴权（authentication）和授权（authorization）。前者是看你能不能登录，后者是看你登录后有什么权限。Kubernetes的1.2版支持以下<a href="http://kubernetes.io/docs/admin/authentication/" target="_blank" rel="external">5种鉴权</a>方式：</p>
<ul>
<li>CA认证（Client certificate authentication）：就是基于SSL证书的认证，基本概念可以参考<a href="/certificate">证书的那些事儿</a></li>
<li>Token文件认证（Token File）：用一个CSV文件指定用户名、用户id和组名，组名是为了给下面的授权使用的</li>
<li>OpenID认证（OpenID Connect ID Token）：基于第三方的<a href="http://openid.net/specs/openid-connect-core-1_0.html" target="_blank" rel="external">OpenID</a></li>
<li>基本认证（Basic authentication）：也是CSV文件，指定了用户名、密码和用户id</li>
<li>Keystone认证（Keystone authentication）：基于openstack的身份认证服务<a href="http://docs.openstack.org/developer/keystone/" target="_blank" rel="external">Keystone</a></li>
</ul>
<p>身份验证通过，接下来就是授权。Kubernetes的1.2版支持以下<a href="http://kubernetes.io/docs/admin/authorization/" target="_blank" rel="external">4种授权</a>方式：</p>
<ul>
<li>总是拒绝（AlwaysDeny）：这个一般用于测试</li>
<li>总是允许（AlwaysAllow）：只要能登录进来，就有所有权限</li>
<li>用户配置（ABAC）：基于用户授权配置，可以对资源（比如pod、service等）和命名空间设置用户/组的只读或可写权限，多租户管理的时候很有用</li>
<li>钩子（Webhook）：也能实现类似用户配置的粒度，只不过是基于一个远程的REST服务</li>
</ul>
<h3 id="u8D26_u6237"><a href="#u8D26_u6237" class="headerlink" title="账户"></a>账户</h3><p>用户账户（User account）和<a href="http://kubernetes.io/docs/user-guide/service-accounts/" target="_blank" rel="external">服务账户</a>（Service account）在kubernetes里是不同的两个东西。用户账户是给人使用的，不可重复，目前创建的时候需要重启API Server。而服务账户是给pod里的容器使用的，在不同的命名空间中可以重复，比较轻量级一点，可以动态创建。如果创建pod的时候指定了服务账户，那就没的说，直接用就好了；但要是没有指定，k8s会自动为pod指定一个指定命名空间的名为default的服务账户。它是在创建命名空间时自动生成的。</p>
<h3 id="u79D8_u5BC6"><a href="#u79D8_u5BC6" class="headerlink" title="秘密"></a>秘密</h3><p>想象一个正常的开发场景，比如tomcat容器访问mysql，攻城狮们理所当然地把mysql的密码存放在tomcat容器里。Kubernetes提供了<a href="http://kubernetes.io/docs/user-guide/secrets/" target="_blank" rel="external">秘密</a>（Secret）这个对象用来保存这样的敏感信息。需要用的时候像挂卷一样把秘密挂载到pod里就好了，密码就不需要直接写了。有了秘密，妈妈再也不用担心我的霸气侧漏了。实际上，上面提到的服务账户本质上就是秘密的集合。</p>
<h3 id="u591A_u79DF_u6237"><a href="#u591A_u79DF_u6237" class="headerlink" title="多租户"></a>多租户</h3><p>Kubernetes本身支持用不同的命名空间来区分多租户，它们之间不会相互干扰。还有一个系统使用的命名空间叫做<code>kube-system</code>。但是系统的资源是恒定的，如果有个租户打算扶着墙进来，再扶着墙出去，是不是其他人都没得吃了？Kubernetes有一个<a href="http://kubernetes.io/docs/admin/resource-quota/" target="_blank" rel="external">资源限额</a>（Resource Quota）的概念，可以用于命名空间上。目前可以限制的资源有CPU和内存。除了资源，在pod、rc、服务等的数量上也可以进行限制。这样就能很方便地像现在的CaaS那样卖实例吧。</p>
<h2 id="Mesos"><a href="#Mesos" class="headerlink" title="Mesos"></a>Mesos</h2><p>Mesos默认使用<a href="https://en.wikipedia.org/wiki/CRAM-MD5" target="_blank" rel="external">CRAM-MD5</a><a href="http://mesos.apache.org/documentation/latest/authentication/" target="_blank" rel="external">鉴权</a>。最典型的用法是master启动的时候，指定一个文件，里面含有用户名（mesos里叫principal）和密码（mesos里叫secret）。只有通过鉴权的framework和slave才能注册进来。<a href="http://mesos.apache.org/documentation/latest/authorization/" target="_blank" rel="external">授权</a>是通过ACLs（Access Control Lists）来实现的，目前可以支持以下十种行为的授权：</p>
<ul>
<li>register_frameworks: 注册framework</li>
<li>run_tasks: 运行任务</li>
<li>teardown_frameworks: 解除framework</li>
<li>set_quotas: 设置配额</li>
<li>remove_quotas: 移除配额</li>
<li>reserve_resources: 保留资源</li>
<li>unreserve_resources: 解除保留资源</li>
<li>create_volumes: 创建持久化卷</li>
<li>destroy_volumes: 删除持久化卷</li>
<li>update_weights: 更新权重</li>
</ul>
<p>Kubernetes的这些多租户的概念，mesos基本上也都有。就是在名称和具体功效上略有差别。比如，mesos的<a href="http://mesos.apache.org/documentation/latest/roles/" target="_blank" rel="external">角色</a>（role）就像是k8s的命名空间一样，指定role的framework只能使用特定role的mesos slave。在资源限额上，mesos支持<a href="http://mesos.apache.org/documentation/latest/quota/" target="_blank" rel="external">配额</a>和<a href="http://mesos.apache.org/documentation/latest/weights/" target="_blank" rel="external">权重</a>（Weights）。配额管的是保留资源以供未来使用，权重管的是角色分配的资源比例。可惜的是对角色和权重的修改都必须重新启动mesos master，而配额可以通过http请求动态修改。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[证书的那些事儿]]></title>
      <url>http://qinghua.github.io/certificate/</url>
      <content type="html"><![CDATA[<p>现在网络这么发达，许多人都在上面购物、理财、预约挂号…网络上传送的可都是自己的重要资料，比如身份证号，信用卡密码等。因特网如何才能保证这些敏感信息的安全？本文试着来探讨一下加密、证书等那些事儿。<br><a id="more"></a></p>
<h2 id="u5386_u53F2"><a href="#u5386_u53F2" class="headerlink" title="历史"></a>历史</h2><h3 id="u9690_u85CF_u4FE1_u606F"><a href="#u9690_u85CF_u4FE1_u606F" class="headerlink" title="隐藏信息"></a>隐藏信息</h3><p>古代交通不便，一般都是通过送信的方式传递信息。那么如何保证信件的内容不被泄露呢？一种方法是隐藏字迹。中国古代使用矾书，也就是用明矾水来书写保密书信。水干之后没有任何痕迹，泡水时字才显示。还有一招是使用淀粉水在纸上写字，再把纸泡在碘水中显示字迹。据知乎的水波说<a href="https://www.zhihu.com/question/20986883/answer/16811680" target="_blank" rel="external">在西方世界里，也有使用牛奶或者羊奶书写信息，待干掉以后再用高温烘烤使之重新显现字迹的说法</a>。这些办法一旦为人所知，便会轻易被破解，而且还有点儿此地无银三百两的意思。</p>
<h3 id="u52A0_u5BC6_u4FE1_u606F"><a href="#u52A0_u5BC6_u4FE1_u606F" class="headerlink" title="加密信息"></a>加密信息</h3><p>除了隐藏信息以外，还有加密的方法。在宋代兵书《武经总要》里，<a href="https://www.zhihu.com/question/34846340/answer/60080691" target="_blank" rel="external">约定了40个常用的军事短语</a>，送信内容为一首40个字的五言律诗，每个字代表一个军事短语。然后在相应的字上做标记，对方就明白了，这个叫字验。这个就有点儿密码表的意思了。从知乎某匿名用户的回答上找了一张图：<br><img src="/img/ziyan.jpg" alt=""></p>
<p>在西方世界里，古希腊军队将长条羊皮纸缠绕在约定长度和粗细的木棍上书写，木棍称为Scytale。把羊皮纸解下来后就变成没有意义的字母了。古罗马的凯撒大帝用的是字母移位的办法。比如有封信写着：IFMMP，多半我们是不知道啥意思的。要是对方事先说明了把每个字母都右移一位的方法，也就是A变成B，B变成C…Z变成A，那我们就能很轻松地把IFMMP变成HELLO，明白对方的意思。</p>
<h2 id="u73B0_u4EE3"><a href="#u73B0_u4EE3" class="headerlink" title="现代"></a>现代</h2><h3 id="u5BF9_u79F0_u52A0_u5BC6"><a href="#u5BF9_u79F0_u52A0_u5BC6" class="headerlink" title="对称加密"></a>对称加密</h3><p>对上文说的凯撒大帝移位法而言，“右移”就是算法（algorithm），一位的“1”就是密钥（key）。计算机普及后，最早的有影响力的算法是<a href="https://en.wikipedia.org/wiki/Data_Encryption_Standard" target="_blank" rel="external">DES</a>（Data Encryption Standard），它的秘钥是64位，但只有56位会用来计算。所以，只要最多尝试2<sup>56</sup>（大约是七万亿）次的暴力破解，就能解密。随着计算机硬件的发展，这个数量级在1998年只要56小时就能破解，到了1999年变成了24小时以内。这样就不够安全了。于是四年之后，<a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard" target="_blank" rel="external">AES</a>（Advanced Encryption Standard）取代了DES成为了新的标准。它可以使用128、192或256位密钥。最低的128位也能承受大约三千万亿亿次的暴力破解，至少目前看起来还算是比较安全的。在DES向AES的过渡期间，使用了<a href="https://en.wikipedia.org/wiki/Triple_DES" target="_blank" rel="external">3DES</a>算法。所谓3DES，可以理解成重要的事情做三遍，用不同密钥的DES加密三次，几乎也就等同于56×3=168位的密钥，这样也算是比较安全了。</p>
<h3 id="u975E_u5BF9_u79F0_u52A0_u5BC6"><a href="#u975E_u5BF9_u79F0_u52A0_u5BC6" class="headerlink" title="非对称加密"></a>非对称加密</h3><p>对称密钥有一个问题，就是密钥通过什么渠道来传输。加密算法再好，密钥被偷走了，也无济于事。在这样的背景下，非对称加密问世了。它的密钥包含着一个公钥和一个私钥。私钥顾名思义是只有你自己的电脑才知道的，公钥是公开的。用私钥加密的数据只有用公钥才能解开，同样的，用公钥加密的数据只有用私钥才能解开。假如两台电脑甲和乙相互通信，双方先把自己的公钥告诉对方。当甲给乙发消息时，用乙的公钥来加密，由于只有乙有相对应的私钥，所以只有乙能解密，甲要是忘记了消息内容连自己都解不了，更遑论第三方了。反之亦然。非对称加密最常用的算法是<a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)" target="_blank" rel="external">RSA</a>（发明者Rivest、Shmir和Adleman的姓氏首字母）。它的私钥和公钥是通过至少百位的大质数来生成的。由于在数学上很难计算大整数的因数，所以目前来说是比较安全的。万一哪天数学或<a href="http://www.guokr.com/question/530973/" target="_blank" rel="external">量子计算机</a>上有了突破，这种算法也就不再可靠了。1999年，512位的RSA被成功分解。2009年，768位的RSA也被成功分解。现在通用的1024位密钥可能也不那么可靠了，从安全的角度来说应该升级到2048位或以上。密钥这么大，非对称加密的速度比较慢（与对称加密有千倍的差距）也是可以理解的。所以在实际的使用当中一般用对称加密来加密数据，非对称加密来加密对称加密的密钥。</p>
<h3 id="u6563_u5217_u7B97_u6CD5"><a href="#u6563_u5217_u7B97_u6CD5" class="headerlink" title="散列算法"></a>散列算法</h3><p>好吧，密码破解不了，但是攻击者可以截取加密后的数据包，然后篡改。这样“传位十四皇子”就变成了“传位于四皇子”啦。散列算法堵死了这条路。它能把很长的数据变成固定长度的文本。也称为数据的<a href="https://en.wikipedia.org/w/index.php?title=Message_digest&amp;redirect=no" target="_blank" rel="external">摘要</a>（digest）或<a href="https://en.wikipedia.org/wiki/Fingerprint_(computing)" target="_blank" rel="external">指纹</a>（fingerprint）。相同数据的摘要一定是一样的，不同数据的摘要有可能一样，但是通常不一样。正因如此，散列算法是不可逆的，也就是说不能从数据摘要倒推出数据来。但是它可以用来校验数据是否被更改。如果数据有变化，那么摘要通常都是不一样的，概率取决于散列的长度，越长越不容易相同。配合上文所说的非对称加密，如果我用自己的私钥加密了某个摘要，那它就只有用我的公钥才能解密。这就说明了这个摘要一定是我发出的，因为别人不可能有我的私钥。进而推导出摘要所代表的消息也是由我发出的。这个加了密的摘要称为<a href="https://en.wikipedia.org/wiki/Digital_signature" target="_blank" rel="external">数字签名</a>（Digital signature）。它保证数据的完整性，确定数据的发出者，是具有法律效力的。</p>
<p>原来常用的散列算法有<a href="https://en.wikipedia.org/wiki/MD5" target="_blank" rel="external">MD5</a>和<a href="https://en.wikipedia.org/wiki/SHA-1" target="_blank" rel="external">SHA1</a>，2004年后，山东大学的王小云教授通过碰撞法分别攻破了这两种算法。也就是说，在已知摘要的情况下，可以很快计算出另一个拥有相同摘要的文本，这样就实现了文本篡改。行话叫散列碰撞（Hash Collision）。所以<a href="https://en.wikipedia.org/wiki/National_Security_Agency" target="_blank" rel="external">NSA</a>推出了<a href="https://en.wikipedia.org/wiki/SHA-2" target="_blank" rel="external">SHA2</a>和<a href="https://en.wikipedia.org/wiki/SHA-3" target="_blank" rel="external">SHA3</a>成为了新的标准。虽然MD5和SHA1被破解，也不用特别在意。因为虽然能找到相同摘要的数据，但是篡改者并不能随心所欲地把数据修改成自己想要的样子。</p>
<h2 id="u534F_u8BAE"><a href="#u534F_u8BAE" class="headerlink" title="协议"></a>协议</h2><h3 id="SSL/TLS"><a href="#SSL/TLS" class="headerlink" title="SSL/TLS"></a>SSL/TLS</h3><p>有了加密算法，是不是传输就安全了呢？确实如果严格去用的话，是安全了，但是也麻烦了很多。每次都要跟对方沟通用什么算法，传送密钥，传送摘要…累不累？所以需要有一套协议，大家都遵守这样的协议，就能少掉很多我们不需要太关注的、技术上的事情。这个协议叫做<a href="https://en.wikipedia.org/w/index.php?title=Secure_Sockets_Layer&amp;redirect=no" target="_blank" rel="external">SSL</a>（Secure Sockets Layer）。它所做的事情，主要就是上面说的交换公钥，用对方的公钥加密数据，用自己的私钥解密，还支持MD5用于验证数据的完整性。SSL是网景公司发明的，由于应用广泛，成为了事实上的标准。<a href="https://en.wikipedia.org/wiki/Internet_Engineering_Task_Force" target="_blank" rel="external">IETF</a>（Internet Engineering Task Force）在1999年把SSL 3.0标准化，称为<a href="https://en.wikipedia.org/wiki/Transport_Layer_Security" target="_blank" rel="external">TLS</a>（Transport Layer Security）。除了标准化以外，相对SSL来说TLS也更加安全一些。</p>
<h3 id="HTTP/HTTPS"><a href="#HTTP/HTTPS" class="headerlink" title="HTTP/HTTPS"></a>HTTP/HTTPS</h3><p>因特网发明后，大部分的网站都是用的<a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol" target="_blank" rel="external">HTTP</a>协议。这个协议关心的是如何方便地获取到自己需要的资源，并不关心安全性。如果你的网络被监视（比如你的wifi提供者，运营商等等），对方是可以明文看到你的所有信息的，行话叫嗅探（sniffer）。在HTTP上应用SSL/TLS的协议叫做<a href="https://en.wikipedia.org/wiki/HTTPS" target="_blank" rel="external">HTTPS</a>（HTTP over SSL/TLS）。有了它，我们就还可以像以前的HTTP那样方便地在网上冲浪，而不用太担心安全问题，但不是完全不需要担心，一会儿我们会说到。</p>
<p>顺便说一句，SSL/TLS并不是只能用于HTTP，还可以用于<a href="https://en.wikipedia.org/wiki/File_Transfer_Protocol" target="_blank" rel="external">FTP</a>（File Transfer Protocol）、<a href="https://en.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol" target="_blank" rel="external">SMTP</a>（Simple Mail Transfer Protocol）等一系列应用层协议。</p>
<h2 id="CA_u53CA_u8BC1_u4E66"><a href="#CA_u53CA_u8BC1_u4E66" class="headerlink" title="CA及证书"></a>CA及证书</h2><h3 id="u6570_u5B57_u8BC1_u4E66"><a href="#u6570_u5B57_u8BC1_u4E66" class="headerlink" title="数字证书"></a>数字证书</h3><p>前面说到了HTTPS，为什么有了这么安全的技术我们还是不能高枕无忧呢？这是因为：虽然它能保证我们的信息不被第三方破解，但是它绕不开一个很现实的绕口问题：你怎么知道他就是他声称的那个他？举个栗子：你以为正在访问的网站是某个银行，但是却不知道其实你被钓鱼（phishing）了。对方正等着你输入你的银行卡号和密码，好用它们去真正的银行网站给自己转账。这一切发生得这么自然，你的信息只有钓鱼网站能解密看到，钓鱼网站的信息也只有你能解密看到…就算是你注意到了对方的域名，也有可能因为0和O、1和l傻傻分不清楚或者<a href="https://en.wikipedia.org/wiki/Domain_hijacking" target="_blank" rel="external">域名劫持</a>（domain hijacking）而上当。数字证书就是用来阻止这事儿发生的。虽然我不认识你，但是如果我认识一个很知名的家伙，他肯为你做担保，那我也可以相信你。这个知名的家伙就叫<a href="https://en.wikipedia.org/wiki/Certificate_authority" target="_blank" rel="external">CA</a>（Certificate Authority）。它为通信的双方起了一个中间人的作用。CA的担保就叫<a href="https://en.wikipedia.org/wiki/Public_key_certificate" target="_blank" rel="external">数字证书</a>（Digital Certificate或Public Key Certificate）。这个证书是什么格式，有什么内容呢？这是由<a href="https://en.wikipedia.org/wiki/Public_key_infrastructure" target="_blank" rel="external">PKI</a>（Public Key Infrastructure）标准决定的。常用的标准有<a href="https://en.wikipedia.org/wiki/X.509" target="_blank" rel="external">X.509</a>和<a href="https://en.wikipedia.org/wiki/PKCS_12" target="_blank" rel="external">PKCS #12</a>。它们定义了证书里应该含有签发机构名、证书用户名、有效期、算法、公钥等信息。</p>
<h3 id="u4FE1_u4EFB_u94FE"><a href="#u4FE1_u4EFB_u94FE" class="headerlink" title="信任链"></a>信任链</h3><p>回到刚才的问题：你怎么知道他就是他声称的那个他？我无条件地信任CA，网站又有了CA的数字证书，我就能信任他就是证书里声称的那个他。如果我因为对CA的信任而有了损失，则证书可以用来追究CA的法律责任。如果我们信任A，A信任B，B信任C，那么我们也会信任B和C，这个叫做<a href="https://en.wikipedia.org/wiki/Chain_of_trust" target="_blank" rel="external">证书信任链</a>（Chain of trust）。中间的B称为<a href="https://hk.godaddy.com/en/help/what-is-an-intermediate-certificate-868" target="_blank" rel="external">中级证书</a>（Intermediate certificate），位于信任链顶端的A称为<a href="https://en.wikipedia.org/wiki/Root_certificate" target="_blank" rel="external">根证书</a>（Root certificate）。如果根证书出了问题，那么它所信任的其它证书也就不再可信了。这个后果可是非常严重，可能会影响整个因特网的信任体系。所以，需要尽可能地少动用根证书以减少根证书的私钥被盗用的风险。如果网站想要被认证，直接用中级证书认证就好了。万一中级证书出问题，吊销掉中级证书也只会影响一部分客户，比整个根证书被吊销掉强。但事情也不绝对。对CA来说，公信力就是一切。说个案例：<a href="https://en.wikipedia.org/wiki/China_Internet_Network_Information_Center" target="_blank" rel="external">中国互联网络信息中心</a>（China Internet Network Information Center，CNNIC）是中国的顶级域名<code>.cn</code>和中文域名的注册管理机构。MCS集团用CNNIC签发的中级证书，发行了多个冒充成Google的假证书。于是在2015年4月份，chrome、firefox都宣布不再信任CNNIC的证书。如果你用chrome来打开<a href="https://www.cnnic.net.cn/" target="_blank" rel="external">https://www.cnnic.net.cn/</a>，应该会看到：<br><img src="/img/https_untrust.png" alt=""></p>
<p>而不是：<br><img src="/img/https_trust.png" alt=""></p>
<p>一般来说操作系统和浏览器都会内置一些信任的CA，比较著名的有公信力的CA有Verisign，GeoTrust等。我们打开google的时候，也能点击小锁看到它的证书：<br><img src="/img/google-ca.jpg" alt=""></p>
<p>再点击证书信息就能看到它的证书链。最上面的是GeoTrust的根证书，它包含的内容都在里面写着了，感兴趣的话就自己看看吧：<br><img src="/img/ca1.jpg" alt=""></p>
<p>中间的是谷歌自己的中级证书：<br><img src="/img/ca2.jpg" alt=""></p>
<p>最后是站点自己的证书，这个有效期一般比较短：<br><img src="/img/ca3.jpg" alt=""></p>
<p>证书是可以自签名的，也就是说，你自己作为CA来发行这个证书。当然，这个证书也只有你自己会信任，广大的网友同志们的眼睛是雪亮的，不会无缘无故地信任你的。那如果我是大企业，是不是就值得信任了？谷歌、微软等通常都是可以信任的。12306，你信任它吗？每个人都会有自己的答案吧。打开<a href="https://www.12306.cn/" target="_blank" rel="external">12306</a>看看它的根证书，SRCA（SinoRail Certification Authority）是个什么鬼？这个是中国铁路自己啊。知道为什么首页上总有一个“为保障您顺畅购票，请下载安装根证书”的提示了吧。</p>
<h3 id="u7533_u8BF7_u8BC1_u4E66"><a href="#u7533_u8BF7_u8BC1_u4E66" class="headerlink" title="申请证书"></a>申请证书</h3><p>首先，如果你觉得自己的网站没有什么信息不能公开的，那就没必要费那钱和时间去购买证书。</p>
<p>证书有很多种类型，价格也很不一样，大约一年数千到数万元人民币吧。简单列出几种：</p>
<ul>
<li>通配符证书（Wildcard SSL Certificates）：自己的域名和下一级子域名可以使用。比如<code>google.com</code>和<code>maps.google.com</code>就是域名和二级子域名的关系</li>
<li>多域名证书（Subject Alternative Name SSL Certificates）：不仅限于子域名，不同域名也能使用</li>
<li>增强型证书（Extended Validation SSL Certificate）：总之就是验证流程更麻烦，结果就是可以在地址栏显示公司的名称，增加可信度。就像这样：<br><img src="/img/github-ca.jpg" alt=""></li>
</ul>
<p>申请证书需要给CA提供一个<a href="https://en.wikipedia.org/wiki/Certificate_signing_request" target="_blank" rel="external">CSR</a>（certificate sigining request）文件。通常做法就是通过程序把域名、联系人和公钥等信息都放在这个文件里，发送给某个CA。交费之后，如果CA方面没有问题，就会对CSR文件设置有效期等操作，当然还需要用自己的私钥对证书签名，再发送给用户。最后用户把证书绑定到自己的网站上。</p>
<h2 id="u52A8_u624B_u65F6_u95F4"><a href="#u52A8_u624B_u65F6_u95F4" class="headerlink" title="动手时间"></a>动手时间</h2><h3 id="u52A0_u89E3_u5BC6"><a href="#u52A0_u89E3_u5BC6" class="headerlink" title="加解密"></a>加解密</h3><p><a href="https://en.wikipedia.org/wiki/OpenSSL" target="_blank" rel="external">OpenSSL</a>是SSL/TLS的开源实现，我们就用它来练练手吧。首先，用DES加解密文本：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World! | openssl enc <span class="operator">-e</span> -des <span class="operator">-a</span>    <span class="comment">## 需要输入两次密码，如果你输入的都是1，那么可以用下一条命令解密</span></span><br><span class="line"><span class="built_in">echo</span> U2FsdGVkX19NMXhTRoTNJE0YV+TKcRL0+xzT9UMUN5Y= | openssl enc <span class="operator">-d</span> -des <span class="operator">-a</span></span><br></pre></td></tr></table></figure></p>
<p>其中的<code>-a</code>代表base64编码。还可以多试几次加密，就算文本、密码相同，每次加密的结果也很可能是不一样的。把上面的<code>-des</code>变成<code>-des3</code>或者是<code>-aes-256-cbc</code>就可以自行尝试DES3和AES加解密了。接下来尝试RSA。首先生成RSA的私钥：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openssl genrsa -out private.pem <span class="number">2048</span></span><br><span class="line">cat private.pem</span><br></pre></td></tr></table></figure></p>
<p>PEM表示这是base64编码的密钥，也可以改成DER即二进制的密钥，加上一个<code>-outform DER</code>的参数就行。然后通过私钥生成公钥：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openssl rsa -in private.pem -pubout -out public.pem</span><br><span class="line">cat public.pem</span><br></pre></td></tr></table></figure></p>
<p>接下来用刚刚生成的公钥加密，私钥解密：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World! | openssl rsautl -encrypt -pubin -inkey public.pem -out encrypt</span><br><span class="line">cat encrypt</span><br><span class="line">cat encrypt | openssl rsautl -decrypt -inkey private.pem</span><br></pre></td></tr></table></figure></p>
<p>然后私钥加密，公钥解密。注意这个的意义其实是私钥签名，公钥认证：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World! | openssl rsautl -sign -inkey private.pem -out encrypt</span><br><span class="line">cat encrypt</span><br><span class="line">cat encrypt | openssl rsautl -verify -pubin -inkey public.pem</span><br></pre></td></tr></table></figure></p>
<h3 id="u6563_u5217"><a href="#u6563_u5217" class="headerlink" title="散列"></a>散列</h3><p>现在轮到散列算法了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World! | openssl dgst -md5</span><br><span class="line"><span class="built_in">echo</span> Hello World! | openssl dgst -sha1</span><br></pre></td></tr></table></figure></p>
<p>也可以用linux自带的小工具实现：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Hello World! | md5sum</span><br><span class="line"><span class="built_in">echo</span> Hello World! | sha1sum</span><br><span class="line"><span class="built_in">echo</span> Hello World! | sha224sum</span><br><span class="line"><span class="built_in">echo</span> Hello World! | sha256sum</span><br><span class="line"><span class="built_in">echo</span> Hello World! | sha384sum</span><br><span class="line"><span class="built_in">echo</span> Hello World! | sha512sum</span><br></pre></td></tr></table></figure></p>
<p>是不是越来越长了？散列越长越不容易被碰撞。</p>
<h3 id="u8BC1_u4E66"><a href="#u8BC1_u4E66" class="headerlink" title="证书"></a>证书</h3><p>申请证书，首先需要生成CSR文件。而CSR文件需要先生成私钥：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openssl genrsa -out private.pem <span class="number">2048</span></span><br><span class="line">openssl req -new -key private.pem -out domain.csr</span><br><span class="line">cat domain.csr</span><br></pre></td></tr></table></figure></p>
<p>生成私钥文件是可以加密的，加上一个参数比如<code>-des3</code>就可以了。生成CSR文件的时候需要填写各种信息，没耐心就随便写点什么甚至一路回车也行，反正又不是真的去找CA。如果你真的有需求，<a href="https://support.rackspace.com/how-to/generate-a-csr-with-openssl/#create-a-csr" target="_blank" rel="external">这里有一张表格</a>说明了应该怎么填。填完的东西可以这么看：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl req -noout -text -in domain.csr</span><br></pre></td></tr></table></figure></p>
<p>现在我们假装自己是个CA，有自己的密钥对，然后对刚才提交的CSR文件签名：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openssl genrsa -out private_ca.pem <span class="number">2048</span></span><br><span class="line">openssl x509 -req -days <span class="number">365</span> -in domain.csr -signkey private_ca.pem -out my_domain.crt</span><br><span class="line">cat my_domain.crt</span><br></pre></td></tr></table></figure></p>
<p>大功告成！生成的my_domain.crt就是我们要的证书。由于这个是自签名证书，默认是不被我们的操作系统信任的。如果我们需要增加信任，可以参考<a href="https://briansnelson.com/How_to_add_trusted_root_certificates" target="_blank" rel="external">这里</a>。比如在Ubuntu/Debian里可以这么做：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo cp my_domain.crt /usr/<span class="built_in">local</span>/share/ca-certificates/</span><br><span class="line">sudo update-ca-certificates</span><br></pre></td></tr></table></figure></p>
<p>如果要取消信任，可以这么做：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo rm /usr/<span class="built_in">local</span>/share/ca-certificates/my_domain.crt</span><br><span class="line">sudo update-ca-certificates --fresh</span><br></pre></td></tr></table></figure></p>
<h2 id="u53C2_u8003_u8D44_u6599"><a href="#u53C2_u8003_u8D44_u6599" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://computer.howstuffworks.com/encryption.htm" target="_blank" rel="external">How Encryption Works</a><br><a href="http://victor1980.blog.51cto.com/3664622/1659447" target="_blank" rel="external">有关SSL证书的一些事儿</a><br><a href="http://seanlook.com/2015/01/07/tls-ssl/" target="_blank" rel="external">SSL/TLS原理详解</a><br><a href="http://seanlook.com/2015/01/15/openssl-certificate-encryption/" target="_blank" rel="external">OpenSSL 与 SSL 数字证书概念贴</a><br><a href="https://program-think.blogspot.com/2010/02/introduce-digital-certificate-and-ca.html" target="_blank" rel="external">数字证书及 CA 的扫盲介绍</a><br><a href="https://program-think.blogspot.com/2014/11/https-ssl-tls-1.html" target="_blank" rel="external">扫盲 HTTPS 和 SSL/TLS 协议</a><br><a href="http://www.ruanyifeng.com/blog/2011/08/what_is_a_digital_signature.html" target="_blank" rel="external">数字签名是什么？</a><br><a href="http://boxingp.github.io/blog/2015/04/04/should-cnnic-certificate-to-be-trusted/" target="_blank" rel="external">CNNIC证书值得信任吗？</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[用容器轻松搭建Portus运行环境]]></title>
      <url>http://qinghua.github.io/portus/</url>
      <content type="html"><![CDATA[<p>Docker官方并没有提供docker registry的用户界面，对权限的控制粒度也比较粗。SUSE的<a href="http://port.us.org/" target="_blank" rel="external">Portus</a>很好地解决了这个问题。除了界面以外，它还提供了更细粒度的权限控制、用户认证等功能。本文尝试从零开始用容器搭建一个portus环境。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配一台IP是<strong>192.168.33.18</strong>的虚拟机。Registry配上portus会比较耗内存，所以我们给它2G内存，默认是512M。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line">config.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">  v.memory = <span class="number">2048</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后在终端运行以下命令启动并连接虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>我们将会把docker registry和portus都安装在同一台虚拟机上。一方面是比较方便，另一方面也避免了<a href="https://github.com/SUSE/Portus/issues/510" target="_blank" rel="external">时钟同步问题</a>。为了启动一个带认证的docker registry，首先要生成自签名证书：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; ssl.conf</span><br><span class="line">[ req ]</span><br><span class="line">prompt             = no</span><br><span class="line">distinguished_name = req_subj</span><br><span class="line">x509_extensions    = x509_ext</span><br><span class="line"></span><br><span class="line">[ req_subj ]</span><br><span class="line">CN = Localhost</span><br><span class="line"></span><br><span class="line">[ x509_ext ]</span><br><span class="line">subjectKeyIdentifier   = <span class="built_in">hash</span></span><br><span class="line">authorityKeyIdentifier = keyid,issuer</span><br><span class="line">basicConstraints       = CA:<span class="literal">true</span></span><br><span class="line">subjectAltName         = @alternate_names</span><br><span class="line"></span><br><span class="line">[ alternate_names ]</span><br><span class="line">DNS.<span class="number">1</span> = localhost</span><br><span class="line">IP.<span class="number">1</span>  = <span class="number">192.168</span>.<span class="number">33.18</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo mkdir /certs</span><br><span class="line">sudo sh -c <span class="string">"openssl req -config ssl.conf \</span><br><span class="line">-new -x509 -nodes -sha256 -days 365 -newkey rsa:4096 \</span><br><span class="line">-keyout /certs/server-key.pem -out /certs/server-crt.pem"</span></span><br></pre></td></tr></table></figure></p>
<p>证书生成好了，但是由于这是自签名证书，客户端还需要配置证书文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /etc/docker/certs.d/<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span></span><br><span class="line">sudo cp /certs/server-crt.pem /etc/docker/certs.d/<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/ca.crt</span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<p>接下来生成一个registry的配置文件，里面指定刚才的证书和token方式的认证。认证服务器设置到一会儿要启动的portus去：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; config.yml</span><br><span class="line">version: <span class="number">0.1</span></span><br><span class="line">loglevel: debug</span><br><span class="line">storage:</span><br><span class="line">    cache:</span><br><span class="line">        blobdescriptor: inmemory</span><br><span class="line">    filesystem:</span><br><span class="line">        rootdirectory: /var/lib/registry</span><br><span class="line">    delete:</span><br><span class="line">        enabled: <span class="literal">true</span></span><br><span class="line">http:</span><br><span class="line">    addr: :<span class="number">5000</span></span><br><span class="line">    headers:</span><br><span class="line">        X-Content-Type-Options: [nosniff]</span><br><span class="line">    tls:</span><br><span class="line">        certificate: /certs/server-crt.pem</span><br><span class="line">        key: /certs/server-key.pem</span><br><span class="line">auth:</span><br><span class="line">    token:</span><br><span class="line">        realm: https://<span class="number">192.168</span>.<span class="number">33.18</span>/v2/token</span><br><span class="line">        service: <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span></span><br><span class="line">        issuer: <span class="number">192.168</span>.<span class="number">33.18</span></span><br><span class="line">        rootcertbundle: /certs/server-crt.pem</span><br><span class="line">notifications:</span><br><span class="line">    endpoints:</span><br><span class="line">      - name: portus</span><br><span class="line">        url: https://<span class="number">192.168</span>.<span class="number">33.18</span>/v2/webhooks/events</span><br><span class="line">        timeout: <span class="number">500</span>ms</span><br><span class="line">        threshold: <span class="number">5</span></span><br><span class="line">        backoff: <span class="number">1</span>s</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>然后就可以启动registry容器了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --name registry \</span><br><span class="line">    -p <span class="number">5000</span>:<span class="number">5000</span> \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /var/lib/registry:/var/lib/registry \</span><br><span class="line">    -v /certs:/certs \</span><br><span class="line">    -v `<span class="built_in">pwd</span>`/config.yml:/etc/docker/registry/config.yml \</span><br><span class="line">    registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>Docker registry配置完成后，就该准备portus了。Portus需要一个数据库来存储信息，官方推荐<a href="https://mariadb.org/" target="_blank" rel="external">MariaDB</a>，当然mysql也是没问题的。我们把数据库启动起来：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --name mariadb \</span><br><span class="line">    --net=host \</span><br><span class="line">    --restart=always \</span><br><span class="line">    <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">    <span class="operator">-e</span> TERM=xterm \</span><br><span class="line">    mariadb:<span class="number">10.1</span>.<span class="number">10</span></span><br></pre></td></tr></table></figure></p>
<p>等数据库启动完成，我们连接上去：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mariadb mysql -uroot -p123456</span><br></pre></td></tr></table></figure></p>
<p>为portus创建用户和数据库：<br><figure class="highlight sh"><figcaption><span>sql</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create database portus;</span><br><span class="line">GRANT ALL ON portus.* TO <span class="string">'portus'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'portus'</span>;</span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>万事俱备，让我们来启动portus：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">docker run -it <span class="operator">-d</span> \</span><br><span class="line">    --name portus \</span><br><span class="line">    --net host \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /certs:/certs \</span><br><span class="line">    -v /usr/sbin/update-ca-certificates:/usr/sbin/update-ca-certificates \</span><br><span class="line">    -v /etc/ca-certificates:/etc/ca-certificates \</span><br><span class="line">    --env DB_ADAPTER=mysql2 \</span><br><span class="line">    --env DB_ENCODING=utf8 \</span><br><span class="line">    --env DB_HOST=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    --env DB_PORT=<span class="number">3306</span> \</span><br><span class="line">    --env DB_USERNAME=portus \</span><br><span class="line">    --env DB_PASSWORD=portus \</span><br><span class="line">    --env DB_DATABASE=portus \</span><br><span class="line">    --env RACK_ENV=production \</span><br><span class="line">    --env RAILS_ENV=production \</span><br><span class="line">    --env PUMA_SSL_KEY=/certs/server-key.pem \</span><br><span class="line">    --env PUMA_SSL_CRT=/certs/server-crt.pem \</span><br><span class="line">    --env PUMA_PORT=<span class="number">443</span> \</span><br><span class="line">    --env PUMA_WORKERS=<span class="number">4</span> \</span><br><span class="line">    --env MACHINE_FQDN=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    --env SECRETS_SECRET_KEY_BASE=secret-goes-here \</span><br><span class="line">    --env SECRETS_ENCRYPTION_PRIVATE_KEY_PATH=/certs/server-key.pem \</span><br><span class="line">    --env SECRETS_PORTUS_PASSWORD=portuspw \</span><br><span class="line">    h0tbird/portus:v2.<span class="number">0.2</span>-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>启动完成后，在浏览器打开<code>https://192.168.33.18/</code>，应该会看到证书不被浏览器所信任的提示。无视之，选择继续的话，应该就能看到注册页面啦：<br><img src="/img/portus-sign-up.jpg" alt=""></p>
<h2 id="u6743_u9650_u7BA1_u7406"><a href="#u6743_u9650_u7BA1_u7406" class="headerlink" title="权限管理"></a>权限管理</h2><p>Portus现在只能管理一个私有库。它有一个团队的概念，每一个团队可以有多个命名空间，每个命名空间就是多个镜像的集合。每个团队有三种角色：</p>
<ul>
<li>查看者（Viewer）：只能pull镜像</li>
<li>贡献者（Contributor）：除了pull，还可以push镜像</li>
<li>所有者（Owner）：除了推拉镜像，还可以对团队成员进行管理</li>
</ul>
<p>由于角色是定义在团队里的，所以命名空间就不需要再考虑权限问题了，它只是镜像的集合而已。命名空间也有三种类型：</p>
<ul>
<li>全局（Global）：只有管理员可以push，其他人只能pull</li>
<li>团队（Team）：团队成员可以做自己角色支持的操作</li>
<li>个人（Personal）：只有所有者和管理员可以推拉</li>
</ul>
<p>命名空间还可以设置为public，这样不需要login也能pull。</p>
<p>说完一些基本概念，让我们来尝试一下。首先，portus需要配置一个用户，来调用docker registry的API，与其进行同步。<a href="http://port.us.org/docs/How-to-setup-secure-registry.html#synchronizing-the-registry-and-portus" target="_blank" rel="external">同步</a>有<a href="http://port.us.org/features/1_Synchronizing-the-Registry-and-Portus.html" target="_blank" rel="external">两种方式</a>：一是在docker registry的配置文件里写的<code>notifications</code>，这样每当有人push一个新镜像上去，docker registry将会通知portus修改数据库。可是时间长了，有可能数据库偶尔挂掉或是网络不稳定啥的导致两边数据不一致。Portus针对这种情况也提供了一个crono的job，设置定时运行即可，一会儿我们会试验。现在先让我们来创建这个用户：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> portus bundle <span class="built_in">exec</span> rake portus:create_api_account</span><br></pre></td></tr></table></figure></p>
<p>接下来就可以在注册页面自行注册啦，注册完毕后会跳转到登记registry页面：<br><img src="/img/portus-new-registry.jpg" alt=""></p>
<p>照上图填入registry，点击<strong>Create</strong>按钮创建一个docker registry。接着创建一个新用户。点击左边的<strong>Admin</strong>，再点击中间的<strong>Users</strong>，然后点击右边的<strong>Create new user</strong>，填写用户信息：<br><img src="/img/portus-new-user.jpg" alt=""></p>
<p>点击<strong>Add</strong>按钮就可以创建一个新用户了。接下来创建一个团队。点击左边的<strong>Teams</strong>，再点击右边的<strong>Create new team</strong>，填写团队信息：<br><img src="/img/portus-new-team.jpg" alt=""></p>
<p>点击<strong>Add</strong>按钮就可以创建一个新团队了。点击刚刚创建好的团队，再点击右边的<strong>Add namespace</strong>，填写命名空间信息：<br><img src="/img/portus-new-namespace.jpg" alt=""></p>
<p>点击<strong>Add</strong>按钮就可以创建一个新命名空间了。接下来把用户添加到这个团队中。点击右边的<strong>Add members</strong>，填写刚才增加的用户信息：<br><img src="/img/portus-new-member.jpg" alt=""></p>
<p>点击<strong>Add</strong>按钮就可以把用户加进来了。回到控制台，搞一个镜像，push一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker pull microbox/etcd:<span class="number">2.1</span>.<span class="number">1</span></span><br><span class="line">docker tag microbox/etcd:<span class="number">2.1</span>.<span class="number">1</span> <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/microbox/etcd:<span class="number">2.1</span>.<span class="number">1</span></span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/microbox/etcd:<span class="number">2.1</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>出错了：<strong>unauthorized: authentication required</strong>，我们必须用docker先登录：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u gggg <span class="operator">-e</span> gggg@<span class="number">123</span>.com <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span></span><br></pre></td></tr></table></figure></p>
<p>填上自己刚才设置的密码，登录成功之后，再试着push一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/microbox/etcd:<span class="number">2.1</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>Bingo！换一个命名空间试试看：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker tag h0tbird/portus:v2.<span class="number">0.2</span>-<span class="number">1</span> <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/h0tbird/portus:v2.<span class="number">0.2</span>-<span class="number">1</span></span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/h0tbird/portus:v2.<span class="number">0.2</span>-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>出错了：<strong>unauthorized: authentication required</strong>，可见我们的权限控制确实起作用了。</p>
<h2 id="u955C_u50CF_u540C_u6B65"><a href="#u955C_u50CF_u540C_u6B65" class="headerlink" title="镜像同步"></a>镜像同步</h2><p>接下来我们试试定时同步任务。首先需要在容器里信任我们的自签名证书：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> portus mkdir /usr/<span class="built_in">local</span>/share/ca-certificates</span><br><span class="line">docker cp /certs/server-crt.pem portus:/usr/<span class="built_in">local</span>/share/ca-certificates/ca.crt</span><br><span class="line">docker <span class="built_in">exec</span> portus update-ca-certificates</span><br></pre></td></tr></table></figure></p>
<p>然后启动定时同步任务，设置为每10秒钟同步一次：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it portus bash</span><br><span class="line">RAILS_ENV=production CATALOG_CRON=<span class="string">"10.seconds"</span> bundle <span class="built_in">exec</span> crono</span><br></pre></td></tr></table></figure></p>
<p>等十秒钟，就会看到<strong>[catalog] Created the tag ‘2.1.1’</strong>的提示。如果先前没有信任自签名证书，同步的时候会报<strong>certificate verify failed</strong>的错误。现在回到portus的界面，点击左边的<strong>Dashboard</strong>，就能看到刚才push的microbox/etcd镜像已经显示在右边了：<br><img src="/img/portus-dashboard.jpg" alt=""></p>
<p>最后一步就是自动同步了，先把刚才的crono给Ctrl+C掉，Ctrl+D退出portus容器。由于docker registry需要调用portus的API，所以我们需要在registry容器里也信任这个证书：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker cp /certs/server-crt.pem registry:/usr/<span class="built_in">local</span>/share/ca-certificates/ca.crt</span><br><span class="line">docker <span class="built_in">exec</span> registry update-ca-certificates</span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<p>然后再push一个镜像：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker tag registry:<span class="number">2.3</span>.<span class="number">0</span> <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/microbox/registry:<span class="number">2.3</span>.<span class="number">0</span></span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/microbox/registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>到portus的dashboard刷新一下，搞定！在中间的<strong>Recent activities</strong>还能看到是谁push的这个镜像，对审计、追踪来说很有帮助。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[HTC VIVE初体验]]></title>
      <url>http://qinghua.github.io/vr-htc-vive/</url>
      <content type="html"><![CDATA[<p>没想到到深圳来出差，竟然碰上了<a href="https://bbs.htc.com/cn/news.php?mod=viewthread&amp;tid=68841" target="_blank" rel="external">HTC Vive中国开发者峰会</a>。虽然我不是VR开发者，但怎么说也是一名消费者，借着周末赶紧去体验了一把。趁着现在热乎劲还没过去，写一下我的一些见闻及感受吧。<br><a id="more"></a></p>
<h2 id="u65E5_u7A0B"><a href="#u65E5_u7A0B" class="headerlink" title="日程"></a>日程</h2><p>3月9~11日是开发者峰会，11~18日的每天10:00~18:00是公众开放体验时间，可以在深圳大学附近的威盛科技大厦29层参与免费体验。一共有8个项目，分为A、B两组，每组4个项目。一次排队只能排一个组，每人有十分钟的时间，可以自己选择两个项目来体验。A组有蓝色世界、空间画刷、神秘商店和太空海盗，较B组来说会稍微刺激一些。B组有机器人修理、亚利桑那阳光、未来办公室和预算消减（真个怪名字）。具体项目内容下文会详细叙述。</p>
<h2 id="u4F53_u9A8C"><a href="#u4F53_u9A8C" class="headerlink" title="体验"></a>体验</h2><p>Vive包含一个头戴设备（含耳机），两个手柄和两个基站。玩之前先坐在椅子上，首先工作人员会帮忙戴上头戴设备及耳机。Vive是可以把眼镜一起套在设备里的。第一次体验的时候，戴得比较正，几乎感觉不出来里面还有个我自己的眼镜。第二次的工作人员就有点儿不那么专业，戴着不那么舒服，也影响了效果。可见姿势很重要。这时候眼前看到的是一片白色的背景，还有很多经纬线。往下看的话，有很多同心圆，中心就是自己所在的位置。如果转向了，可以根据脚下的显示回到中心。这感觉像是Vive的操作系统。上下左右前后都是有内容的，都可以扭头、转身去看。但是光转动眼珠子是不行的哦，因为Vive是根据陀螺仪来判断方位的。底下稍稍有点儿漏光，不过不注意的话也不太能感觉出来。这时旁边的工作人员会帮助选择要体验的项目，教玩家一些基本用法。之后工作人员把手柄递给我。这时我眼前看到的是两个悬空的手柄在等着我伸手去够。在没有准备的时候可能会吓一小跳。手柄下面有扳机，两侧有按钮，大拇指可以够着正面，有一个圆形的触摸板，也可以当作按钮按下。然后工作人员让我站起来，我可以自由移动了。如果快要碰到实体墙的话，眼镜里会有虚拟的透明墙出现，这时就知道不要再过去了，不然要碰鼻子滴。</p>
<p>很快工作人员用电脑帮我调到第一个项目：亚利桑那阳光。这是个打僵尸的游戏，那两个带扳机的手柄就是我的手枪，按下触摸板可以装子弹。工作人员提醒我向左边看，我一扭头，原来是几个玻璃瓶。用手枪瞄准玻璃瓶，把它们都打碎，游戏也就正式开始了。手枪带有激光瞄准，准心处是有红点的，所以知道子弹会射向哪里，但是因为手总是不可能完全静止，会有轻微的抖动，所以射击出来也不是那么的准，需要适应一下。画面随即切换到了户外，估计是亚利桑那洲吧，有几个僵尸向我冲过来。用刚才的手枪把僵尸们都干掉吧。随着剧情的深入，可以拿到更高级的武器，打起来也就更爽了。可惜手柄没有震动反馈，不然加上后坐力一定更能带来更深的沉浸感。如果仔细观看，还是有一些颗粒感的。</p>
<p>五分钟很快就结束了，接下来是未来办公室项目。这是一个虚拟的办公室，工作人员会引导我用手柄拿杯子，接咖啡，开抽屉，拿文件，开电脑什么的。中间咖啡杯没拿稳还摔地上了，可惜了一整杯咖啡 ：） 吐个槽，怎么未来办公室的电脑屏幕还是CRT的呢，也太不和谐了。</p>
<p>B组的两个项目体验完了，我就到A组去，这回排了一个半小时。听说这还是快的了，去年12月在北京的活动，都需要排3个小时队。接下来我选的项目是神秘商店。我在一个商店里，寻找白色的图腾。每找到一个，商店就会把我传送到另一个地方去。可能出现各种奇怪的东西，比如一个盒子里蹦出来一个弹簧人，吓了我一跳。可以拿手上的手柄（这次是魔法杖）狠狠地抽它。</p>
<p>然后是太空海盗。这也是一个射击游戏。两个手柄一个是枪，一个是盾。海盗们源源不断地乘小飞机逼近并射击我，需要用盾挡住他们的进攻并用枪把他们击落。由于这个比较立体，不像上面打僵尸那个游戏，僵尸只会在地面跑，所以难度还是会大一点的。</p>
<p>A组也体验了两个项目，接下来继续回到B组。中午了，排队的人少了一些，这回只用了一个小时就排到了。首先是预算消减。这是一个密室逃脱类游戏，由于空间限制，不可能无限制地走动，所以是通过手柄来移动的。剧情上，大约就是费劲心思搞到一把刀，然后用它去捅某个怪物…我比较愚钝一点，反倒是被怪物给捅了…</p>
<p>下一个是机器人修理。一开始先是开抽屉等基本动作，然后大门打开，有个有问题的机器人过来，让我帮忙修理它。也不知道是我水平不行还是程序设计如此，总之没有修好，然后地板自动翻开，机器人变成一堆零件掉了下去。虽然知道实际上只是普通的地板而已，但是看着脚下的深渊，还是不由的让人感叹如此深的沉浸感。</p>
<p>本想就此结束体验，但是听说剩下的A组两个项目都是最好玩的…于是咬咬牙又等了一个半小时。这回是蓝色世界。我在一艘沉船上，看着周围的鱼儿游来游去。后来来了一条大鲸鱼，细节之处看得非常清楚。最后鲸鱼游走，大尾巴打过来，那种感觉还是很震撼的。</p>
<p>最后一个项目是google开发的空间画刷。一支手柄是菜单，另一支是画笔。只要旋转菜单手柄就可以切换菜单，再用画笔手柄来选择。中间工作人员会帮助切换不同的背景，比如雪世界等等。而画笔也能画出不同的效果，比如火焰般的燃烧效果。而这一切都是在三维空间上存在着的，于是乎就有了神笔马良般的感觉。Google出品，果然不一般。可惜啊，体验时间很快又结束了。就这么的一天过去了。不过一个项目也没落下，还是很值得的。</p>
<h2 id="u884C_u4E1A_u73B0_u72B6"><a href="#u884C_u4E1A_u73B0_u72B6" class="headerlink" title="行业现状"></a>行业现状</h2><p>现在的现实模拟技术主要分为两类：VR（Virtual Reality）虚拟现实和AR（Augmented Reality）增强现实。前者就像玩游戏那样，一切都是假的。后者是在现实中增加虚拟的物体，代表产品有谷歌眼镜和微软的HoloLens。VR贵则数千，便宜则一两百甚至几十。高级的代表产品有Oculus Rift（这就是被facebook以20亿刀收购的公司开发的产品），PS VR（还未发售，据说快了）和HTC Vive（最贵，但效果也最好，支持几平米级别的移动）。为了减少头晕现象，需要提高刷新率至90Hz，降低延迟至15ms。除了PS VR有PS4以外，另外两款都需要配合高性能PC（至少970以上显卡才能玩转，整机估计得上万元了）才能跑得动，这也有点儿限制了高端VR的普及。便宜的VR以三星Gear VR、暴风魔镜等为代表，不像高端VR那样自带屏幕，一般就是透镜加手机。甚至谷歌搞出一个Cardboard，就是纸盒加透镜。由于大众手机分辨率目前一般是1920×1080，分成左右眼的两个屏幕后就变成960×1080，如果看片的话，由于影片的宽高比，1080还得往下降到540左右，所以导致颗粒感比较明显。如果配上更高的分辨率和源文件，那效果就会更好一些。当然这个价格的话，体验体验还是可以的哈。其实还有一个混合现实（Mixed Reality），它包括增强现实和增强虚拟，在新的可视化环境里物理和数字对象共存，并实时互动。不过目前还是以研究居多，暂时还没有代表产品问世。</p>
<p>体验完最顶级的VR后，正好这几天又碰上阿尔法狗大胜李世石。看来黑客帝国的时代离我们人类不远了。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[轻松搭建Docker Registry运行环境]]></title>
      <url>http://qinghua.github.io/docker-registry/</url>
      <content type="html"><![CDATA[<p>我们知道Docker官方提供了一个公有的registry叫做Docker Hub。但是企业内部可能有些镜像还是不方便放到公网上去，所以docker也提供了registry镜像来让需要的人自己搭建私有仓库。本文从零开始搭建Docker Registry的运行环境，并添加用户界面和认证功能。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配两台虚拟机，一台叫做<strong>registry</strong>，它的IP是<strong>192.168.33.18</strong>；另一台叫做<strong>client</strong>，它的IP是<strong>192.168.33.19</strong>。Registry配上界面会比较耗内存，所以我们给它1G内存，默认是512M。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"registry"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"registry"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line">  host.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> |v|</span><br><span class="line">    v.memory = <span class="number">1024</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"client"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"client"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后分别在两个终端运行以下命令启动并连接两台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh registry</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh client</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>启动一个registry是很容易的：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">5000</span>:<span class="number">5000</span> \</span><br><span class="line">    --name registry \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /var/lib/registry:/var/lib/registry \</span><br><span class="line">    registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>这里指定了一个<code>/var/lib/registry</code>的卷，是为了把真实的镜像数据储存在主机上，而别在容器挂掉之后丢失数据。就算这样，也还是不保险。要是主机挂了呢？Docker官方建议可以放到<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/rados.md" target="_blank" rel="external">ceph</a>、<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/swift.md" target="_blank" rel="external">swift</a>这样的存储里，或是<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/s3.md" target="_blank" rel="external">亚马逊S3</a>、<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/azure.md" target="_blank" rel="external">微软Azure</a>、<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/gcs.md" target="_blank" rel="external">谷歌GCS</a>、<a href="https://github.com/docker/distribution/blob/master/docs/storage-drivers/oss.md" target="_blank" rel="external">阿里云OSS</a>之类的云商那里。Docker registry提供了配置文件，可以从容器里复制出来查看：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker cp registry:/etc/docker/registry/config.yml config.yml</span><br><span class="line">cat config.yml</span><br></pre></td></tr></table></figure></p>
<p>配置文件里有一个<code>storage</code>，按照<a href="https://github.com/docker/distribution/blob/master/docs/configuration.md#storage" target="_blank" rel="external">这里</a>写的配置，然后执行以下命令重新挂载这个文件来启动registry就可以了，有条件的话可以去试一试：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker rm -fv registry</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">5000</span>:<span class="number">5000</span> \</span><br><span class="line">    --name registry \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /var/lib/registry:/var/lib/registry \</span><br><span class="line">    -v `<span class="built_in">pwd</span>`/config.yml:/etc/docker/registry/config.yml \</span><br><span class="line">    registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>Docker Registry配置完了，我们在client上传一个镜像试试：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker pull busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br><span class="line">docker tag busybox:<span class="number">1.24</span>.<span class="number">1</span> <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>结果push的时候就挂了。原来是我们没有配置认证信息，所以这是一个“不安全”的registry。Docker要求在docker daemon的启动参数里增加<code>--insecure-registry</code>，才能允许我们上传镜像：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"--insecure-registry 192.168.33.18:5000\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>这回就没问题啦。同样地在registry端也配置一下，然后把registry:2.3.0这个镜像上传：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"--insecure-registry 192.168.33.18:5000\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br><span class="line">docker tag registry:<span class="number">2.3</span>.<span class="number">0</span> <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/library/registry:<span class="number">2.3</span>.<span class="number">0</span></span><br><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/library/registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>如果是没有用户的镜像（通常是官方镜像），打标签和上传都需要加一个<code>library/</code>。客户端必须再配置一个参数<code>--registry-mirror</code>才能在我们自己的私有registry里下载镜像：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'$d'</span> /etc/default/docker</span><br><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"--insecure-registry 192.168.33.18:5000 --registry-mirror http://192.168.33.18:5000\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br><span class="line">docker pull registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>应该有飞一般的感觉了吧。如果镜像不在registry里，客户端会自动去docker hub下载。但是每次打标签再上传岂不是很麻烦？所幸docker提供了一个proxy的功能。只要在<code>config.yml</code>里增加如下配置，重启registry容器即可。这样，客户端pull的镜像，也会自动同步到registry里去。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proxy:&#10;  remoteurl: https://registry-1.docker.io</span><br></pre></td></tr></table></figure></p>
<h2 id="u754C_u9762"><a href="#u754C_u9762" class="headerlink" title="界面"></a>界面</h2><p>Docker官方只提供了REST API，并没有给我们一个界面。好在有热心人士出马，所以我们只需执行以下命令就可以给我们的私有库提供一个UI了：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">8080</span>:<span class="number">8080</span> \</span><br><span class="line">    --name web \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_HOST=<span class="number">172.17</span>.<span class="number">0.1</span> \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_PORT=<span class="number">5000</span>\</span><br><span class="line">    hyper/docker-registry-web</span><br></pre></td></tr></table></figure></p>
<p>然后打开<code>http://192.168.33.18:8080</code>，应该就能看到如下界面：<br><img src="/img/docker-registry-web.png" alt=""></p>
<p>上面是个简易版，如果有更深入的需求，可以尝试SUSE的<a href="http://port.us.org/" target="_blank" rel="external">Portus</a>。除了界面以外，它还提供了更细粒度的权限控制、用户认证等功能。</p>
<h2 id="u8BA4_u8BC1"><a href="#u8BA4_u8BC1" class="headerlink" title="认证"></a>认证</h2><p>我们刚刚配好的insecure registry是不支持认证的，如果要上产品环境，找CA申请一个证书吧。我们自己测试的话，可以用自签名证书。我们准备使用IP代替域名，所以需要在证书里面包含我们的IP：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /certs</span><br><span class="line">sudo sed -i <span class="string">'/^\[ v3_ca \]$/a subjectAltName = IP:192.168.33.18'</span> /etc/ssl/openssl.cnf</span><br><span class="line">sudo sh -c <span class="string">"openssl req \</span><br><span class="line">    -newkey rsa:4096 -nodes -sha256 -keyout /certs/domain.key \</span><br><span class="line">    -x509 -days 365 -out /certs/domain.crt"</span></span><br></pre></td></tr></table></figure></p>
<p>随便填点值完成这繁琐的流程，就能看见certs里面多了两个文件。现在可以用以下命令来启动registry：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> registry</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">5000</span>:<span class="number">5000</span> \</span><br><span class="line">    --name registry \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /var/lib/registry:/var/lib/registry \</span><br><span class="line">    -v /certs:/certs \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_HTTP_TLS_KEY=/certs/domain.key \</span><br><span class="line">    registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>客户端现在就不需要<code>--insecure-registry</code>了，但是由于这是自签名证书，客户端还需要把证书文件复制过去：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">'$d'</span> /etc/default/docker</span><br><span class="line">sudo mkdir -p /etc/docker/certs.d/<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/</span><br><span class="line">sudo scp vagrant@<span class="number">192.168</span>.<span class="number">33.18</span>:/certs/domain.crt /etc/docker/certs.d/<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/ca.crt</span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<p>注意vagrant的默认密码也是vagrant。现在push就没有问题了：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>提示镜像已经存在，并没有阻止我们提交。接下来我们加上认证。首先在registry生成用户名hello和密码world：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /auth</span><br><span class="line">sudo sh -c <span class="string">"docker run --entrypoint htpasswd registry:2.3.0 -Bbn hello world &gt; /auth/htpasswd"</span></span><br></pre></td></tr></table></figure></p>
<p>还得指定认证方式和认证文件等参数，重新启动registry容器：<br><figure class="highlight sh"><figcaption><span>registry</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> registry</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">5000</span>:<span class="number">5000</span> \</span><br><span class="line">    --name registry \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /var/lib/registry:/var/lib/registry \</span><br><span class="line">    -v /auth:/auth \</span><br><span class="line">    -v /certs:/certs \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_HTTP_TLS_KEY=/certs/domain.key \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_AUTH=htpasswd \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_AUTH_HTPASSWD_REALM=<span class="string">"Registry Realm"</span> \</span><br><span class="line">    <span class="operator">-e</span> REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \</span><br><span class="line">    registry:<span class="number">2.3</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>这回客户端用<code>docker push 192.168.33.18:5000/busybox:1.24.1</code>来尝试push就会失败啦。但是我们可以用用户名hello和密码world登录啦：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u hello -p world <span class="operator">-e</span> email_whatever <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span></span><br></pre></td></tr></table></figure></p>
<p>再次push，就没有问题了：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push <span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">5000</span>/busybox:<span class="number">1.24</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[轻松搭建nfs存储环境]]></title>
      <url>http://qinghua.github.io/nfs-demo/</url>
      <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Network_File_System" target="_blank" rel="external">NFS</a>是1984年SUN公司开发的一套网络文件系统。它能够让用户像在本机一样操作远程文件。本文用一个比较简单的方式来搭建NFS试验环境。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init ubuntu/trusty64</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;ubuntu/trusty64&quot;</code>，在它的下面添加如下几行代码，相当于给它分配两台虚拟机，一台叫做<strong>server</strong>，它的IP是<strong>192.168.33.17</strong>；另外一台叫做<strong>client</strong>，它的IP是<strong>192.168.33.18</strong>。我们将会在<strong>server</strong>上启动nfs服务并共享一个目录，然后在client挂载并操作这个目录。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"server"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"server"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"client"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"client"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>分别在两个终端运行以下命令启动并连接两台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh server</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh client</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>启动nfs服务需要nfs-kernel-server这个包，让我们安装并启动：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y nfs-kernel-server</span><br><span class="line">sudo service nfs-kernel-server start</span><br></pre></td></tr></table></figure></p>
<p>服务启动完成后，会生成一个<code>/etc/exports</code>文件，里面就是nfs的配置。我们创建一个文件夹<code>/tmp/nfs</code>，把它配置成nfs的目录，然后重新启动nfs服务：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /tmp/nfs</span><br><span class="line">sudo sh -c <span class="string">"echo '/tmp/nfs *(rw,no_subtree_check,no_root_squash)' &gt;&gt; /etc/exports"</span></span><br><span class="line">sudo service nfs-kernel-server start</span><br></pre></td></tr></table></figure></p>
<p>往<code>/etc/exports</code>里写的那一行里，<code>*</code>代表允许所有的客户端访问。如果只想让192.168.33.18访问，把星号改为这个IP就好了。需要支持多个IP的话，用逗号分隔就行。括号里的<code>rw</code>表示允许读写。<code>no_subtree_check</code>表示在服务器端不检查此nfs文件夹的父目录权限。<code>no_root_squash</code>表示客户端root用户也被视为对此nfs文件夹有全部权限的服务端root用户，生产环境不建议使用，因为那样nfs上的文件就可能被随意篡改。想要了解更多的设置，请自行<code>man exports</code>。</p>
<h2 id="u6D4B_u8BD5"><a href="#u6D4B_u8BD5" class="headerlink" title="测试"></a>测试</h2><p>这时候我们就可以去client挂载server的这个nfs目录了：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t nfs <span class="number">192.168</span>.<span class="number">33.17</span>:/tmp/nfs /mnt</span><br><span class="line">ls /mnt</span><br></pre></td></tr></table></figure></p>
<p>往里面写一个文件：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Hello World!"</span> &gt; /mnt/hw.txt</span><br><span class="line">ll /mnt</span><br></pre></td></tr></table></figure></p>
<p>然后去server端看一下，是不是已经被同步过来了：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll /tmp/nfs</span><br></pre></td></tr></table></figure></p>
<p>Server端也可以往nfs的文件夹里写文件：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Hello NFS"</span> &gt; /tmp/nfs/hn.txt</span><br></pre></td></tr></table></figure></p>
<p>照样可以同步到client端：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll /mnt</span><br></pre></td></tr></table></figure></p>
<p>无论server端还是client端，都可以通过下面这个命令来查看<code>192.168.33.17</code>上公开了哪些nfs文件夹：<br><figure class="highlight sh"><figcaption><span>server or client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">showmount <span class="operator">-e</span> <span class="number">192.168</span>.<span class="number">33.17</span></span><br></pre></td></tr></table></figure></p>
<p>再来一个nfs2文件夹：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /tmp/nfs2</span><br><span class="line">sudo sh -c <span class="string">"echo '/tmp/nfs2 *(rw,no_subtree_check,no_root_squash)' &gt;&gt; /etc/exports"</span></span><br></pre></td></tr></table></figure></p>
<p>这时候再<code>showmount -e 192.168.33.17</code>看一下，<code>/tmp/nfs2</code>并没有出现在列表中。因为nfs并不会去监视<code>/etc/exports</code>这个文件，所以这时候它还不知道我们改变了配置。虽然重新启动nfs服务可以解决这个问题，但是它会中断现存的nfs服务。建议使用以下命令：<br><figure class="highlight sh"><figcaption><span>server</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo exportfs -ar    <span class="comment"># a代表所有的文件夹，r代表重新导出</span></span><br></pre></td></tr></table></figure></p>
<p>这时候在再<code>showmount -e 192.168.33.17</code>看一下，<code>/tmp/nfs2</code>已经出来啦。加载一下：<br><figure class="highlight sh"><figcaption><span>client</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo umount /mnt</span><br><span class="line">sudo mount -t nfs <span class="number">192.168</span>.<span class="number">33.17</span>:/tmp/nfs2 /mnt</span><br><span class="line">ls /mnt</span><br></pre></td></tr></table></figure></p>
<p>完全没问题。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如果有10000台机器，你想怎么玩？（八）网络]]></title>
      <url>http://qinghua.github.io/kubernetes-in-mesos-8/</url>
      <content type="html"><![CDATA[<p>这次聊聊docker、k8s、mesos+k8s的网络，了解一下容器、pod和服务间是怎样通信的。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a><a id="more"></a>
</li>
</ul>
<h2 id="Docker_u7684_u7F51_u7EDC_u6A21_u578B"><a href="#Docker_u7684_u7F51_u7EDC_u6A21_u578B" class="headerlink" title="Docker的网络模型"></a>Docker的网络模型</h2><p>Docker默认使用<a href="/docker-bridge-network">桥接网络</a>。Docker daemon启动时，会在主机上启动一个名为docker0的网桥接口。当容器启动的时候，自动分配一对VETH设备，一个连接到docker0上，另一个连接到容器内部的eth0里，于是同一台主机上的容器便能够跨越网络的命名空间，经由主机相互通信。可是不同的主机上的容器就不是那么简单的了。有一种办法是把容器的端口映射到主机的某个端口上去，这样其他主机上的容器可以通过访问这个主机端口的方式实现跨主机通信。Docker 1.6版本之后发布了一个<a href="/docker-overlay-network">覆盖网络</a>overlay network。当使用这个覆盖网络的时候，它便能实现容器的跨主机通信和隔离。</p>
<h2 id="Kubernetes_u7684_u7F51_u7EDC_u6A21_u578B"><a href="#Kubernetes_u7684_u7F51_u7EDC_u6A21_u578B" class="headerlink" title="Kubernetes的网络模型"></a><a href="http://kubernetes.io/docs/admin/networking/" target="_blank" rel="external">Kubernetes的网络模型</a></h2><p>Kubernetes把每个pod当成是一个节点，在这个pod内的所有容器的网络命名空间是共享的，也就意味着它们共享着一个IP地址。Pod内的容器可以用localhost相互通信，与其他容器通信时，直接使用其他pod的IP地址就可以了。如果把每个pod当成一个虚拟机，这样的设计方式是再正常不过的了。用户无需再去考虑pod间的通信问题，也不用考虑pod和主机端口映射的问题了。从这样的易用性出发，kubernetes对网络有三个要求：</p>
<ul>
<li>所有的容器可以在不使用NAT的情况下相互通信</li>
<li>所有的主机和容器可以在不使用NAT的情况下相互通信</li>
<li>容器自己的IP和外部看它的IP是一样的</li>
</ul>
<p>Kubernetes项目启动的时候，docker还只提供了桥接网络。所以单纯地安装docker和kubernetes并不能够满足kubernetes对网络的要求。常见的公有云如GCE、AWS的基础设施都是默认满足网络要求的。私有云的话，一个方法是直接路由，也就是在所有主机的路由表增加其他主机的docker0网桥。但是，增删主机时所有节点都需要重新配置，非常麻烦。另一种方法使用覆盖网络来实现容器跨主机互通，操作相对容易一些。有不少人使用<a href="https://github.com/coreos/flannel#flannel" target="_blank" rel="external">Flannel</a>，也有人使用<a href="http://kubernetes.io/docs/admin/ovs-networking/" target="_blank" rel="external">Open vSwitch</a>、<a href="https://github.com/zettio/weave" target="_blank" rel="external">Weave</a>、<a href="https://github.com/Metaswitch/calico" target="_blank" rel="external">Calico</a>。就flannel来说，它的网络传输如下图：<br><img src="https://raw.githubusercontent.com/coreos/flannel/master/packet-01.png" alt=""></p>
<p>Flannel会在每个主机上运行一个叫做flanneld的代理，它通过etcd保证所有主机上的容器都不会出现重复IP，并能通过物理网络将数据包投递到目标节点的flanneld去。有兴趣的话可以参考<a href="http://dockone.io/article/618" target="_blank" rel="external">《一篇文章带你了解Flannel》</a>。大部分通过覆盖网络实现跨主机容器互通的方案是工作在L2层，在性能上是有些损耗的，但是Calico略微有点不一样。它直接工作在L3层，没有封包解包的损耗，所以性能上影响很小。所付出的代价就是它仅能支持TCP、UDP、ICMP等协议，不过通常来说也足够了。Open vSwitch功能强大，但是配置也比较麻烦。</p>
<p>虽然docker 1.9版正式宣布内置的覆盖网络可以使用在产品环境了，但是kubernetes并不打算支持docker的覆盖网络，据说技术上最主要的原因是kubernetes并不仅仅是为docker这一种容器技术服务的，kubernetes既不想再引入一个键值存储，也不愿意把自己的键值存储暴露给docker用。当然如果用户自己把docker需要的键值存储管理起来，docker自己的覆盖网络还是能够工作的。非技术上的原因是kubernetes认为docker不够开放，都是因为利益啊。详情可以参考<a href="http://blog.kubernetes.io/2016/01/why-Kubernetes-doesnt-use-libnetwork.html" target="_blank" rel="external">这篇文章</a>。</p>
<h2 id="Kubernetes_u7684_u670D_u52A1"><a href="#Kubernetes_u7684_u670D_u52A1" class="headerlink" title="Kubernetes的服务"></a><a href="http://kubernetes.io/docs/user-guide/services/" target="_blank" rel="external">Kubernetes的服务</a></h2><p>Kubernetes是怎么做到一个pod内的容器只有一个IP地址的呢？假如我们启动一个包含俩容器的pod，然后到启动pod的那台虚拟机查看，就会发现除了这两个容器之外，kubernetes另外还启动了一个叫做<code>gcr.io/google_containers/pause</code>的容器。Pod自身的两个容器都是通过在<code>docker run</code>时，net指定使用pause容器网络的办法，把自己需要的端口由pause容器暴露出来的。如下图所示：<br><img src="/img/pause.png" alt=""></p>
<p>当我们通过replication controller，把pod暴露为服务的时候，kubernetes会通过etcd，为这个服务分配一个唯一的虚拟IP。这样做的好处是：避免了用户自己定义的端口有可能重复的问题。每个kube-proxy都会往iptables里写一条关于service虚拟IP的规则。当内部用户使用这个服务的时候，这个虚拟IP便会把流量导入到kube-proxy监听的某个端口上，由kube-proxy使用轮询或基于客户端IP的会话保持的方式，决定最终来提供服务的pod。外部用户由于没有kube-proxy，是不能访问这个服务的，除非我们把服务通过负载均衡或者NodePort的方式暴露出去，本文就不再赘述了。</p>
<h2 id="Kubernetes-Mesos_u7684_u7F51_u7EDC_u6A21_u578B"><a href="#Kubernetes-Mesos_u7684_u7F51_u7EDC_u6A21_u578B" class="headerlink" title="Kubernetes-Mesos的网络模型"></a><a href="https://github.com/kubernetes/kubernetes/blob/master/contrib/mesos/docs/issues.md#user-content-endpoints" target="_blank" rel="external">Kubernetes-Mesos的网络模型</a></h2><p>Mesos使用最基本的Docker网络模型，也就是主机内共享的桥接网络。这点跟kubernetes的要求是矛盾的。所以当一个pod的端点（endpoint），也就是以<strong>pod的IP:端口</strong>的格式暴露出来的时候，由于pod的IP并不能被其他主机所访问，就会导致通信出问题。于是Kubernetes-Mesos开发组想了一个权宜之计：把端点以<strong>主机IP:端口</strong>的格式暴露出来。由于主机的IP是在集群内是都能访问的，所以只要开放端口，通信就能正常工作。如果用户没有指定主机的端口，那就随机分配一个。这个策略是可以通过指定scheduler和controller-manager的启动参数<code>-host-port-endpoints=false</code>（默认为true）来绕过去的，也就是能恢复到kubernetes默认的网络方案去，不过我们就得自己来想办法实现容器的跨主机通信了。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Docker的覆盖网络是怎么工作的]]></title>
      <url>http://qinghua.github.io/docker-overlay-network/</url>
      <content type="html"><![CDATA[<p>我们都知道docker支持多种网络，覆盖网络overlay顾名思义，就是指建立在另一个网络上的网络。比如p2p技术，就是建立在因特网上的覆盖网络。而因特网，原来也是建立在电话网络上的覆盖网络，但现在，电话网络更像是建立在因特网上的覆盖网络（引自<a href="https://en.wikipedia.org/wiki/Overlay_network" target="_blank" rel="external">维基百科</a>）。Docker使用覆盖网络可以解决容器跨主机互通和隔离的问题。在覆盖网络这个领域里，除了docker的overlay network以外，Flannel、Calico、Weave等也都是干这个的。<br><a id="more"></a></p>
<h2 id="u80CC_u666F_u4ECB_u7ECD"><a href="#u80CC_u666F_u4ECB_u7ECD" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>如果把容器的网络驱动设置成overlay，就意味着所有的容器都可以直接ping通，所以所有容器的ip地址都不会重复。为了做到这一点，就需要一个轻量级的存储来存放已经分配出去的ip信息，和其它的一些配置信息。技术上Docker使用了<a href="https://github.com/docker/libkv" target="_blank" rel="external">libkv</a>和<a href="https://github.com/docker/libnetwork" target="_blank" rel="external">libnetwork</a>来实现自己的覆盖网络。它们都是用go语言所写，前者是对操作分布式键值存储系统如consul，etcd，zookeeper的抽象层，后者实现了容器的网络连接，它的设计如下图：<br><img src="https://raw.githubusercontent.com/docker/libnetwork/master/docs/cnm-model.jpg" alt=""></p>
<p>容器网络模型由三部分组成：</p>
<ul>
<li>Sandbox：保存着容器的网络配置，一个Sandbox里可以有多个Endpoint</li>
<li>Endpoint：可以通过Network和其他的Endpoint通信，每个Endpoint都只在一个网络里</li>
<li>Network：由数个可以相互通信的Endpoint组成，不同的网络相互隔离</li>
</ul>
<p>基于这个网络模型，在同一个网络里的所有容器，都是可以相互通信的。如果容器需要隔离，创建另一个网络即可。通过<a href="/docker-swarm">创建Swarm集群</a>来应用覆盖网络是个很自然的做法，也是docker官方的推荐。而kubernetes并不打算支持docker的覆盖网络，据说技术上最主要的原因是kubernetes并不仅仅是为docker这一种容器技术服务的，kubernetes既不想再引入一个键值存储，也不愿意把自己的键值存储暴露给docker用。当然如果用户自己把docker需要的键值存储管理起来，docker自己的覆盖网络还是能够工作的。非技术上的原因是kubernetes认为docker不够开放，都是因为利益啊。详情可以参考<a href="http://blog.kubernetes.io/2016/01/why-Kubernetes-doesnt-use-libnetwork.html" target="_blank" rel="external">这篇文章</a>。这里为了不引入更多复杂度，我们不用Swarm，直接使用原生的docker daemon来做实验。</p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配三台虚拟机，一台叫做<strong>conf</strong>，它的IP是<strong>192.168.33.19</strong>；另两台叫做<strong>node1</strong>和<strong>node2</strong>，它们的IP是<strong>192.168.33.17</strong>和<strong>192.168.33.18</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"node1"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"node1"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"node2"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"node2"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"conf"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"conf"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后分别在三个终端运行以下命令启动并连接三台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh node1</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh node2</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh conf</span><br></pre></td></tr></table></figure>
<p>Ubuntu 14.04的内核版本是3.13，而<a href="https://github.com/docker/docker/issues/14145" target="_blank" rel="external">使用docker overlay网络需要Linux内核版本3.16+</a>，所以需要升级内核，建议升级到3.19或以上。因为我们用node1和node2来跑覆盖网络，所以只用升级这两台虚拟机就好了，conf不用管。命令如下：<br><figure class="highlight sh"><figcaption><span>node1 and node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install linux-generic-lts-vivid</span><br><span class="line"></span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure></p>
<p>等待重启之后，重新连接进vagrant虚拟机：<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh node1</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh node2</span><br></pre></td></tr></table></figure>
<p>完成之后再用<code>uname -r</code>看一下，现在应该已经是3.19了。</p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>如上所述，docker的overlay网络支持consul，etcd，zookeeper等键值存储系统。这里我们使用比较轻量点的etcd，整个镜像不到15Mb。运行以下命令启动etcd：<br><figure class="highlight sh"><figcaption><span>conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --net=host \</span><br><span class="line">  --name=etcd \</span><br><span class="line">  kubernetes/etcd:<span class="number">2.0</span>.<span class="number">5</span> \</span><br><span class="line">  /usr/<span class="built_in">local</span>/bin/etcd \</span><br><span class="line">  --addr=<span class="number">192.168</span>.<span class="number">33.19</span>:<span class="number">4001</span> \</span><br><span class="line">  --bind-addr=<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">4001</span> \</span><br><span class="line">  --data-dir=/var/etcd/data</span><br></pre></td></tr></table></figure></p>
<p>在node上，docker daemon启动参数里需要支持TCP，指定键值存储等。在node1和node2上分别运行以下命令：<br><figure class="highlight sh"><figcaption><span>node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store=etcd://192.168.33.19:4001 --cluster-advertise=192.168.33.17:2375\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store=etcd://192.168.33.19:4001 --cluster-advertise=192.168.33.18:2375\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure>
<p>有了这些，就可以开始创建overlay网络了。</p>
<h2 id="u8FD0_u884C_u5BB9_u5668"><a href="#u8FD0_u884C_u5BB9_u5668" class="headerlink" title="运行容器"></a>运行容器</h2><p>现在在node1或者node2上创建两个overlay网络backend和frontend：<br><figure class="highlight sh"><figcaption><span>node1 or node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker network create --driver overlay --subnet=<span class="number">10.0</span>.<span class="number">7.0</span>/<span class="number">24</span> backend</span><br><span class="line">docker network create --driver overlay --subnet=<span class="number">10.0</span>.<span class="number">8.0</span>/<span class="number">24</span> frontend</span><br><span class="line">docker network ls</span><br></pre></td></tr></table></figure></p>
<p>所有的网络在节点上是分享的，不管在哪个node上运行<code>docker network ls</code>，都能看到全部网络。另外，墙裂建议创建覆盖网络的时候使用<code>--subnet</code>，否则，docker将随机分配一个子网段，虽然docker内部是不会出现重复IP的，但是跟docker外部的其他虚拟机、容器等就不能保证不重复了。</p>
<p>现在在node1上分别用两个网络各自创建一个容器：<br><figure class="highlight sh"><figcaption><span>node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=backend --name=be1 busybox sleep <span class="number">3600</span></span><br><span class="line">docker run <span class="operator">-d</span> --net=frontend --name=fe1 busybox sleep <span class="number">3600</span></span><br><span class="line">docker <span class="built_in">exec</span> be1 ifconfig</span><br><span class="line">docker <span class="built_in">exec</span> fe1 ifconfig</span><br></pre></td></tr></table></figure></p>
<p>执行上面的<code>ifconfig</code>可以看到，eth0的IP地址确实是在指定的<code>subnet</code>里的。然后在node2上用backend网络创建一个容器：<br><figure class="highlight sh"><figcaption><span>node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=backend --name=be2 busybox sleep <span class="number">3600</span></span><br><span class="line">docker <span class="built_in">exec</span> be2 ifconfig</span><br></pre></td></tr></table></figure></p>
<p>容器建好以后，我们可以<code>ping</code>一下看看：<br><figure class="highlight sh"><figcaption><span>node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> be2 ping -c <span class="number">4</span> be1</span><br><span class="line">docker <span class="built_in">exec</span> be2 ping -c <span class="number">4</span> fe1</span><br></pre></td></tr></table></figure></p>
<p>上面命令<code>ping</code>里的主机名可以换成IP。运行完可以看到，node2上是可以访问node1上的相同overlay网络的，但是不同的overlay网络不能访问。在node1运行<code>docker exec be1 ping -c 4 fe1</code>也能知道，即使主机相同，不同的overlay网络也是不能访问的，这就很好地实现了网络隔离。原来默认使用bridge的时候，相同主机的所有容器都是可以相互访问的，而不同主机的所有容器都是不能相互访问的。</p>
<p>最后我们再按照上面的原理图中间那个容器那样，创建一个既使用backend网络也使用frontend网络的容器：<br><figure class="highlight sh"><figcaption><span>node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=backend --name=bf2 busybox sleep <span class="number">3600</span></span><br><span class="line">docker network connect frontend bf2</span><br><span class="line">docker <span class="built_in">exec</span> bf2 ifconfig</span><br></pre></td></tr></table></figure></p>
<p>从上面的<code>ifconfig</code>中可以看到，这个新容器既有<code>10.0.7</code>网段的IP地址，又有<code>10.0.8</code>网段的IP地址。网络搭好以后，我们可以<code>ping</code>一下看看，果然它能够和两个覆盖网络里的容器通信：<br><figure class="highlight sh"><figcaption><span>node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> bf2 ping -c <span class="number">4</span> be1</span><br><span class="line">docker <span class="built_in">exec</span> bf2 ping -c <span class="number">4</span> fe1</span><br></pre></td></tr></table></figure></p>
<p>有兴趣的话，我们还可以去conf虚拟机上看看etcd里面的东东：<br><figure class="highlight sh"><figcaption><span>conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it etcd etcdctl ls /docker/nodes                           <span class="comment"># 所有的节点信息</span></span><br><span class="line">docker <span class="built_in">exec</span> -it etcd etcdctl ls /docker/network/v1.<span class="number">0</span>/endpoint           <span class="comment"># 所有的endpoint信息</span></span><br><span class="line">docker <span class="built_in">exec</span> -it etcd etcdctl ls /docker/network/v1.<span class="number">0</span>/overlay/network    <span class="comment"># 所有的overlay网络信息</span></span><br></pre></td></tr></table></figure></p>
<h2 id="u53C2_u8003_u8D44_u6599"><a href="#u53C2_u8003_u8D44_u6599" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://xelatex.github.io/2015/11/15/Battlefield-Calico-Flannel-Weave-and-Docker-Overlay-Network/" target="_blank" rel="external">这篇文章</a>比较了Docker Overlay Network、Flannel、Calico、Weave等4种覆盖网络，作者还有其它文章分别使用了这些覆盖网络。<br>此外，当然还有<a href="https://docs.docker.com/engine/userguide/networking/dockernetworks/#an-overlay-network" target="_blank" rel="external">Docker官方文档</a>和<a href="https://docs.docker.com/engine/userguide/networking/get-started-overlay/" target="_blank" rel="external">新手教程</a>。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[看例子学awk]]></title>
      <url>http://qinghua.github.io/awk/</url>
      <content type="html"><![CDATA[<p>写<a href="/sed">sed</a>不写awk真是对强迫症的我的一种折磨啊。这次重温一下Linux/Unix下另一个也很老（还是比我老）的文本处理神器：awk（名字来源于三个创始人的姓的首字母）。Linux下的<a href="http://www.delorie.com/gnu/docs/gawk/gawk_3.html" target="_blank" rel="external">gawk</a>是awk的GNU实现。要是不经常使用，很容易忘记。可以把本文当成一个例子库，有用的时候来查一下。<br><a id="more"></a></p>
<p>假设我们有一个不合法格式的csv文件如下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;staff.csv</span><br><span class="line">Country Name Age</span><br><span class="line">US Gavo <span class="number">35</span></span><br><span class="line">US Jane <span class="number">21</span></span><br><span class="line">US Bill <span class="number">25</span></span><br><span class="line">China Jimmy <span class="number">42</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<h2 id="u67E5_u8BE2"><a href="#u67E5_u8BE2" class="headerlink" title="查询"></a>查询</h2><h3 id="u67E5_u8BE2_u5217"><a href="#u67E5_u8BE2_u5217" class="headerlink" title="查询列"></a>查询列</h3><p>最基本的用法就是过滤出某列来：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;print $1&#125;'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>其中的大括号表示一个动作，<code>print</code>就是一个打印的动作。<code>$1</code>代表第1列，<code>$0</code>代表所有列。所以以下的命令也很好理解：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;print $3&#125;'</span> staff.csv</span><br><span class="line">awk <span class="string">'&#123;print $0&#125;'</span> staff.csv</span><br><span class="line">awk <span class="string">'&#123;print $1 $2&#125;'</span> staff.csv       <span class="comment"># 中间的字符不管用</span></span><br><span class="line">awk <span class="string">'&#123;print $1" "$1&#125;'</span> staff.csv     <span class="comment"># 加引号才生效</span></span><br><span class="line">awk <span class="string">'&#123;print NR FS NF&#125;'</span> staff.csv    <span class="comment"># NR代表行号，FS代表分隔符，NF代表列数</span></span><br><span class="line">awk <span class="string">'&#123;print $(NF-1)&#125;'</span> staff.csv     <span class="comment"># 支持计算，加上$代表列内容</span></span><br></pre></td></tr></table></figure></p>
<p>我们可以把这个不合法的csv文件变得合法：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;print $1","$2","$3 &gt; "staff.csv"&#125;'</span> staff.csv</span><br><span class="line">cat staff.csv</span><br></pre></td></tr></table></figure></p>
<p>注意这里awk的语法：写文件是在大括号里面，而不是外面。当然外面也是可以的，只要别和<code>staff.csv</code>重名就好。</p>
<h3 id="u5206_u9694_u7B26"><a href="#u5206_u9694_u7B26" class="headerlink" title="分隔符"></a>分隔符</h3><p>现在再来一次<code>awk &#39;{print $1}&#39; staff.csv</code>，就会发现awk无视逗号，把一整行都当成第一列了。可以用以下命令指定分隔符：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">awk -F<span class="string">","</span> <span class="string">'&#123;print $1,$2&#125;'</span> staff.csv             <span class="comment"># -F必须写在前面，双引号可以省略，变成-F,</span></span><br><span class="line">awk <span class="string">'&#123;print $1,$2&#125;'</span> FS=<span class="string">","</span> staff.csv            <span class="comment"># FS必须写在后面，双引号可以省略，变成FS=,</span></span><br><span class="line">awk <span class="string">'&#123;print $1,$2&#125;'</span> FS=<span class="string">","</span> OFS=<span class="string">":"</span> staff.csv    <span class="comment"># OFS指定输出分隔符</span></span><br></pre></td></tr></table></figure></p>
<p>上面最后一个命令中，虽然OFS指定了输出分隔符，但是需要在<code>$1</code>和<code>$2</code>中间加上这个分隔符才能生效。另外，有时候省略双引号会出错的，比如对于<code>|</code>这个符号来说，有“或者”的意思，可能有歧义，所以还是加上双引号比较稳妥。</p>
<h3 id="u6761_u4EF6"><a href="#u6761_u4EF6" class="headerlink" title="条件"></a>条件</h3><p>awk支持丰富的条件语法以及正则表达式匹配：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'NR==1&#123;print $3&#125;'</span> FS=, staff.csv                     <span class="comment"># 打印第1行第3列</span></span><br><span class="line">awk <span class="string">'NR!=1&#123;print $3&#125;'</span> FS=, staff.csv                     <span class="comment"># 打印所有行的第3列，除了第1行</span></span><br><span class="line">awk <span class="string">'/Gavo/&#123;print $3&#125;'</span> FS=, staff.csv                    <span class="comment"># 打印Gavo的Age</span></span><br><span class="line">awk <span class="string">'/Gavo|Jane/&#123;print $0&#125;'</span> FS=, staff.csv               <span class="comment"># 打印Gavo或Jane的记录</span></span><br><span class="line">awk <span class="string">'$3&gt;40&#123;print $0&#125;'</span> FS=, staff.csv                     <span class="comment"># 打印40岁以上的记录，注意这里表头也按字符串来比较了</span></span><br><span class="line">awk <span class="string">'/Gavo/ || $3&gt;40&#123;print $0&#125;'</span> FS=, staff.csv           <span class="comment"># 打印Gavo或40岁以上的记录</span></span><br><span class="line">awk <span class="string">'$1 ~ /US/&#123;print $0&#125;'</span> FS=, staff.csv                 <span class="comment"># 打印第1列为US的所有记录</span></span><br><span class="line">awk <span class="string">'$3 ~ /^2/&#123;print $0&#125;'</span> FS=, staff.csv                 <span class="comment"># 打印第三列以2开头的所有记录，即所有二十多岁的记录</span></span><br><span class="line">awk <span class="string">'/Gavo/&#123;print $0&#125;/Jane/&#123;print $2&#125;'</span> FS=, staff.csv    <span class="comment"># 打印Gavo的整行记录并打印Jane的第二列</span></span><br></pre></td></tr></table></figure></p>
<p>还支持在动作里写更复杂的条件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;if (NR==1) print $0;&#125;&#123;print $2&#125;'</span> FS=, staff.csv    <span class="comment"># 打印第1行和所有行的第2列</span></span><br><span class="line">awk <span class="string">'c=(NR==1)&#123;print $0&#125; !c&#123;print $2&#125;'</span> FS=, staff.csv    <span class="comment"># 简易版的if else，把条件赋值给变量c</span></span><br><span class="line">awk <span class="string">'(NR==1)&#123;print $0;next&#125;&#123;print $2&#125;'</span> FS=, staff.csv    <span class="comment"># next的意思是跳过后面的命令（print $2）</span></span><br><span class="line">awk <span class="string">'&#123;r=(NR==1)?$0:$2; print r&#125;'</span> FS=, staff.csv          <span class="comment"># 三目赋值运算符</span></span><br></pre></td></tr></table></figure></p>
<p>以上命令的后三条的效果是一样的。下面是大招：条件表达式的这一套全齐了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;if (NR==1) &#123;print $1;&#125; else if (NR==2) &#123;print $2&#125; else &#123;print $3&#125;&#125;'</span> FS=, staff.csv</span><br></pre></td></tr></table></figure></p>
<h2 id="u4F20_u53C2"><a href="#u4F20_u53C2" class="headerlink" title="传参"></a>传参</h2><p>为了命令简单起见，我们再把csv文件换成最早那个但是去掉表头：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;staff.csv</span><br><span class="line">US Gavo <span class="number">35</span></span><br><span class="line">US Jane <span class="number">21</span></span><br><span class="line">US Bill <span class="number">25</span></span><br><span class="line">China Jimmy <span class="number">42</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<h3 id="u5165_u53C2"><a href="#u5165_u53C2" class="headerlink" title="入参"></a>入参</h3><p>过了一年，要把所有人的Age都加上一岁：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;print $3&#125;'</span> staff.csv</span><br><span class="line">awk <span class="string">'&#123;print $3+1&#125;'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>如果这个一岁是个变量，那就这么做：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">age=<span class="number">1</span></span><br><span class="line">awk -v value=<span class="variable">$age</span> <span class="string">'&#123;print $3+value&#125;'</span> staff.csv</span><br><span class="line">awk <span class="string">'&#123;print $3+value&#125;'</span> value=<span class="variable">$age</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>如果这个一岁是个环境变量，那就这么做：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> AGE=<span class="number">1</span></span><br><span class="line">awk <span class="string">'&#123;print $3+ENVIRON["AGE"]&#125;'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<h3 id="u51FA_u53C2"><a href="#u51FA_u53C2" class="headerlink" title="出参"></a>出参</h3><p>如果我们想拿到Jane和Bill的Age，怎么做呢？<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">value=`awk <span class="string">'&#123;if($2=="Jane")print "jane_age="$3;if($2=="Bill")print "bill_age="$3&#125;'</span> staff.csv`</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$value</span></span><br><span class="line"><span class="built_in">eval</span> <span class="variable">$value</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$jane_age</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$bill_age</span></span><br></pre></td></tr></table></figure></p>
<p>这个方案的思路是在<code>awk</code>里拼命令，然后出来执行。</p>
<h2 id="u7EDF_u8BA1"><a href="#u7EDF_u8BA1" class="headerlink" title="统计"></a>统计</h2><h3 id="u6C42_u548C"><a href="#u6C42_u548C" class="headerlink" title="求和"></a>求和</h3><p>求所有人的年龄总和：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;s+=$3&#125;END&#123;print s&#125;'</span> staff.csv</span><br><span class="line">awk <span class="string">'&#123;s+=$3;print $2":"$3&#125;END&#123;print "SUM:"s&#125;'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<h3 id="u6C42_u5E73_u5747"><a href="#u6C42_u5E73_u5747" class="headerlink" title="求平均"></a>求平均</h3><p>下一个命令可以求平均值，也就是求和之后除以行数NR：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;a+=$3&#125;END&#123;print a/NR&#125;'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>如果没有<code>END</code>，awk会在每处理一行之后打印一次。</p>
<h3 id="u53BB_u91CD"><a href="#u53BB_u91CD" class="headerlink" title="去重"></a>去重</h3><p>查看所有的国家，去除重复项目：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;a[$1];&#125;END&#123;for (i in a)print i&#125;'</span> staff.csv</span><br><span class="line">awk <span class="string">'&#123;print $1&#125;'</span> staff.csv | uniq                   <span class="comment"># 用这个多简单</span></span><br></pre></td></tr></table></figure></p>
<p>上面第一个命令比较复杂：<code>a[$1]</code>是awk的数组（其实是字典），<code>a[&quot;US&quot;]=1</code>意味着在a的数组里，<code>&quot;US&quot;</code>的值为1。在这里并没有用到它的值，而是利用了字典的键不能重复的原理。后面有一个<code>for</code>循环，把字典的键都打印出来。如果要打印值，用<code>a[i]</code>就好了。比如下面这个打印每个国家的总年龄：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;a[$1]+=$3;&#125;END&#123;for (i in a)print a[i]&#125;'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<h2 id="u6587_u4EF6_u5904_u7406"><a href="#u6587_u4EF6_u5904_u7406" class="headerlink" title="文件处理"></a>文件处理</h2><h3 id="u5206_u5272_u6587_u4EF6"><a href="#u5206_u5272_u6587_u4EF6" class="headerlink" title="分割文件"></a>分割文件</h3><p>还是以上面那个文件为例：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;staff.csv</span><br><span class="line">US Gavo <span class="number">35</span></span><br><span class="line">US Jane <span class="number">21</span></span><br><span class="line">US Bill <span class="number">25</span></span><br><span class="line">China Jimmy <span class="number">42</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>将不同国家的记录写到不同文件中：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;print &gt; $1".csv"&#125;'</span> staff.csv</span><br><span class="line">ls</span><br><span class="line">cat China.csv</span><br><span class="line">cat US.csv</span><br></pre></td></tr></table></figure></p>
<p>这回看起来好简单啊。<code>print</code>默认打印出整行，所以可以省略<code>$0</code>。<br>每两行写一个文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'NR%2==1&#123;f=++i".csv";&#125;&#123;print &gt; f&#125;'</span> staff.csv</span><br><span class="line">ls</span><br><span class="line">cat <span class="number">1</span>.csv</span><br><span class="line">cat <span class="number">2</span>.csv</span><br></pre></td></tr></table></figure></p>
<p>把多余文件删掉：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm !(staff.csv)</span><br><span class="line">ls</span><br></pre></td></tr></table></figure></p>
<h3 id="u5897_u5220_u6539_u5217"><a href="#u5897_u5220_u6539_u5217" class="headerlink" title="増删改列"></a>増删改列</h3><p>先看下面这个命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;$1=++i FS $1&#125;1'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>它在每行的前面增加了行号。关于<code>$1=++i FS $1</code>，以第一行为例：<code>++i</code>为1，<code>FS</code>为分隔符，<code>$1</code>为第一列，这三项结合起来赋值给前面的<code>$1</code>，所以第一列就变成了<code>1 US</code>。后面的<code>1</code>代表<code>True</code>，是整行打印的意思，也可以用<code>{print $0}</code>来代替。所以：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;$(NF+1)=++i&#125;1'</span> staff.csv                        <span class="comment"># 最后一列加一列</span></span><br><span class="line">awk <span class="string">'&#123;$(NF+1)=++i FS NF&#125;1'</span> staff.csv                  <span class="comment"># 最后一列加两列</span></span><br><span class="line">awk <span class="string">'&#123;$NF=++i FS $NF&#125;1'</span> staff.csv                     <span class="comment"># 倒数第二列加一列</span></span><br><span class="line">awk <span class="string">'&#123;$2=toupper($2)&#125;1'</span> staff.csv                     <span class="comment"># Name列变大写，tolower就是变小写</span></span><br><span class="line">awk <span class="string">'&#123;$2=substr($2,0,3)&#125;1'</span> staff.csv                  <span class="comment"># Name列截前3个字符</span></span><br><span class="line">awk <span class="string">'&#123;$2=""&#125;1'</span> staff.csv                              <span class="comment"># 清空Name列</span></span><br><span class="line">awk <span class="string">'&#123;NF=2&#125;1'</span> staff.csv                               <span class="comment"># 删除最后一列</span></span><br><span class="line">awk <span class="string">'&#123;for(i=1;i&lt;NF;i++)$i=$(i+1);NF=2&#125;1'</span> staff.csv    <span class="comment"># 删除第一列</span></span><br><span class="line">awk <span class="string">'&#123;$2=$2$3;NF=2&#125;1'</span> staff.csv                       <span class="comment"># 合并最后两列</span></span><br></pre></td></tr></table></figure></p>
<h2 id="u53C2_u8003_u8D44_u6599"><a href="#u53C2_u8003_u8D44_u6599" class="headerlink" title="参考资料"></a>参考资料</h2><p>The UNIX School 里的<a href="http://www.theunixschool.com/p/awk-sed.html" target="_blank" rel="external">awk and sed tutorials</a>含有大量的例子和解释，非常容易上手，本文就是以其为基础整理而成。<br>酷壳的<a href="http://coolshell.cn/articles/9070.html" target="_blank" rel="external">AWK 简明教程</a>很适合入门。<br>当然还有最全面的<a href="http://www.gnu.org/software/gawk/manual/gawk.html" target="_blank" rel="external">官方文档</a>。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[看例子学sed]]></title>
      <url>http://qinghua.github.io/sed/</url>
      <content type="html"><![CDATA[<p>这次重温一下Linux/Unix下一个很老（反正比我老）很有用的流编辑器：sed（stream editor）。要是不经常使用，很容易忘记。可以把本文当成一个例子库，有用的时候来查一下。后来还写了篇<a href="/awk">看例子学awk</a>。<br><a id="more"></a></p>
<p>假设我们有一个csv文件如下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;staff.csv</span><br><span class="line">Gavo,<span class="number">35</span></span><br><span class="line">Jane,<span class="number">21</span></span><br><span class="line">Bill,<span class="number">25</span></span><br><span class="line">Jimmy,<span class="number">42</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<h2 id="u589E"><a href="#u589E" class="headerlink" title="增"></a>增</h2><h3 id="u5728_u6587_u4EF6_u5934/_u5C3E_u589E_u52A0_u4E00_u884C"><a href="#u5728_u6587_u4EF6_u5934/_u5C3E_u589E_u52A0_u4E00_u884C" class="headerlink" title="在文件头/尾增加一行"></a>在文件头/尾增加一行</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'1i Name,Age'</span> staff.csv</span><br><span class="line">cat staff.csv</span><br></pre></td></tr></table></figure>
<p>如果没有<code>-i</code>，第一行命令会输出新的文件内容但不会改变<code>staff.csv</code>。<code>1i</code>中的<code>1</code>是指第1行，<code>i</code>是指在读取文件此行前增加（include）记录。如果把<code>i</code>换成<code>a</code>，指的是读取文件此行后增加（append）记录。是不是有点vi的感觉？下面这条命令的结果就会把新行插入到第三行：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'2a Hetty,29'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>要是想在文件尾增加一行的话，用：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'$a Hetty,29'</span> staff.csv    <span class="comment"># 指定行内容</span></span><br><span class="line">sed $<span class="string">'$a \\\n'</span> staff.csv       <span class="comment"># 增加空行</span></span><br></pre></td></tr></table></figure></p>
<p>其中的<code>$</code>指最后一行。如果要在倒数第二行增加一行呢？把<code>a</code>换成<code>i</code>吧。倒数第三行呢？你确认你真的有这么奇葩的需求么…</p>
<h3 id="u5728_u5339_u914D_u7684_u5730_u65B9_u589E_u52A0_u4E00_u884C"><a href="#u5728_u5339_u914D_u7684_u5730_u65B9_u589E_u52A0_u4E00_u884C" class="headerlink" title="在匹配的地方增加一行"></a>在匹配的地方增加一行</h3><p>如果我们要在Jane上面增加一行，这么做：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/Jane/i Hetty,29'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>意思是当匹配到<code>Jane</code>的时候，便做后面的操作。接下来的<code>i</code>不用说了吧，也能替换成<code>a</code>。这里的匹配指的是部分匹配，也能匹配多行。如果需要插入的行以空格开头，就用反斜杠<code>\</code>来转义这个空格。试试下列命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/Jane/i \ \ Hetty,29'</span> staff.csv    <span class="comment"># 行头插入两个空格</span></span><br><span class="line">sed <span class="string">'/21/i Hetty,29'</span> staff.csv          <span class="comment"># 部分匹配</span></span><br><span class="line">sed <span class="string">'/J/i Hetty,29'</span> staff.csv           <span class="comment"># 匹配了两行</span></span><br></pre></td></tr></table></figure></p>
<p>下面这个命令可以在Gavo后面增加两行：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/Gavo/a Hetty,29\nEmma,45'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<h2 id="u5220"><a href="#u5220" class="headerlink" title="删"></a>删</h2><h3 id="u5220_u9664_u7279_u5B9A_u884C"><a href="#u5220_u9664_u7279_u5B9A_u884C" class="headerlink" title="删除特定行"></a>删除特定行</h3><p>删（delete）和<a href="/sed/#u589E">増</a>很相似，区别是把<code>i</code>或<code>a</code>换成<code>d</code>即可：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'1d'</span> staff.csv         <span class="comment"># 删除第一行</span></span><br><span class="line">sed <span class="string">'$d'</span> staff.csv         <span class="comment"># 删除最后一行</span></span><br><span class="line">sed <span class="string">'/Jane/d'</span> staff.csv    <span class="comment"># 删除包含Jane的一行</span></span><br><span class="line">sed <span class="string">'/21/d'</span> staff.csv      <span class="comment"># 删除包含21的一行</span></span><br><span class="line">sed <span class="string">'/J/d'</span> staff.csv       <span class="comment"># 删除包含J的两行</span></span><br></pre></td></tr></table></figure></p>
<h3 id="u5220_u9664_u5173_u8054_u884C"><a href="#u5220_u9664_u5173_u8054_u884C" class="headerlink" title="删除关联行"></a>删除关联行</h3><p>下面这个命令把Gavo这一行和下一行都删掉：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/Gavo/&#123;N;d;&#125;'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>其中的<code>N</code>就是下一行（next line）的意思。如果不想删除Gavo这行，用这个命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/Gavo/&#123;N;s/\n.*//;&#125;'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>相当于匹配了两行也就是<code>Gavo,35\nJane,21</code>之后，再把<code>\n</code>之后的所有文本替换成空白，即删除。替换的命令在下面的<a href="/sed/#u6539">改</a>中会详细介绍。</p>
<h2 id="u6539"><a href="#u6539" class="headerlink" title="改"></a>改</h2><h3 id="u6240_u6709_u884C_u5934/_u5C3E_u589E_u52A0_u9879_u76EE"><a href="#u6240_u6709_u884C_u5934/_u5C3E_u589E_u52A0_u9879_u76EE" class="headerlink" title="所有行头/尾增加项目"></a>所有行头/尾增加项目</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'s/^/China,/'</span> staff.csv</span><br><span class="line">cat staff.csv</span><br></pre></td></tr></table></figure>
<p>其中的<code>s</code>表示替换（substitute），<code>^</code>表示开头，相对应的<code>$</code>表示结尾。</p>
<h3 id="u6240_u6709_u884C_u4FEE_u6539_u9879_u76EE"><a href="#u6240_u6709_u884C_u4FEE_u6539_u9879_u76EE" class="headerlink" title="所有行修改项目"></a>所有行修改项目</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'s/China/US/'</span> staff.csv</span><br><span class="line">cat staff.csv</span><br></pre></td></tr></table></figure>
<p>这样就能把所有的China换成US。如果想把所有的名字后面都加上一个<code>-dev</code>呢？运行：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/,[A-Z][a-z]*/&amp;-dev/'</span> staff.csv</span><br><span class="line">sed -r <span class="string">'s/(,.*),/\1-dev,/'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>其中第一条命令的正则表达式<code>,[A-Z][a-z]*</code>匹配逗号和名字，<code>&amp;</code>表示匹配上的内容，比如对于第二行来说，是<code>,Gavo</code>。第二条命令略微麻烦点，<code>-r</code>表示扩展的正则表达式（extended regular expressions），圆括号表示分组，第一个圆括号中间是第一组，替换的时候用<code>\1</code>表示匹配上的内容。所以<code>\1</code>就是<code>,</code>之前的文本。文件的每一行都有两个逗号，sed会匹配最远的那一个。比如对于第二行来说，匹配到了第二个逗号，所以<code>\1</code>的值就是<code>US,Gavo</code>。加完<code>-dev</code>之后要再补上逗号。所以sed是非常灵活的，可以用多种办法来实现一个功能。</p>
<h3 id="u6B63_u5219_u66FF_u6362"><a href="#u6B63_u5219_u66FF_u6362" class="headerlink" title="正则替换"></a>正则替换</h3><p>给所有的项目都加上引号：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -r <span class="string">'s/[^,]+/"&amp;"/g'</span> staff.csv</span><br><span class="line">sed <span class="operator">-e</span> <span class="string">'s/^\|$/"/g'</span> <span class="operator">-e</span> <span class="string">'s/,/","/g'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>上面的命令是实现的两种方式。第一条命令的意思是除了逗号以外的所有匹配文本都加双引号。对于第二行来说，匹配到了三个文本：<code>US</code>，<code>Gavo</code>和<code>35</code>。第二条命令的思路则完全不同，是先在首尾都加上双引号，然后再把所有的<code>,</code>都替换成<code>&quot;,&quot;</code>。中间的竖线<code>|</code>用反斜杠转义后就是正则表达式中的“或”的意思。</p>
<h3 id="u5168_u5C40_u66FF_u6362"><a href="#u5168_u5C40_u66FF_u6362" class="headerlink" title="全局替换"></a>全局替换</h3><p>把所有的<code>l</code>改成<code>L</code>：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/l/L/g'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>后面的<code>/g</code>代表整行范围内的所有匹配全部替换，不加<code>g</code>的话就会被替换成<code>BiLl</code>。可以换成<code>2</code>只替换第二个匹配项。还可以选择<code>i</code>来忽略大小写，也可以一起用。它们都是正则表达式的范畴。</p>
<p>同时替换<code>l</code>和<code>m</code>，以下两种方式都可以：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/l/L/g; s/m/M/g'</span> staff.csv</span><br><span class="line">sed <span class="operator">-e</span> <span class="string">'s/l/L/g'</span> <span class="operator">-e</span> <span class="string">'s/m/M/g'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<h3 id="u5927_u5C0F_u5199_u66FF_u6362"><a href="#u5927_u5C0F_u5199_u66FF_u6362" class="headerlink" title="大小写替换"></a>大小写替换</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/.*/\L&amp;/'</span> staff.csv</span><br><span class="line">sed <span class="string">'s/.*/\U&amp;/'</span> staff.csv</span><br></pre></td></tr></table></figure>
<p><code>\L</code>就是全部小写（lowercase），<code>\U</code>就是全部大写（uppercase）。<code>&amp;</code>在上文有提到，表示匹配上的内容。</p>
<h3 id="u4FEE_u6539_u6307_u5B9A_u884C"><a href="#u4FEE_u6539_u6307_u5B9A_u884C" class="headerlink" title="修改指定行"></a>修改指定行</h3><p>现在表头的第一列也成了US，把它改成Country：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'1s/US/Country/'</span> staff.csv</span><br><span class="line">cat staff.csv</span><br></pre></td></tr></table></figure></p>
<p>把第2行到4行的US替换成China：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'2,4s/US/China/'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>把第3行整行替换掉：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'3s/.*/China,Hetty,29/'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<h3 id="u5220_u9664_u6240_u6709_u7B26_u53F7"><a href="#u5220_u9664_u6240_u6709_u7B26_u53F7" class="headerlink" title="删除所有符号"></a>删除所有符号</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/[[:punct:]]//g'</span> staff.csv</span><br></pre></td></tr></table></figure>
<p><code>[[:punct:]]</code>是正则表达式中预先定义的子字符类（character classes），代表所有的标点符号。sed支持的<a href="http://www.gnu.org/software/grep/manual/html_node/Character-Classes-and-Bracket-Expressions.html" target="_blank" rel="external">子字符类</a>如下：</p>
<ul>
<li>[:alnum:]：[0-9A-Za-z]</li>
<li>[:alpha:]：[A-Za-z]</li>
<li>[:blank:]：空格和TAB</li>
<li>[:cntrl:]：控制字符（Control characters），ASCII码为000~037和177 (DEL)</li>
<li>[:digit:]：[0-9]</li>
<li>[:graph:]：[:alnum:]和[:punct:]</li>
<li>[:lower:]：[a-z]</li>
<li>[:print:]：[:alnum:]、[:punct:]和空格</li>
<li>[:punct:]：符号 ! “ # $ % &amp; ‘ ( ) * + , - . / : ; &lt; = &gt; ? @ [ \ ] ^ _ ` { | } ~</li>
<li>[:space:]：[:blank:]和回车、换行等</li>
<li>[:upper:]：[A-Z]</li>
<li>[:xdigit:]：16进制 [0-9A-Fa-f]</li>
</ul>
<h2 id="u67E5"><a href="#u67E5" class="headerlink" title="查"></a>查</h2><h3 id="u67E5_u770B_u7279_u5B9A_u884C"><a href="#u67E5_u770B_u7279_u5B9A_u884C" class="headerlink" title="查看特定行"></a>查看特定行</h3><p>查（print）也和<a href="/sed/#u589E">増</a>很类似，区别是把<code>i</code>或<code>a</code>换成<code>p</code>即可：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'1p'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>但是…只是把匹配的行多打一遍而已。如果想要达到<code>grep</code>般的效果，加上<code>-n</code>就可以了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">'1p'</span> staff.csv         <span class="comment"># 查看第一行</span></span><br><span class="line">sed -n <span class="string">'$p'</span> staff.csv         <span class="comment"># 查看最后一行</span></span><br><span class="line">sed -n <span class="string">'/Jane/p'</span> staff.csv    <span class="comment"># 查看包含Jane的一行</span></span><br><span class="line">sed -n <span class="string">'/21/p'</span> staff.csv      <span class="comment"># 查看包含21的一行</span></span><br><span class="line">sed -n <span class="string">'/J/p'</span> staff.csv       <span class="comment"># 查看包含J的两行</span></span><br><span class="line">sed -n <span class="string">'/21$/p'</span> staff.csv     <span class="comment"># 查看以21结尾的一行</span></span><br><span class="line">sed -n <span class="string">'/21/!p'</span> staff.csv     <span class="comment"># 查看包含21以外的其它行</span></span><br></pre></td></tr></table></figure></p>
<p>倒数第二个命令中的<code>21$</code>表示以21结尾。如果要以21开头，用<code>^21</code>。最后一个命令中的<code>!</code>是取反的意思，所以21的记录就反而被隐藏了，而其他的记录倒都显示出来了。</p>
<h3 id="u67E5_u770B_u884C_u8303_u56F4"><a href="#u67E5_u770B_u884C_u8303_u56F4" class="headerlink" title="查看行范围"></a>查看行范围</h3><p>如果想要查看直到匹配某条记录，用下面这条命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/Jane/q'</span> staff.csv</span><br></pre></td></tr></table></figure></p>
<p>其中的<code>q</code>代表查到后退出（quit）。还有几种方式：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">'1,/Jane/p'</span> staff.csv         <span class="comment"># 从第一行开始到匹配Jane的记录为止</span></span><br><span class="line">sed -n <span class="string">'/Gavo/,/Jane/p'</span> staff.csv    <span class="comment"># 从匹配Gavo的记录开始到匹配Jane的记录为止</span></span><br><span class="line">sed -n <span class="string">'/Jane/,$p'</span> staff.csv         <span class="comment"># 从匹配Jane的记录开始到最后一行为止</span></span><br></pre></td></tr></table></figure></p>
<h3 id="u67E5_u770B_u5947/_u5076_u6570_u884C"><a href="#u67E5_u770B_u5947/_u5076_u6570_u884C" class="headerlink" title="查看奇/偶数行"></a>查看奇/偶数行</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'n;d'</span> staff.csv       <span class="comment"># 奇数行</span></span><br><span class="line">sed <span class="string">'1d;n;d'</span> staff.csv    <span class="comment"># 偶数行</span></span><br></pre></td></tr></table></figure>
<p>第一条命令中的<code>n;</code>表示输出当前行并立即读取下一行。第二条命令先把第一行记录删除，于是再输出的奇数行就自然变成原来的偶数行了。</p>
<h3 id="u4ECE_u5355_u884C_u4E2D_u67E5_u627E"><a href="#u4ECE_u5355_u884C_u4E2D_u67E5_u627E" class="headerlink" title="从单行中查找"></a>从单行中查找</h3><p>比如想从<strong>I am 18 years old</strong>里查找18这个年龄，可以这么做：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"I am 18 years old"</span> | sed -n <span class="string">"s/I am \(.*\) years old/\1/p"</span></span><br></pre></td></tr></table></figure></p>
<p>这里<code>\1</code>代表第一个被匹配上的内容也就是<code>\(.*\)</code>。发挥想象力：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"I am 18 years old"</span> | sed -n <span class="string">"s/I am \(.*\) \(.*\) old/\2: \1/p"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="u53C2_u8003_u8D44_u6599"><a href="#u53C2_u8003_u8D44_u6599" class="headerlink" title="参考资料"></a>参考资料</h2><p>The UNIX School 里的<a href="http://www.theunixschool.com/p/awk-sed.html" target="_blank" rel="external">awk and sed tutorials</a>含有大量的例子和解释，非常容易上手，本文就是以其为基础整理而成。<br>酷壳的<a href="http://coolshell.cn/articles/9104.html" target="_blank" rel="external">sed 简明教程</a>很适合入门。<br>当然还有最全面的<a href="http://www.gnu.org/software/sed/manual/sed.html" target="_blank" rel="external">官方文档</a>。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[SaltStack环境安装及入门]]></title>
      <url>http://qinghua.github.io/saltstack/</url>
      <content type="html"><![CDATA[<p><a href="http://saltstack.com/" target="_blank" rel="external">SaltStack</a>简称salt，是一个配置管理工具，类似<a href="http://www.ansible.com/get-started" target="_blank" rel="external">Ansible</a>、<a href="https://www.chef.io/chef/" target="_blank" rel="external">Chef</a>和<a href="https://puppetlabs.com/" target="_blank" rel="external">Puppet</a>，可以用脚本批量操作多台机器。SaltStack运行得很快，可以很容易管理上万台服务器，还有<a href="http://docs.saltstack.cn/zh_CN/latest/" target="_blank" rel="external">部分中文文档</a>。它分为服务器（master）和客户端（minion），服务器也是一个客户端。<a href="http://ohmystack.com/articles/salt-1-basic/" target="_blank" rel="external">Salt (1) 入门</a>是个不错的参考教程。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init ubuntu/trusty64</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;ubuntu/trusty64&quot;</code>，在它的下面添加如下几行代码，相当于给它分配三台虚拟机，一台叫做<strong>master</strong>，它的IP是<strong>192.168.33.17</strong>；另两台叫做<strong>minion1</strong>和<strong>minion2</strong>，它们的IP是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>，其中minion2安装CentOS，其它安装Ubuntu。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"master"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"master"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"minion1"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"minion1"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"minion2"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.box = <span class="string">"bento/centos-7.1"</span></span><br><span class="line">  host.vm.hostname = <span class="string">"minion2"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>改好之后，分别在三个终端运行以下命令启动并连接三台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh master</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh minion1</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh minion2</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>首先在master的虚拟机上安装salt-master和salt-minion，注意master自己也是一个minion：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget -O - https://repo.saltstack.com/apt/ubuntu/<span class="number">14.04</span>/amd64/latest/SALTSTACK-GPG-KEY.pub | sudo apt-key add -</span><br><span class="line">sudo sh -c <span class="string">"echo 'deb http://repo.saltstack.com/apt/ubuntu/14.04/amd64/latest trusty main' &gt;&gt; /etc/apt/sources.list"</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y salt-minion</span><br><span class="line">sudo apt-get install -y salt-master</span><br></pre></td></tr></table></figure></p>
<p>然后在minion上安装salt-minion，这次salt-master就没有必要了，Ubuntu和CentOS的安装方法不太一样：<br><figure class="highlight sh"><figcaption><span>minion1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -O - https://repo.saltstack.com/apt/ubuntu/<span class="number">14.04</span>/amd64/latest/SALTSTACK-GPG-KEY.pub | sudo apt-key add -</span><br><span class="line">sudo sh -c <span class="string">"echo 'deb http://repo.saltstack.com/apt/ubuntu/14.04/amd64/latest trusty main' &gt;&gt; /etc/apt/sources.list"</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y salt-minion</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>minion2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm --import https://repo.saltstack.com/yum/redhat/<span class="number">7</span>/x86_64/latest/SALTSTACK-GPG-KEY.pub</span><br><span class="line">sudo sh -c <span class="string">"cat &lt;&lt; EOF &gt;/etc/yum.repos.d/saltstack.repo</span><br><span class="line">[saltstack-repo]</span><br><span class="line">name=SaltStack repo for RHEL/CentOS 7</span><br><span class="line">baseurl=https://repo.saltstack.com/yum/redhat/7/x86_64/latest</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://repo.saltstack.com/yum/redhat/7/x86_64/latest/SALTSTACK-GPG-KEY.pub</span><br><span class="line">EOF"</span></span><br><span class="line">sudo yum clean expire-cache</span><br><span class="line">sudo yum update</span><br><span class="line">sudo yum install -y salt-minion</span><br></pre></td></tr></table></figure>
<p>默认安装好的minion会自动试图连接到名为salt的master去，所以我们得配置一下，然后重新启动salt-minion服务：<br><figure class="highlight sh"><figcaption><span>master and minion1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">"s/#master: salt/master: 192.168.33.17/"</span> /etc/salt/minion</span><br><span class="line">sudo service salt-minion restart</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>minion2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i <span class="string">"s/#master: salt/master: 192.168.33.17/"</span> /etc/salt/minion</span><br><span class="line">sudo systemctl start salt-minion</span><br></pre></td></tr></table></figure>
<p>随便在哪台虚拟机上运行<code>sudo tail -1 /var/log/salt/minion</code>，如果看到错误消息<strong>The Salt Master has cached the public key for this node</strong>，那就说明前面的安装都是顺利的—这是因为第一次运行的时候，需要建立互信。Salt维护着一个互信列表，在master上运行<code>sudo salt-key</code>可以看到这个表，现在应该是这样子的：<br><strong>Accepted Keys:</strong><br><strong>Denied Keys:</strong><br><strong>Unaccepted Keys:</strong><br>master<br>minion1<br>minion2<br><strong>Rejected Keys:</strong></p>
<p>从上表可以看出，互信列表里的记录有<a href="https://docs.saltstack.com/en/latest/ref/cli/salt-key.html#description" target="_blank" rel="external">四种状态</a>：</p>
<ul>
<li>Unaccepted：待处理</li>
<li>Accepted：互信</li>
<li>Rejected：运维人员运行命令拒绝</li>
<li>Denied：master自动拒绝（比如ID重复等）</li>
</ul>
<p>现在所有的minion包括master自己都是处于Unaccepted的状态。运行以下命令就可以把它们都加入到Accepted：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo salt-key --accept=master --yes</span><br><span class="line">sudo salt-key --accept=minion1 --yes</span><br><span class="line">sudo salt-key --accept=minion2 --yes</span><br></pre></td></tr></table></figure></p>
<p>如果minion太多了，也可以用<code>sudo salt-key --accept-all --yes</code>来全部accept。忽略<code>--yes</code>可以让我们手动确认所有的Unaccepted记录。再次运行<code>sudo salt-key</code>，确认所有的minion都已经加入到Accepted Keys里了，安装步骤就此完成。</p>
<p>小贴士：可以用<code>sudo salt-key --reject=minion2</code>来把minion2加入到Rejected列表中。用<code>sudo salt-key --include-all --accept-all</code>来把Rejected列表中的minion再加到Accepted中来。</p>
<h2 id="u8FD0_u884C_u547D_u4EE4"><a href="#u8FD0_u884C_u547D_u4EE4" class="headerlink" title="运行命令"></a>运行命令</h2><h3 id="salt"><a href="#salt" class="headerlink" title="salt"></a>salt</h3><p>安装完成之后，在master上运行<code>sudo salt &#39;*&#39; test.ping</code>可以看到各minion是否能联通。其中的<strong>*</strong>代表<a href="https://docs.saltstack.com/en/latest/topics/targeting/index.html" target="_blank" rel="external">目标（target）</a>，这里即是所有的minion（master现在也是一个minion），<strong>test.ping</strong>称为<a href="https://docs.saltstack.com/en/latest/ref/modules/" target="_blank" rel="external">执行模块（execution module）</a>，也就是需要在目标上调用的方法。<a href="https://docs.saltstack.com/en/latest/ref/modules/all/index.html" target="_blank" rel="external">这个列表</a>里记载了所有的原生执行模块。我们来尝试一下其中的<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.cmdmod.html#module-salt.modules.cmdmod" target="_blank" rel="external">cmdmod</a>模块：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo salt minion1 cmd.run <span class="string">'ifconfig'</span></span><br></pre></td></tr></table></figure></p>
<p>这就相当于在minion1上直接运行ifconfig了。</p>
<h3 id="salt-call"><a href="#salt-call" class="headerlink" title="salt-call"></a>salt-call</h3><p>Salt还提供了一个<code>salt-call</code>命令，它只能在本机执行，所以无需输入目标。在master上运行<code>sudo salt-call cmd.run &#39;hostname&#39;</code>，效果相当于直接本地运行命令。有所不同的是，它相当于运行在salt的机制上。Salt需要两个端口来运行：通过<a href="http://zeromq.org/" target="_blank" rel="external">ZeroMQ</a>在4505发消息，4506用来接收结果，所有的minion都会订阅4505端口。也就是说，运行这条命令使用了这两个端口。配合上<code>--log-level=debug</code>的参数，使得<code>salt-call</code>非常适用于调试。运行以下命令：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo salt-call --log-level=debug disk.usage</span><br></pre></td></tr></table></figure></p>
<p>从打印出来的调试信息，我们能看到原来<strong>disk.usage</strong>模块用的是<code>df -P</code>命令。</p>
<h3 id="salt-run"><a href="#salt-run" class="headerlink" title="salt-run"></a>salt-run</h3><p>最后再介绍一个<code>salt-run</code>命令。我们简单地试一试在master上运行<code>sudo salt-run manage.up</code>，这个manage.up是个runner，在这里它的作用类似于test.ping模块，也是查看所有minion的状态，但是不需要指定目标即可使用。它一般分为几个步骤，一个步骤内并行执行，步骤之间串行执行。比如先部署好数据库（步骤A）再部署应用服务器（步骤B）。<code>salt-run</code>运行的命令称为runner，<a href="https://docs.saltstack.com/en/latest/ref/runners/all/index.html" target="_blank" rel="external">这个列表</a>里记载了所有的原生runner。</p>
<h2 id="u6307_u5B9A_u76EE_u6807"><a href="#u6307_u5B9A_u76EE_u6807" class="headerlink" title="指定目标"></a>指定目标</h2><p>Salt支持多种方式来指定目标，简单尝试一下就知道啦。<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo salt <span class="string">'minion*'</span> test.ping                      <span class="comment"># 通配符，以"minion"开头的minion</span></span><br><span class="line">sudo salt -L <span class="string">'minion1,minion2'</span> test.ping           <span class="comment"># 列表，minion1和minion2</span></span><br><span class="line">sudo salt -E <span class="string">'minion(1|2)'</span> test.ping               <span class="comment"># 正则表达式，minion1或minion2</span></span><br><span class="line">sudo salt -S <span class="string">'192.168.33.19'</span> test.ping             <span class="comment"># IP，minion2</span></span><br><span class="line">sudo salt -G <span class="string">'os:Ubuntu'</span> test.ping                 <span class="comment"># Grains，操作系统为Ubuntu</span></span><br><span class="line">sudo salt -C <span class="string">'minion* and G@os:Ubuntu'</span> test.ping   <span class="comment"># 组合，以"minion"开头的minion并且操作系统为Ubuntu</span></span><br><span class="line">sudo salt -C <span class="string">'master or G@os:CentOS'</span> test.ping     <span class="comment"># 组合，master或操作系统为CentOS（不区分大小写）</span></span><br><span class="line">sudo salt -I <span class="string">'region:cn'</span> test.ping                 <span class="comment"># Pillar，region为cn的minion</span></span><br></pre></td></tr></table></figure></p>
<p>其中的<a href="https://docs.saltstack.com/en/latest/topics/targeting/grains.html" target="_blank" rel="external">Grains</a>是minion的属性，包含机器名、IP、操作系统、CPU等多种信息。可以运行<code>sudo salt &#39;*&#39; grains.items</code>来查看所有的grains数据。那<a href="https://docs.saltstack.com/en/latest/topics/pillar/index.html" target="_blank" rel="external">Pillar</a>又是什么鬼？简言之，Pillar是存放在master的变量，Grains是minion自己的常量。上面的命令中，Pillar这行应该会报错：<strong>No minions matched the target</strong>，这是因为我们没有在master上加Pillar的缘故。Pillar文件为yaml格式，默认存放在<code>/srv/pillar</code>里。由于pillar只会加密传送给指定的minion，所以可以存放密码等敏感信息。我们来加两个文件：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">"cat &lt;&lt; EOF &gt; /srv/pillar/top.sls</span><br><span class="line">base:</span><br><span class="line">  '*':</span><br><span class="line">    - default</span><br><span class="line">EOF"</span></span><br><span class="line">sudo sh -c <span class="string">"echo 'region: cn' &gt; /srv/pillar/default.sls"</span></span><br></pre></td></tr></table></figure></p>
<p>首先创建的<code>top.sls</code>是一个默认的入口文件，它表示所有minion都适用<code>default.sls</code>文件。而<code>default.sls</code>里指定了变量。运行以下的命令：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo salt <span class="string">'*'</span> pillar.items            <span class="comment"># 生成、分发、查看现在的pillar。用pillar.raw可以仅查看当前值</span></span><br><span class="line">sudo salt -I <span class="string">'region:cn'</span> test.ping    <span class="comment"># 再次运行test.ping，以pillar为目标</span></span><br></pre></td></tr></table></figure></p>
<p>就能看到现在已经不报错了，所有的minion都是目标。</p>
<h2 id="u7EF4_u6301_u72B6_u6001_uFF08State_uFF09"><a href="#u7EF4_u6301_u72B6_u6001_uFF08State_uFF09" class="headerlink" title="维持状态（State）"></a>维持状态（State）</h2><p>Salt提供了<a href="https://docs.saltstack.com/en/latest/topics/tutorials/starting_states.html" target="_blank" rel="external">state</a>（点开一看，设计哲学是“简单，简单，简单”。大家都知道重要的事情说三遍）的方式让我们维持所有minion的状态一致。我们来加两个文件：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">"cat &lt;&lt; EOF &gt; /srv/salt/top.sls</span><br><span class="line">base:</span><br><span class="line">  '*':</span><br><span class="line">    - default</span><br><span class="line">EOF"</span></span><br><span class="line">sudo sh -c <span class="string">"cat &lt;&lt; EOF &gt; /srv/salt/default.sls</span><br><span class="line">/tmp/ggg:</span><br><span class="line">  file.directory:</span><br><span class="line">    - makedirs: True</span><br><span class="line">EOF"</span></span><br></pre></td></tr></table></figure></p>
<p>如同pillar，首先创建的<code>top.sls</code>是一个默认的入口文件，<code>default.sls</code>里指定了这个状态需要有<code>/tmp/ggg</code>这个文件夹。接下来让我们来运行它：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo salt <span class="string">'master'</span> state.show_sls default    <span class="comment"># 检查并显示default的state</span></span><br><span class="line">sudo salt <span class="string">'master'</span> state.sls default         <span class="comment"># 运行default的state</span></span><br><span class="line">ls /tmp                                      <span class="comment"># 能够看到ggg文件夹已经创建好了</span></span><br><span class="line">sudo salt <span class="string">'master'</span> state.sls default         <span class="comment"># 再次运行default的state</span></span><br><span class="line">ls /tmp                                      <span class="comment"># 运行几次都一样，维持状态</span></span><br></pre></td></tr></table></figure></p>
<p>State可以说是salt的核心功能。它通过yaml格式的数据文件sls（<strong>S</strong>a<strong>L</strong>t <strong>S</strong>tate file）确保了一个系统应该是什么状态的。<a href="https://docs.saltstack.com/en/latest/ref/states/all/" target="_blank" rel="external">这个列表</a>里记载了所有可用的状态。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[用容器轻松搭建Bamboo来提供Marathon的对外服务]]></title>
      <url>http://qinghua.github.io/marathon-bamboo/</url>
      <content type="html"><![CDATA[<p>如果我们在marathon上部署了一个tomcat服务并希望它能暴露给外网，应该怎么做呢？<a href="https://github.com/QubitProducts/bamboo" target="_blank" rel="external">Bamboo</a>提供了一个非常方便运行的办法帮我们做到这一点。它集成了HAproxy，当marathon检测到应用挂掉并重启应用时，bamboo能够检测到并更新HAproxy的配置文件，然后自动重启HAproxy，从而无须人工干预便能持续不断地对外提供服务。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>参考《用容器轻松搭建Marathon运行环境》的<a href="/mesos-marathon/#u51C6_u5907_u5DE5_u4F5C">准备工作</a>一节，用vagrant搭建两台虚拟机<strong>master</strong>和<strong>slave</strong>。</p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>参考《用容器轻松搭建Marathon运行环境》的<a href="/mesos-marathon/#u642D_u5EFA_u73AF_u5883">搭建环境</a>一节，但是用下面这个命令来启动marathon。它相比原来的命令多了一个<code>--event_subscriber http_callback</code>的参数，如果不配置，便不能实现HAproxy动态加载服务的功能。<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=ma \</span><br><span class="line">    mesosphere/marathon:v0.<span class="number">15.0</span> \</span><br><span class="line">    --master zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/mesos \</span><br><span class="line">    --zk zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/marathon \</span><br><span class="line">    --event_subscriber http_callback</span><br></pre></td></tr></table></figure></p>
<p>现在启动Bamboo镜像，在里面指定marathon的地址：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    -p <span class="number">8000</span>:<span class="number">8000</span> \</span><br><span class="line">    -p <span class="number">80</span>:<span class="number">80</span> \</span><br><span class="line">    --name=bam \</span><br><span class="line">    <span class="operator">-e</span> MARATHON_ENDPOINT=http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span> \</span><br><span class="line">    <span class="operator">-e</span> BAMBOO_ENDPOINT=http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8000</span> \</span><br><span class="line">    <span class="operator">-e</span> BAMBOO_ZK_HOST=<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span> \</span><br><span class="line">    <span class="operator">-e</span> BAMBOO_ZK_PATH=/bamboo \</span><br><span class="line">    <span class="operator">-e</span> BIND=<span class="string">":8000"</span> \</span><br><span class="line">    <span class="operator">-e</span> CONFIG_PATH=<span class="string">"config/production.example.json"</span> \</span><br><span class="line">    <span class="operator">-e</span> BAMBOO_DOCKER_AUTO_HOST=<span class="literal">true</span> \</span><br><span class="line">    gregory90/bamboo:<span class="number">0.2</span>.<span class="number">11</span></span><br></pre></td></tr></table></figure></p>
<p>在浏览器打开<code>http://192.168.33.18:8000/</code>应该能看到下图：<br><img src="/img/bamboo.png" alt=""></p>
<h2 id="u670D_u52A1_u53D1_u73B0"><a href="#u670D_u52A1_u53D1_u73B0" class="headerlink" title="服务发现"></a>服务发现</h2><p>现在让我们用marathon来启动一个tomcat服务。在任意一台机器上运行以下命令，把创建tomcat服务的请求发送给marathon的REST api：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span>/v2/apps \</span><br><span class="line">    -H <span class="string">"Content-type: application/json"</span> \</span><br><span class="line">    <span class="operator">-d</span> <span class="string">'&#123;"cpus":0.5,"mem":200,"disk":0,"instances":1,"id":"tomcat", </span><br><span class="line">    "container":&#123;"docker":&#123;"image":"tomcat","network":"BRIDGE","portMappings": </span><br><span class="line">    [&#123;"containerPort":8080,"hostPort":0,"servicePort":0,"protocol":"tcp"&#125;]&#125;&#125;&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>分别刷新marathon和bamboo，就能看到它们各自多了个tomcat的服务。点击bamboo页面上的<strong>/tomcat</strong>记录最右边的加号按钮，在<strong>acl</strong>里输入<code>path_beg -i /</code>（表示运行在根目录上，有兴趣的话可以参考HAproxy的<a href="http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#7" target="_blank" rel="external">ACL语法</a>），然后点击<strong>Create</strong>按钮：<br><img src="/img/bamboo-tomcat.png" alt=""></p>
<p>顺利的话，打开<code>http://192.168.33.18/</code>应该能看到tomcat出现啦：<br><img src="/img/tomcat.jpg" alt=""></p>
<p>这个时候，在marathon的页面上点击tomcat这行记录，便会到tomcat application页面里。如下图选中当前的tomcat实例，点击<strong>Kill</strong>按钮：<br><img src="/img/marathon-application-tomcat.png" alt=""></p>
<p>稍等几秒，就会看到tomcat的服务运行地址从<code>192.168.33.19:31071</code>变成了<code>192.168.33.19:31571</code>，端口因机而异。再回去刷新tomcat的<code>http://192.168.33.18/</code>页面，是不是仍然提供服务呢？如果你手快，应该能看到<strong>503 Service Unavailable</strong>，那就多刷新两下 ：） 到slave虚拟机上用命令删除tomcat容器，再观察一下，是不是一样的效果呢？</p>
<h2 id="u5176_u4ED6_u65B9_u6CD5"><a href="#u5176_u4ED6_u65B9_u6CD5" class="headerlink" title="其他方法"></a>其他方法</h2><p>Marathon官方还支持其它<a href="https://mesosphere.github.io/marathon/docs/service-discovery-load-balancing" target="_blank" rel="external">三种服务发现的方法</a>：</p>
<ol>
<li>Mesos-DNS：Mesosphere公司提供的DNS产品，不仅适用于marathon，而且适用于其它Mesos Framework。</li>
<li>Marathon-lb：感觉上跟k8s的<a href="http://kubernetes.io/v1.1/docs/user-guide/services.html#type-nodeport" target="_blank" rel="external">NodePort</a>有点像，不过它像Bamboo那样包含了HAproxy。</li>
<li>haproxy-marathon-bridge：现在已经不推荐了。需要在每个slave上安装HAproxy，定时更新HAproxy的配置文件。</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[轻松搭建Docker Swarm运行环境]]></title>
      <url>http://qinghua.github.io/docker-swarm/</url>
      <content type="html"><![CDATA[<p><a href="https://docs.docker.com/swarm/" target="_blank" rel="external">Docker Swarm</a>是官方发布的集群容器管理工具。它的特点是：比较轻量级，无缝支持标准的docker API。<a href="http://blog.daocloud.io/swarm_analysis_part1/" target="_blank" rel="external">深入浅出Swarm</a>一文很清晰地讲解了它的架构和命令。本文从零开始搭建并管理一个swarm集群。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配三台虚拟机，一台叫做<strong>manager</strong>，它的IP是<strong>192.168.33.17</strong>；另两台叫做<strong>node1</strong>和<strong>node2</strong>，它们的IP是<strong>192.168.33.18</strong>和<strong>192.168.33.19</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"manager"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"manager"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.17"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"node1"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"node1"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"node2"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"node2"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后分别在三个终端运行以下命令启动并连接三台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh manager</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh node1</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh node2</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>想要让swarm管理node，首先得让docker daemon支持TCP。在三台虚拟机上运行以下命令：<br><figure class="highlight sh"><figcaption><span>manager and node1 and node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo rm /etc/docker/key.json    <span class="comment"># 免得我们用vagrant生成的docker id都一样，删掉了重启docker服务会自动生成一个新的</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<p>接下来，我们用最简单的静态节点列表方式来启动swarm环境。把node1和node2的节点信息都写到参数里即可：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -t -p <span class="number">2376</span>:<span class="number">2375</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> manage nodes://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2375</span>,<span class="number">192.168</span>.<span class="number">33.19</span>:<span class="number">2375</span></span><br></pre></td></tr></table></figure></p>
<p>2376是我随便设的一个端口，可以改成2375外的其他可用端口，因为2375已经被docker daemon占用了。可以Ctrl+C后，用下面这个命令查看：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo netstat -tulnp | grep <span class="number">2375</span></span><br></pre></td></tr></table></figure></p>
<p>随便在哪台机器运行以下命令就能看到这个集群的信息和节点信息。这里的2376就是上面随便设出来的2376：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> info</span><br></pre></td></tr></table></figure></p>
<h2 id="u8FD0_u884C_u5BB9_u5668"><a href="#u8FD0_u884C_u5BB9_u5668" class="headerlink" title="运行容器"></a>运行容器</h2><p>Swarm的环境已经搭建完成，现在我们可以用swarm来运行容器了。随便在哪台机器运行以下命令来创建一个busybox容器：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> run <span class="operator">-d</span> busybox sleep <span class="number">3000</span></span><br></pre></td></tr></table></figure></p>
<p>可能一开始的时候有点儿慢，这是因为需要下载镜像的缘故。如果等不及，就到两个node里分别先把镜像下载下来：<code>docker pull busybox</code>。之所以说无缝支持docker API，那是因为swarm里能运行所有的docker命令。跑一下<code>docker -H tcp://192.168.33.17:2376 ps</code>就能看到一个busybox的容器已经启动起来了。容器的<strong>NAMES</strong>属性里有node的信息，所以不需要分别在两个node运行<code>docker ps</code>就能看到这个busybox的容器在哪个node运行。之后，再运行3次上面的命令，共创建4个busybox的容器，就能看到它们被均匀分配到两个node上了。这是因为swarm默认的调度策略所致。目前swarm支持<a href="https://docs.docker.com/swarm/scheduler/strategy/" target="_blank" rel="external">三种调度策略</a>：</p>
<ul>
<li>spread：默认，swarm会把任务分配到目前运行的容器数量最少的node上去。这里说的容器数量包括已经停止的容器。</li>
<li>binpack：把任务分配到目前最大负荷的node上去。目的是把其他机器的资源留给将来可能要运行的大容器。</li>
<li>random：随机分配。</li>
</ul>
<p>如果看到的容器分配不均匀，那很可能是存在着非运行中的容器，可以用<code>docker ps -a</code>看一下。如果想要修改调度策略，可以在manager启动的时候指定<code>--strategy</code>参数，比如修改成binpack：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> `docker ps -aq`</span><br><span class="line">docker run -t -p <span class="number">2376</span>:<span class="number">2375</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> manage nodes://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2375</span>,<span class="number">192.168</span>.<span class="number">33.19</span>:<span class="number">2375</span> --strategy binpack</span><br></pre></td></tr></table></figure></p>
<p>这回再试试启动4个新的busybox，是不是都跑到同一个node上去了？Swarm的过滤器功能还允许我们指定让容器运行在哪个node上。目前，<a href="https://docs.docker.com/swarm/scheduler/filter/" target="_blank" rel="external">swarm支持如下的过滤器</a>：</p>
<ul>
<li>node过滤器<ul>
<li>constraint：限制新任务只能在label符合的node上执行</li>
<li>health：限制新任务只能在“健康”的node上执行</li>
</ul>
</li>
<li>容器过滤器<ul>
<li>affinity：使新任务在已运行某个名字或label的容器，或者有某个镜像的node上执行</li>
<li>dependency：使新任务在有依赖（–volumes-from=dependency、–link=dependency:alias或–net=container:dependency）的node上执行</li>
<li>port：使新任务在某个端口可用的node上执行</li>
</ul>
</li>
</ul>
<p>可以在manager启动的时候指定<code>--filter</code>参数来启用过滤器功能。我们先来试验一下constraint。由于Label是docker daemon的属性，所以我们又要修改<code>/etc/default/docker</code>并重启docker daemon。假设node1为ssd，node2为普通disk：<br><figure class="highlight sh"><figcaption><span>node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> `docker ps -aq`</span><br><span class="line">sudo sed -i <span class="string">'$d'</span> /etc/default/docker    <span class="comment"># 删掉最后一行，因为要加新的label</span></span><br><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --label storage=ssd\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> `docker ps -aq`</span><br><span class="line">sudo sed -i <span class="string">'$d'</span> /etc/default/docker    <span class="comment"># 删掉最后一行，因为要加新的label</span></span><br><span class="line">sudo sh -c <span class="string">'echo DOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --label storage=disk\" &gt;&gt; /etc/default/docker'</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure>
<p>随便在哪台机器运行以下命令，看看两个node是不是分别多出来<code>storage==ssd</code>和<code>storage==disk</code>：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> info</span><br></pre></td></tr></table></figure></p>
<p>如果不是，就重启一下manager的swarm容器。然后就可以指定<code>storage=ssd</code>的node运行：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> run <span class="operator">-d</span> <span class="operator">-e</span> constraint:storage==ssd busybox sleep <span class="number">3000</span></span><br></pre></td></tr></table></figure></p>
<p>随便再创建几个容器玩玩，再换<code>constraint:storage==disk</code>试试看。熟悉之后，我们再试验一下affinity：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> run <span class="operator">-d</span> <span class="operator">-e</span> constraint:storage==disk --name=bb busybox sleep <span class="number">3000</span></span><br></pre></td></tr></table></figure></p>
<p>要的就是<strong>bb</strong>这个名字。然后运行以下命令让新容器运行在有bb容器的node上，也就是node2：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> run <span class="operator">-d</span> <span class="operator">-e</span> affinity:container==bb busybox sleep <span class="number">3000</span></span><br></pre></td></tr></table></figure></p>
<p>在node2上运行<code>docker ps</code>应该能看到新容器已经启动起来了。如果constraint和affinity冲突会怎样呢？试试看：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> run <span class="operator">-d</span>  <span class="operator">-e</span> affinity:container==bb <span class="operator">-e</span> constraint:storage==ssd busybox sleep <span class="number">3000</span></span><br></pre></td></tr></table></figure></p>
<p>不出意外的话，应该能看见<strong>unable to find a node that satisfies storage==ssd</strong>的错误消息了吧。</p>
<h2 id="u4E3B_u673A_u53D1_u73B0"><a href="#u4E3B_u673A_u53D1_u73B0" class="headerlink" title="主机发现"></a>主机发现</h2><p>Swarm共支持下面<a href="https://docs.docker.com/swarm/discovery/" target="_blank" rel="external">几种主机发现方式</a>：</p>
<ul>
<li>分布式键值存储<ul>
<li>Consul 0.5.1或更高版本</li>
<li>Etcd 2.0或更高版本</li>
<li>ZooKeeper 3.4.5或更高版本</li>
</ul>
</li>
<li>静态方式<ul>
<li>文件</li>
<li>节点列表</li>
</ul>
</li>
<li>Docker Hub</li>
</ul>
<p>我们刚才<a href="/docker-swarm/#u642D_u5EFA_u73AF_u5883">搭建环境</a>用到的是静态的节点列表方式，现在我们再试试其它几种方式。</p>
<h3 id="u6587_u4EF6"><a href="#u6587_u4EF6" class="headerlink" title="文件"></a>文件</h3><p>先从简单的开始。文件的主机发现方式和节点列表很类似，它们都是静态的。首先把node的信息都写到一个临时文件<code>/tmp/cluster</code>里，然后让swamp容器管理这个文件即可：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="number">192.168</span>.<span class="number">33</span>.[<span class="number">18</span>:<span class="number">19</span>]:<span class="number">2375</span> &gt; /tmp/cluster</span><br><span class="line">docker run -t -p <span class="number">2376</span>:<span class="number">2375</span> -v /tmp/cluster:/tmp/cluster swarm:<span class="number">1.1</span>.<span class="number">0</span> manage file:///tmp/cluster</span><br></pre></td></tr></table></figure></p>
<p>随便选台虚拟机检查一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> info</span><br></pre></td></tr></table></figure></p>
<p>是不是有换汤不换药的感觉？</p>
<h3 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h3><p>接下来试验一下ZooKeeper。先在manager上启动一个ZooKeeper的服务：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=zk \</span><br><span class="line">    <span class="operator">-e</span> MYID=<span class="number">1</span> \</span><br><span class="line">    <span class="operator">-e</span> SERVERS=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    mesoscloud/zookeeper:<span class="number">3.4</span>.<span class="number">6</span>-ubuntu-<span class="number">14.04</span></span><br></pre></td></tr></table></figure></p>
<p>然后运行swarm manager：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -t -p <span class="number">2376</span>:<span class="number">2375</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> manage zk://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2181</span>/swarm</span><br></pre></td></tr></table></figure></p>
<p>由于这回不像文件和节点列表方式那样静态，我们需要把两个node加入到集群里：<br><figure class="highlight sh"><figcaption><span>node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> join --addr=<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2375</span> zk://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2181</span>/swarm</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> join --addr=<span class="number">192.168</span>.<span class="number">33.19</span>:<span class="number">2375</span> zk://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2181</span>/swarm</span><br></pre></td></tr></table></figure>
<p>随便选台虚拟机检查一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> info    <span class="comment"># 由于是动态加载，可能需要等待半分钟左右才能看见node</span></span><br></pre></td></tr></table></figure></p>
<p>Consul和Etcd也都很类似，这里就不一一列举了。</p>
<h3 id="Docker_Hub"><a href="#Docker_Hub" class="headerlink" title="Docker Hub"></a>Docker Hub</h3><p>Docker Hub的主机发现方式，就是在docker hub上使用发现服务来生成一个唯一的集群ID（别在生产环境上这么干！）。Docker hub会为我们保留大概一个星期。在任意一台机器上运行以下命令：<br><figure class="highlight sh"><figcaption><span>manager or node1 or node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm swarm:<span class="number">1.1</span>.<span class="number">0</span> create</span><br></pre></td></tr></table></figure></p>
<p>然后我们就能看见上面的命令生成了一个字符串，这就是我们的集群ID。在我的机器上是这样的：<code>3137ebf83d771f1db06bf4eab7ccc73b</code>。我大天朝的网络，有时候会出现<strong>TLS handshake timeout</strong>，那就再运行一次吧。</p>
<p>这时候可以把manager启动起来了，别忘了替换成你自己的token：<br><figure class="highlight sh"><figcaption><span>manager</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -t -p <span class="number">2376</span>:<span class="number">2375</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> manage token://<span class="number">3137</span>ebf83d771f1db06bf4eab7ccc73b</span><br></pre></td></tr></table></figure></p>
<p>然后可以在两个node上分别运行以下命令，启动swarm的代理并加入到集群中，别忘了替换成你自己的token：<br><figure class="highlight sh"><figcaption><span>node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> join --addr=<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2375</span> token://<span class="number">3137</span>ebf83d771f1db06bf4eab7ccc73b</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> swarm:<span class="number">1.1</span>.<span class="number">0</span> join --addr=<span class="number">192.168</span>.<span class="number">33.19</span>:<span class="number">2375</span> token://<span class="number">3137</span>ebf83d771f1db06bf4eab7ccc73b</span><br></pre></td></tr></table></figure>
<p>随便选台虚拟机检查一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker -H tcp://<span class="number">192.168</span>.<span class="number">33.17</span>:<span class="number">2376</span> info    <span class="comment"># 可能比较慢，下面那条命令更快</span></span><br><span class="line">docker run --rm swarm:<span class="number">1.1</span>.<span class="number">0</span> list token://<span class="number">3137</span>ebf83d771f1db06bf4eab7ccc73b</span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[用容器轻松搭建Chronos运行环境]]></title>
      <url>http://qinghua.github.io/mesos-chronos/</url>
      <content type="html"><![CDATA[<p>Apache Mesos把自己定位成一个数据中心操作系统，它能管理上万台的从机（slave）。Framework相当于这个操作系统的应用程序，每当应用程序需要执行，Framework就会在Mesos中选择一台有合适资源（cpu、内存等）的从机来运行。Chronos是Framework的一种，被Airbnb公司设计用来代替cron执行作业。本文尝试从零开始用docker搭建Mesos和Chronos的运行环境，并用此环境运行作业。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>参考《用容器轻松搭建Marathon运行环境》的<a href="/mesos-marathon/#u51C6_u5907_u5DE5_u4F5C">准备工作</a>一节，用vagrant搭建两台虚拟机<strong>master</strong>和<strong>slave</strong>。</p>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>参考《用容器轻松搭建Marathon运行环境》的<a href="/mesos-marathon/#u642D_u5EFA_u73AF_u5883">搭建环境</a>一节，跳过marathon部分。</p>
<p>搭建mesos master和slave环境完成后，最后在master的虚拟机上启动chronos：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=ch \</span><br><span class="line">    mesosphere/chronos:chronos-<span class="number">2.4</span>.<span class="number">0</span>-<span class="number">0.1</span>.<span class="number">20150828104228</span>.ubuntu1404-mesos-<span class="number">0.27</span>.<span class="number">0</span>-<span class="number">0.2</span>.<span class="number">190</span>.ubuntu1404 \</span><br><span class="line">    usr/bin/chronos \</span><br><span class="line">    --master zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/mesos \</span><br><span class="line">    --zk_hosts zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/marathon \</span><br><span class="line">    --cluster_name ggg</span><br></pre></td></tr></table></figure></p>
<p>我们还可以打开<code>http://192.168.33.18:8080/</code>感知一下chronos：<br><img src="/img/chronos.jpg" alt=""></p>
<h2 id="u8FD0_u884C_u4F5C_u4E1A"><a href="#u8FD0_u884C_u4F5C_u4E1A" class="headerlink" title="运行作业"></a>运行作业</h2><p>在chronos的页面上，点击<strong>New Job</strong>按钮，然后填入如下参数：</p>
<ul>
<li>NAME：test</li>
<li>COMMAND：docker run -d busybox sleep 30</li>
<li>SCHEDULE P：T1M</li>
</ul>
<p>如下图所示：<br><img src="/img/chronos-run-job.jpg" alt=""></p>
<p>点击<strong>Create</strong>按钮，立刻就能看见有一个名为<strong>test</strong>的作业正在运行。同时，在mesos的主页上也能看到有一个任务运行起来了。如果手慢一点或者喝了杯茶，还能看见有几个任务已经是完成的状态了，这是因为根据我们设置的T1M，1分钟之后，chronos就会帮助再重新启动一次作业。另外还可以在slave的虚拟机上用<code>docker ps -a</code>看到busybox容器已经启动起来了。</p>
<p><em>T1M的意思是：1分钟之后。1M的意思是：1个月之后。1Y2M3DT4H5M6S的意思是：1年2月3天4小时5分钟6秒之后。</em></p>
<h2 id="u7BA1_u7406_u4F5C_u4E1A"><a href="#u7BA1_u7406_u4F5C_u4E1A" class="headerlink" title="管理作业"></a>管理作业</h2><p>新建作业的窗口里还有一个<strong>Other settings</strong>的链接，是可以在里面设置一些高级功能的，比如说CPU、内存和磁盘，默认是0.1、128MB和256MB，还能设置作业优先级、运行方式等。已经生成的作业也可以再次修改、强制运行、复制和删除。作业也可以是一次性的，只要把SCHEDULE R设成0就可以了，记得同时调整一下时间T哦。</p>
<p>作业直接还可以指定依赖。我们再创建一个如下作业：</p>
<ul>
<li>NAME：test2</li>
<li>COMMAND：date &gt;&gt; /tmp/test.txt</li>
<li>PARENTS: test</li>
</ul>
<p>这回我们并没有设置SCHEDULE，而是设置了一个PARENTS为<strong>test</strong>，它的意思就是当test运行成功时，运行这个test2的作业。等1分钟，test作业再次运行后，在slave虚拟机上运行<code>docker exec ms1 cat /tmp/test.txt</code>就能看到当前时间已经被写进<code>/tmp/test.txt</code>文件中，test2作业也被成功运行了。</p>
<p>我们再创建第三个作业：</p>
<ul>
<li>NAME：test3</li>
<li>COMMAND：echo “hello world” &gt;&gt; /tmp/test.txt</li>
<li>PARENTS: test, test2</li>
</ul>
<p>这次test3依赖于test和test2，那么它们之间是“或”还是“和”的关系呢？我们强制运行一次test，在slave虚拟机上运行<code>docker exec ms1 cat /tmp/test.txt</code>就能看到增加了一行日期和一行hello world。强制运行一次test2，却只增加了一行日期而没有增加hello world。由此推断，只有当test2和test全部被执行后，才会执行一次test3。那如果我们要“或”的关系怎么办呢？首先把test3的PARENTS修改为test，记得作业是有一个复制功能的吧？再复制一份test3把它的PARENTS设成test2就好啦。</p>
<p>最后介绍一下那个<strong>Graph</strong>按钮，它可以显示一幅图来表示作业之间的依赖关系。对于我们的示例来说，这图应该是长这样的：<br><img src="/img/chronos-graph.png" alt=""></p>
<p>对于一个非常复杂的作业系统来说，这样的图能让我们很容易找到并分析作业间的依赖关系，从而采取优化措施。许多持续集成系统也都能提供类似的依赖图供管理员决策。</p>
<h2 id="u6CE8_u610F_u4E8B_u9879"><a href="#u6CE8_u610F_u4E8B_u9879" class="headerlink" title="注意事项"></a>注意事项</h2><ol>
<li>这个版本的chronos页面上有一些bug，会导致有时候侧面的作业详细信息栏显示有问题，刷新一下整个页面或者尝试多点击几次左边的作业表就好了。</li>
<li>还是页面的bug，有时候新建作业时会提示需要输入OWNER(S)，这是个Email地址，这样当作业出错时会通知这个Email。可是我们的测试应该是不需要错误通知的。当你看到这个提示的时候，随便输个自己的邮箱就好啦。</li>
<li>像marathon一样，chronos也支持REST API，我们可以来试一下。先随便在哪台机器找个路径生成一个<code>test4.json</code>文件，内容如下：<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "<span class="attribute">parents</span>": <span class="value">[</span><br><span class="line">    <span class="string">"test"</span></span><br><span class="line">  ]</span>,</span><br><span class="line">  "<span class="attribute">name</span>": <span class="value"><span class="string">"test4"</span></span>,</span><br><span class="line">  "<span class="attribute">command</span>": <span class="value"><span class="string">"echo test4 &gt;&gt; /tmp/test.txt"</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>然后运行以下命令来发送请求给chronos：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span>/scheduler/dependency <span class="operator">-d</span> @<span class="built_in">test</span>4.json -H <span class="string">"Content-type: application/json"</span></span><br></pre></td></tr></table></figure></p>
<p>刷新chronos的页面就能看见这个作业在跑啦。想要删掉它？当然没问题：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X DELETE http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span>/scheduler/job/<span class="built_in">test</span>4 -H <span class="string">"Content-type: application/json"</span></span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[用容器轻松搭建Marathon运行环境]]></title>
      <url>http://qinghua.github.io/mesos-marathon/</url>
      <content type="html"><![CDATA[<p>Apache Mesos把自己定位成一个数据中心操作系统，它能管理上万台的从机（slave）。Framework相当于这个操作系统的应用程序，每当应用程序需要执行，Framework就会在Mesos中选择一台有合适资源（cpu、内存等）的从机来运行。<a href="https://mesosphere.github.io/marathon/" target="_blank" rel="external">Marathon</a>是Framework的一种，被设计来支持长时间运行的服务。本文尝试从零开始用docker搭建Mesos和Marathon的运行环境，并用此环境长时间运行docker容器。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code>config.vm.box = &quot;minimum/ubuntu-trusty64-docker&quot;</code>，在它的下面添加如下几行代码，相当于给它分配两台虚拟机，一台叫做<strong>master</strong>，它的IP是<strong>192.168.33.18</strong>；另一台叫做<strong>slave</strong>，它的IP是<strong>192.168.33.19</strong>。<br><figure class="highlight ruby"><figcaption><span>Vagrantfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">config.vm.define <span class="string">"master"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"master"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.18"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">config.vm.define <span class="string">"slave"</span> <span class="keyword">do</span> | host |</span><br><span class="line">  host.vm.hostname = <span class="string">"slave"</span></span><br><span class="line">  host.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"192.168.33.19"</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后分别在两个终端运行以下命令启动并连接两台虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host terminal 1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh master</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sh"><figcaption><span>virtual box host terminal 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh slave</span><br></pre></td></tr></table></figure>
<h2 id="u642D_u5EFA_u73AF_u5883"><a href="#u642D_u5EFA_u73AF_u5883" class="headerlink" title="搭建环境"></a>搭建环境</h2><p>在master的虚拟机上启动zookeeper：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=zk \</span><br><span class="line">    <span class="operator">-e</span> MYID=<span class="number">1</span> \</span><br><span class="line">    <span class="operator">-e</span> SERVERS=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    mesoscloud/zookeeper:<span class="number">3.4</span>.<span class="number">6</span>-ubuntu-<span class="number">14.04</span></span><br></pre></td></tr></table></figure></p>
<p>可以用<code>docker ps</code>看到名为zk的容器已经启动起来了。有兴趣的话，可以用下面的命令验证：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it zk zkCli.sh -server <span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">2181</span></span><br></pre></td></tr></table></figure></p>
<p>这里就不详细介绍zookeeper的命令了，<code>ls /</code>可以查看根节点，<code>help</code>可以查看所有命令，<code>quit</code>退出客户端。</p>
<p>接下来在master的虚拟机上启动mesos master：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=mm \</span><br><span class="line">    <span class="operator">-e</span> MESOS_HOSTNAME=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    <span class="operator">-e</span> MESOS_IP=<span class="number">192.168</span>.<span class="number">33.18</span> \</span><br><span class="line">    <span class="operator">-e</span> MESOS_ZK=zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/mesos \</span><br><span class="line">    <span class="operator">-e</span> MESOS_QUORUM=<span class="number">1</span> \</span><br><span class="line">    <span class="operator">-e</span> MESOS_LOG_DIR=/var/<span class="built_in">log</span>/mesos \</span><br><span class="line">    mesoscloud/mesos-master:<span class="number">0.24</span>.<span class="number">1</span>-ubuntu-<span class="number">14.04</span></span><br></pre></td></tr></table></figure></p>
<p>顺利的话，打开<code>http://192.168.33.18:5050/</code>应该能看到下图：<br><img src="/img/mesos-master-init.png" alt=""></p>
<p>然后在slave的虚拟机上启动mesos slave：<br><figure class="highlight sh"><figcaption><span>slave</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --pid=host \</span><br><span class="line">    --privileged=<span class="literal">true</span> \</span><br><span class="line">    --name=ms1 \</span><br><span class="line">    -v /usr/bin/docker:/usr/bin/docker \</span><br><span class="line">    -v /dev:/dev \</span><br><span class="line">    -v /usr/lib/x86_64-linux-gnu/libapparmor.so.<span class="number">1</span>:/usr/lib/x86_64-linux-gnu/libapparmor.so.<span class="number">1</span>:ro \</span><br><span class="line">    -v /var/run/docker.sock:/var/run/docker.sock \</span><br><span class="line">    -v /var/<span class="built_in">log</span>/mesos:/var/<span class="built_in">log</span>/mesos \</span><br><span class="line">    -v /tmp/mesos:/tmp/mesos \</span><br><span class="line">    <span class="operator">-e</span> MESOS_HOSTNAME=<span class="number">192.168</span>.<span class="number">33.19</span> \</span><br><span class="line">    <span class="operator">-e</span> MESOS_IP=<span class="number">192.168</span>.<span class="number">33.19</span> \</span><br><span class="line">    <span class="operator">-e</span> MESOS_MASTER=zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/mesos \</span><br><span class="line">    <span class="operator">-e</span> MESOS_CONTAINERIZERS=docker,mesos \</span><br><span class="line">    mesoscloud/mesos-slave:<span class="number">0.24</span>.<span class="number">1</span>-ubuntu-<span class="number">14.04</span></span><br></pre></td></tr></table></figure></p>
<p>点击mesos页面上的<strong>Slaves</strong>应该能看到下图，这说明slave节点已经关联到mesos master了：<br><img src="/img/mesos-slaves.png" alt=""></p>
<p>最后在master的虚拟机上启动marathon：<br><figure class="highlight sh"><figcaption><span>master</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">    --net=host \</span><br><span class="line">    --name=ma \</span><br><span class="line">    mesosphere/marathon:v0.<span class="number">15.0</span> \</span><br><span class="line">    --master zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/mesos \</span><br><span class="line">    --zk zk://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">2181</span>/marathon</span><br></pre></td></tr></table></figure></p>
<p>点击mesos页面上的<strong>Frameworks</strong>应该能看到下图，这说明marathon已经作为一个framework关联到mesos master了：<br><img src="/img/mesos-frameworks.png" alt=""></p>
<p>我们还可以打开<code>http://192.168.33.18:8080/</code>感知一下marathon：<br><img src="/img/marathon.png" alt=""></p>
<h2 id="u8FD0_u884C_u5BB9_u5668"><a href="#u8FD0_u884C_u5BB9_u5668" class="headerlink" title="运行容器"></a>运行容器</h2><p>在marathon的页面上，点击<strong>Create Application</strong>按钮，然后填入如下参数：</p>
<ul>
<li>ID：test</li>
<li>Command：sleep 30</li>
<li>Image：busybox</li>
</ul>
<p>如下图所示：<br><img src="/img/marathon-run-docker.png" alt=""></p>
<p>点击<strong>+ Create</strong>按钮，不一会儿，就能看见有一个名为<strong>test</strong>的应用程序正在运行。同时，在mesos的主页上也能看到有一个任务运行起来了。如果手慢一点或者喝了杯茶，还能看见有几个任务已经是完成的状态了，这是因为我们只让这个容器<code>sleep 30</code>存活30秒钟，如果容器自己停止了，marathon就会帮助再重新启动一个。另外还可以在slave的虚拟机上用<code>docker ps</code>看到busybox容器已经启动起来了。</p>
<h2 id="u7BA1_u7406_u5E94_u7528"><a href="#u7BA1_u7406_u5E94_u7528" class="headerlink" title="管理应用"></a>管理应用</h2><p>Marathon也能很方便地对容器进行扩缩容。当容器启动起来后，在<strong>Health</strong>栏里会有一个<strong>…</strong>的按钮，按下按钮如下图所示：<br><img src="/img/marathon-actions.png" alt=""></p>
<p>里面的<strong>Scale</strong>按钮提供了一个比较易用的方法，让我们能轻易地改变容器的数量。可以试试在弹出的界面上填2，然后点击<strong>Scale Application</strong>按钮，就会看到<strong>Running Instances</strong>很快就变成了<strong>2 of 2</strong>，点击这一行，就能看到这两个实例的状态，还可以分别下载它们的日志。同时，在mesos的主页上也能看到又多了一个运行中的任务。另外，在slave的虚拟机上用<code>docker ps</code>也能看到2个busybox的容器实例。</p>
<p>除了扩缩容，还能重新启动应用、暂停应用（实际上就是把它缩容为0个实例）和删除应用等。另外，在test这个Application的Configuration页上，还支持修改这个Application的启动参数。比如还可以在这里设置基于TCP、HTTP或者命令的健康检查，设置环境变量等多种操作。如下图所示：<br><img src="/img/marathon-edit-configuration.png" alt=""></p>
<h2 id="u6CE8_u610F_u4E8B_u9879"><a href="#u6CE8_u610F_u4E8B_u9879" class="headerlink" title="注意事项"></a>注意事项</h2><ol>
<li>如果第一次运行一个比较大的镜像，可能需要比较长的下载时间。在这种情况下，需要往mesos master的启动参数里增加<code>-e MESOS_EXECUTOR_REGISTRATION_TIMEOUT=10mins</code>，否则可能会报timeout的错误，默认是1分钟。还得在marathon的启动参数里增加<code>--task_launch_timeout=600000</code>，默认为300000毫秒，即5分钟。</li>
<li>如果一个容器老是启动不起来，可能是分配给它的资源太少。可以在marathon新建应用的窗口里指定分配给此应用的CPU和内存，默认是0.1和16MB。但是不要超过mesos slave能提供的资源哦。</li>
<li>如果你是坚定不移的命令行狂人，可以用<code>curl</code>来发送一个http请求来运行容器。还是用上面的<strong>test</strong>来举例子，先随便在哪台机器找个路径生成一个<code>test.json</code>文件，内容如下：<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "<span class="attribute">id</span>": <span class="value"><span class="string">"test"</span></span>,</span><br><span class="line">  "<span class="attribute">cmd</span>": <span class="value"><span class="string">"sleep 30"</span></span>,</span><br><span class="line">  "<span class="attribute">cpus</span>": <span class="value"><span class="number">0.1</span></span>,</span><br><span class="line">  "<span class="attribute">mem</span>": <span class="value"><span class="number">16.0</span></span>,</span><br><span class="line">  "<span class="attribute">container</span>": <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">type</span>": <span class="value"><span class="string">"DOCKER"</span></span>,</span><br><span class="line">    "<span class="attribute">volumes</span>": <span class="value">[]</span>,</span><br><span class="line">    "<span class="attribute">docker</span>": <span class="value">&#123;</span><br><span class="line">      "<span class="attribute">image</span>": <span class="value"><span class="string">"busybox"</span></span>,</span><br><span class="line">      "<span class="attribute">privileged</span>": <span class="value"><span class="literal">false</span></span>,</span><br><span class="line">      "<span class="attribute">parameters</span>": <span class="value">[]</span>,</span><br><span class="line">      "<span class="attribute">forcePullImage</span>": <span class="value"><span class="literal">false</span></span><br><span class="line">    </span>&#125;</span><br><span class="line">  </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>然后运行以下命令来发送请求给marathon：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span>/v2/apps <span class="operator">-d</span> @test.json -H <span class="string">"Content-type: application/json"</span></span><br></pre></td></tr></table></figure></p>
<p>打开marathon的页面就能看见这个应用在跑啦。想要删掉它？当然没问题：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X DELETE http://<span class="number">192.168</span>.<span class="number">33.18</span>:<span class="number">8080</span>/v2/apps/<span class="built_in">test</span> -H <span class="string">"Content-type: application/json"</span></span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[删除Docker Registry里的镜像怎么那么难]]></title>
      <url>http://qinghua.github.io/docker-registry-delete/</url>
      <content type="html"><![CDATA[<p>除了官方的Docker Hub，Docker也提供了Docker Registry来让大家搭建自己的私有镜像库。虽然它提供了<a href="https://github.com/docker/distribution/blob/master/docs/spec/api.md#user-content-detail" target="_blank" rel="external">删除的API</a>，但是不好用。为什么小小的删除功能没弄好呢？我们该怎么办？<br><a id="more"></a></p>
<h2 id="u95EE_u9898"><a href="#u95EE_u9898" class="headerlink" title="问题"></a>问题</h2><p>有很多人抱怨说<a href="https://github.com/docker/distribution/issues/1183" target="_blank" rel="external">Docker Registry的删除功能并不会真正地释放空间</a>。虽然官方提供了API，但那些都是软删除（soft delete），只是把二进制和镜像的关系解除罢了，并不是真正的删除。真正的删除有那么困难吗？</p>
<p>目前docker官方提供了如下3个软删除的方法：</p>
<ol>
<li><code>DELETE:/v2/&lt;name&gt;/manifests/&lt;reference&gt;</code>：这个API是软删除一个<a href="https://github.com/docker/distribution/blob/master/docs/spec/manifest-v2-2.md" target="_blank" rel="external">清单（manifest）</a>，但是真正占用存储空间的层还在。</li>
<li><code>DELETE:/v2/&lt;name&gt;/blobs/&lt;digest&gt;</code>：这个API类似上面那个，只不过它要软删除的对象是层（layer）罢了。</li>
<li><code>DELETE:/v2/&lt;name&gt;/blobs/uploads/&lt;uuid&gt;</code>：这个只是取消掉另一个上传的进程罢了。</li>
</ol>
<h2 id="u96BE_u70B9"><a href="#u96BE_u70B9" class="headerlink" title="难点"></a><a href="https://github.com/docker/distribution/blob/master/ROADMAP.md#deletes" target="_blank" rel="external">难点</a></h2><p>为了删除不需要的数据，腾出磁盘空间，我们希望有删除功能。但是如果一个不健全的删除功能不小心把有用的数据给删了，那还不如没有这个功能呢。在这个逻辑前提下，docker团队选择了不删除数据。除此之外，还有一个考虑：删除功能是需要很大工作量的。大家知道程序员们的价格是比较高的，以相对便宜的磁盘空间为代价，在眼下先节省这笔人工费开销，并把它投入到更有价值的地方去，不是更有意义么。</p>
<p>那为什么删除功能需要很大的工作量呢？这是因为删除有一个大坑。首先我们来看一下存储模型：所有的数据都被存放到VFS之上，它提供了最终一致性，但是可能需要较长时间才能达到一致。再看镜像的数据结构：一个docker镜像包含了3个概念：标签（tag）、清单（manifest）和层（layer）。标签被关联到清单上，而清单则被关联到层上，就像下图一样：<br><img src="/img/docker-image.jpg" alt=""></p>
<p>其实单说删除本身其实是比较容易的事情，就像现在的垃圾收集算法一样。一个是根搜索法，从根节点开始计算，若某对象不可达，则表明不被用到，可删之。在docker镜像中并没有“根层”的概念，所以需要循环所有的清单来看是否有哪些层不被用到。还有一个是引用计数法，它为每一个对象添加一个引用计数器，为0则表明不被用到。实现起来比较简单，但是很难删除掉循环引用。在docker镜像中，因为它是一个<a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph" target="_blank" rel="external">有向无环图（DAG）</a>，所以并不会有“循环引用”，正是解决这个问题的极佳方案之一。那么这个大坑在哪里呢？问题在于并发。想象一下，如果在删除某层的过程中，有另外一个push的线程误认为此层已经存在，就会在删除之后导致第二个线程push的镜像不能正常工作。</p>
<h2 id="u65B9_u6848"><a href="#u65B9_u6848" class="headerlink" title="方案"></a>方案</h2><p>目前docker官方有几个数据删除的方案（但是还没有实现）：</p>
<ol>
<li>引用计数法：如上文所述是垃圾收集算法的一种。需要维护引用计数器，对于已经存在着的docker registry来说需要数据迁移。</li>
<li>全局锁：引入GC线程来做删除。删除的时候不能写入。实现简单，但是影响性能。</li>
<li>新老代：也是引入GC线程来做删除。将存储分为年轻年老两代，GC线程删除某一代的时候允许同时写入另一代。避免了全局锁的性能问题但是实现起来比较麻烦。</li>
<li>数据库：引入一个数据库，用事务来解决并发的问题。</li>
</ol>
<p>如果你等不及docker官方的实现，并且对自己的私有库的控制力比较强，不需要考虑并发，可以使用<a href="https://github.com/burnettk/delete-docker-registry-image" target="_blank" rel="external">这个脚本</a>来彻底删除，记得先把registry停掉，或者是设置为<a href="https://github.com/docker/distribution/blob/master/docs/configuration.md#user-content-read-only-mode" target="_blank" rel="external">只读模式</a>以避免并发哦。设置一个cron任务，每天凌晨停止服务一小段时间，然后运行脚本，再启动服务就好了。</p>
<p>如果你也认为磁盘空间是比较廉价的，那么使用软删除，也就是上文介绍的官方删除API应该能够符合需求。虽然磁盘空间并没有真正地释放出来，但是删除之后镜像真的就不能再被pull下来了。记得要把<a href="https://github.com/docker/distribution/blob/master/docs/configuration.md#user-content-delete" target="_blank" rel="external">delete的设置</a>打开，否则会得到<code>The operation is unsupported</code>的异常信息。</p>
<p>如果不想使用那些感觉上奇奇怪怪的脚本，还有一个选择是设置两套docker registry，比较稳定的镜像版本放在其中一个库里，不稳定的开发版放另一个库里。每天凌晨把不稳定的版本库清空。这样的话就不会让稳定的版本库的磁盘消耗增长太快，但是也增加了一些管理的难度。没有两全其美的事啊。</p>
<p>写完这篇博客后不久，一个<a href="https://github.com/docker/distribution/blob/master/docs/gc.md" target="_blank" rel="external">gc</a>的commit被合并到了主干，有望在docker registry的2.4版本提供官方的删除功能。不过，它跟上面脚本的思路类似，也需要停止服务或者设置只读模式，并不能完美解决这个问题。抛开解决方案，就删除功能而言，我在测试过程中也发现了一个<a href="https://github.com/docker/distribution/issues/1548" target="_blank" rel="external">缺陷</a>，需要使用docker 1.10版和registry 2.3版才能解决。</p>
<h2 id="v1_vs_v2"><a href="#v1_vs_v2" class="headerlink" title="v1 vs v2"></a>v1 vs v2</h2><p>Docker Registry的老版本v1是用python写的，源码在<a href="https://github.com/docker/docker-registry" target="_blank" rel="external">这里</a>。新版本v2是用go写的，源码在<a href="https://github.com/docker/distribution" target="_blank" rel="external">这里</a>。它们的模型略有变化。老版本v1是个链表，A层链接到B层，B层链接到C层，层层组织起来一个镜像，每一层的ID都是随机生成的。这样一来浪费空间，不能实现层存储的共享，二来有安全隐患，如果不停地提交，会造成ID冲突概率提升。但也正因如此，删除的时候完全没有顾忌，真是成也萧何败也萧何啊。新版本v2的ID是对内容进行sha256哈希之后的结果，所以相同内容的层ID一定是相同的，很好地解决了v1的问题，就是删除功能需要仔细地设计才能实现。除此之外还有鉴权等其他改动，有兴趣的话可以参考<a href="http://www.csdn.net/article/2015-09-09/2825651" target="_blank" rel="external">这篇文章</a>。</p>
<p>题外话：有些人看英文容易把registry和repository搞混。在docker的领域内，repository就是相同名字镜像的集合，比如tomcat的docker repository。而registry就是提供repository服务的系统，比如Docker Hub或者是自己使用Docker Registry安装的私有库等。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如果有10000台机器，你想怎么玩？（七）生命周期]]></title>
      <url>http://qinghua.github.io/kubernetes-in-mesos-7/</url>
      <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的生命周期管理，包括pod、job、node等对象。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a><a id="more"></a>
</li>
</ul>
<h2 id="u5065_u5EB7_u68C0_u67E5"><a href="#u5065_u5EB7_u68C0_u67E5" class="headerlink" title="健康检查"></a>健康检查</h2><p>一个pod在运行中，难免出现容器还好好地跑着，但是却不正常工作的情况。Kubernetes的做法是引入定时<a href="http://kubernetes.io/docs/user-guide/pod-states/#container-probes" target="_blank" rel="external">健康检查</a>，如果健康检查失败，就把这个容器杀掉，然后kubelet就会重新启动一个容器来代替它。目前支持两种健康检查的机制：</p>
<ul>
<li><code>LivenessProbe</code>：如果健康检查失败，就把这个容器杀掉，然后kubelet根据预先设置的<a href="http://kubernetes.io/docs/user-guide/pod-states/#restartpolicy" target="_blank" rel="external">重启规则</a>来决定怎么处理：啥也不干、挂了才重启或者总是重启。</li>
<li><code>ReadinessProbe</code>：如果健康检查失败，这个pod的IP地址将会从endpoints里移除，所以相当于屏蔽这个pod提供的服务而不是将它杀掉。</li>
</ul>
<p>那健康检查怎么做呢？可以是一段脚本，返回非0就代表错误；可以说一个http请求，返回200~400之间代表成功；还可以是一个tcp端口，打开即算成功。可以通过设置kubelet的参数<code>--sync-frequency</code>来设置健康检查的间隔时间。还可以在设置probe的时候指定健康检查的超时时间和第一次健康检查的延时（从容器启动完毕开始）。</p>
<h2 id="u94A9_u5B50_uFF08hook_uFF09"><a href="#u94A9_u5B50_uFF08hook_uFF09" class="headerlink" title="钩子（hook）"></a>钩子（hook）</h2><p>有时候我们需要在pod启动完成或者快要关闭的时候做点儿事情。做的事情可以是执行脚本或者发出http请求，越轻量级越好。Kubernetes提供了两个<a href="http://kubernetes.io/docs/user-guide/container-environment/#container-hooks" target="_blank" rel="external">钩子</a>来做这样的事：</p>
<ul>
<li>postStart：当一个容器被创建成功的时候</li>
<li>preStop：当一个容器即将被关闭的时候</li>
</ul>
<p>钩子的设计理念是“宁滥毋缺”，所以某些情况下它是有可能被执行多次的，设计自己的钩子时需要考虑这样的情况，尽量使操作“无状态”。</p>
<h2 id="u4E00_u6B21_u6027_u4EFB_u52A1"><a href="#u4E00_u6B21_u6027_u4EFB_u52A1" class="headerlink" title="一次性任务"></a>一次性任务</h2><p>Kubernetes除了支持服务，也支持一次性任务<a href="http://kubernetes.io/docs/user-guide/jobs/" target="_blank" rel="external">Job</a>。这个概念在Kubernetes 1.1版中已经有了，<a href="https://github.com/kubernetes/kubernetes/wiki/Release-1.2" target="_blank" rel="external">1.2版</a>才算开发完成。因为一次性任务结束以后还重启没有意义，所以它不支持“总是重启”的重启规则。Job有三种类型：</p>
<ul>
<li>非并行：就是启动一个pod，当pod成功结束了就算是job完成了</li>
<li>固定数量并行：并行启动固定数量个pod，每个pod都成功结束了就算是job完成了</li>
<li>工作队列并行：行启动多个pod，其中一个pod成功结束了，其他pod就开始停止运行。当全部pod都停止了就算是job完成了</li>
</ul>
<p>其实我们当然也能不使用job这个概念而直接启动一个pod来完成我们的一次性任务。可是如果pod运行过程中那个node要是挂掉了那就糟糕了。对了，rc不就是来保证pod总是有实例在运行的机制吗？那么为啥我们还需要job这个概念呢？原来rc是为永不停止的pod设计的，而job是为需要停止的pod设计的，就这么简单。</p>
<h2 id="u6269_u5BB9_u7F29_u5BB9"><a href="#u6269_u5BB9_u7F29_u5BB9" class="headerlink" title="扩容缩容"></a>扩容缩容</h2><p>虚拟机级别上，Mesos也能轻松做到动态增删slave，从而为kubernetes提供更多的offer；与此同时，kubernetes也支持动态增删节点。容器级别上，Kubernetes的replication controller可以很容易地对pod进行扩缩容。此外，Kubernetes 1.2版正式支持<a href="http://kubernetes.io/docs/user-guide/horizontal-pod-autoscaler/" target="_blank" rel="external">HPA</a>。大致来说，就是根据CPU的使用率来自动扩缩容。由于获取CPU使用率需要用到heapster，所以必须部署它。</p>
<p>值得一提的是，Kubernetes在删除pod的时候并不会把容器删除，是出于可能需要在以后查看日志的<a href="https://github.com/kubernetes/kubernetes/issues/1148" target="_blank" rel="external">考虑</a>。</p>
<h2 id="u6EDA_u52A8_u5347_u7EA7"><a href="#u6EDA_u52A8_u5347_u7EA7" class="headerlink" title="滚动升级"></a>滚动升级</h2><p>Kubernetes的replication controller还支持滚动升级（<a href="http://kubernetes.io/docs/user-guide/replication-controller/#rolling-updates" target="_blank" rel="external">rolling update</a>）。当我们想用新版本的镜像来代替已经部署的旧容器的时候，这个特性能用类似蓝绿部署的方式帮我们轻易升级。这个方式是：创建一个副本数为1的rc并关联到新pod，逐渐增加它的副本数并减少旧rc的副本数，最终完全替代。讲起来挺生涩，其实很简单：举个栗子，有一个既存服务来自于3个pod，我们希望用新的pod来代替旧的。这是现在的情况：<br><img src="/img/rolling-update-1.png" alt=""></p>
<p>Kubernetes启动了一个新的rc，它有一个新的pod，并关联到服务上去。<br><img src="/img/rolling-update-2.png" alt=""></p>
<p>然后停掉一个旧的pod，保持这个服务的pod总数还是3个：<br><img src="/img/rolling-update-3.png" alt=""></p>
<p>继续增加新pod：<br><img src="/img/rolling-update-4.png" alt=""></p>
<p>继续停掉旧pod：<br><img src="/img/rolling-update-5.png" alt=""></p>
<p>增加新pod：<br><img src="/img/rolling-update-6.png" alt=""></p>
<p>停掉旧pod，再把没用了的旧rc也删掉：<br><img src="/img/rolling-update-7.png" alt=""></p>
<p>这样新服务就起来了！中间的替换速度是可以由我们设定的，还支持回滚。在<a href="http://kubernetes.io/docs/user-guide/update-demo/" target="_blank" rel="external">这里</a>有一个例子可供参考。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如果有10000台机器，你想怎么玩？（六）性能]]></title>
      <url>http://qinghua.github.io/kubernetes-in-mesos-6/</url>
      <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的性能。纯粹的kubernetes v1.1可以支持250个节点，但是一跟mesos结合起来，由于需要等待、接受资源邀约等行为，确实会更慢一些。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a><a id="more"></a>
</li>
</ul>
<h2 id="u5173_u4E8E_u6027_u80FD"><a href="#u5173_u4E8E_u6027_u80FD" class="headerlink" title="关于性能"></a>关于性能</h2><p>有一篇很知名的<a href="http://blog.kubernetes.io/2015/09/kubernetes-performance-measurements-and.html" target="_blank" rel="external">kubernetes性能测试文章</a>，提到了不少性能测试的考量、结果和计划，也有<a href="http://dockone.io/article/677" target="_blank" rel="external">中文译文</a>。相信看完后对kubernetes自身的性能会有一些感性认识。</p>
<p>Kubernetes v1.0仅仅支持100个节点，kubernetes v1.1已经可以支持250个节点了。官方也希望能<a href="https://github.com/kubernetes/kubernetes/blob/master/docs/admin/multi-cluster.md#user-content-selecting-the-right-number-of-clusters" target="_blank" rel="external">在2016年初支持1000个节点</a>。Kubernetes还提供了一个性能测试工具<a href="https://github.com/kubernetes/kubernetes/blob/master/docs/devel/kubemark-guide.md" target="_blank" rel="external">Kubemark</a>。它由真实的master和虚拟的空节点组成，默认跑在<a href="https://cloud.google.com/container-engine/" target="_blank" rel="external">GCE</a>上，这样不需要大量机器便可以进行性能测试了。</p>
<h2 id="u6CE8_u610F_u4E8B_u9879"><a href="#u6CE8_u610F_u4E8B_u9879" class="headerlink" title="注意事项"></a>注意事项</h2><p>根据<a href="https://github.com/kubernetes/kubernetes/blob/master/docs/admin/cluster-large.md" target="_blank" rel="external">kubernetes大集群</a>这篇文章的描述，1.1版本支持最大250个节点，每个节点30个pod，每个pod 1~2个容器。当使用50台以上的节点时，最好用单独的etcd来存储事件。可以在kubernetes的api server启动参数里配置类似<code>--etcd-servers-overrides=/events#http://192.168.33.11:4001</code>这样的值来分离事件etcd。</p>
<p>如果想尽可能的模拟生产环境，所以在测试环境中使用kubernetes自身的系统插件（如DNS、Heapster、ElasticSearch等）时，也需要注意由于集群规模的增大，默认的插件资源有可能不够，从而导致OOM最终使插件不停地挂掉重启。可以通过配置resources的limit来增大插件的内存供给。</p>
<p>Docker的性能方面，由于Ubuntu的docker存储驱动默认使用AUFS，速度比Device Mapper快上不少。所以如果用CentOS来做node，会明显感觉容器的启动删除都比较慢。网上也有<a href="https://www.linux-toys.com/?p=374" target="_blank" rel="external">文章</a>指出这点，笔者测试的感觉与之相符。</p>
<h2 id="u6027_u80FD_u8C03_u4F18"><a href="#u6027_u80FD_u8C03_u4F18" class="headerlink" title="性能调优"></a>性能调优</h2><h3 id="Mesos+Kubernetes"><a href="#Mesos+Kubernetes" class="headerlink" title="Mesos+Kubernetes"></a>Mesos+Kubernetes</h3><p>Mesos的资源分配现在是酱紫的：</p>
<ol>
<li>slave告诉master自己有什么资源</li>
<li>master把这个资源包装成offer发送给framework（这里是kubernetes）</li>
<li>framework接受或拒绝</li>
<li>若是framework接受了，让slave运行任务<br><img src="/img/mesos-architecture.jpg" alt=""></li>
</ol>
<p>有一点需要注意的是mesos master并不是一收到slave的资源便把它发送给framework的。想象一下如果有1000台机器的话，那offer的发送频率得是什么样子。Mesos master是每隔一段时间发送一次。它的启动参数里有一个<code>--allocation_interval</code>，它决定了这个间隔时间，默认为1秒。当有多个slave的时候，有可能master在前一秒告诉framework有10份offer，后一秒又告诉说现在有15份（有些被拒绝的offer回来了，有些新slave能提供新offer，有些被接受了，但是还有余裕…）。</p>
<p>Mesos+Kubernetes的<a href="https://github.com/kubernetes/kubernetes/blob/master/contrib/mesos/docs/issues.md#scheduling" target="_blank" rel="external">分配算法</a>现在还很原始：一个offer只会运行一个pod。所以如果请求启动的大量的pod的时候，就需要很多个offer来运行这些pod。为了提高性能，一个办法是在一个offer里安排多个pods，这个是kubernetes的未来计划，现在还不是我们的菜。另一个办法是提高mesos master的offer频率。虽然把<code>--allocation_interval</code>调低可以增加offer发送频率，但是如果offer回流得很慢，那又有什么意义呢。所以kubernetes的scheduler处理得越久，offer的流动性就越差，pod的启动速度就越慢。接着往下走，如何提高scheduler的处理速度呢？最简单的处理办法：换高配！4C8G的虚拟机撑死能扛住250台mesos slave。软件上就还得靠优化scheduler的流程了。另外，mesos master由于要不断发offer出去，还要处理被接受或拒绝的offer，也需要比较强的配置，但是kubernetes master的配置影响力更大，需要相对更好的配置。</p>
<p>Mesos+Kubernetes的scheduler还支持一些细粒度的性能调优，有兴趣的朋友可以去<a href="https://github.com/kubernetes/kubernetes/blob/master/contrib/mesos/docs/scheduler.md#user-content-tuning" target="_blank" rel="external">看一看</a>。</p>
<h3 id="u7EAFkubernetes"><a href="#u7EAFkubernetes" class="headerlink" title="纯kubernetes"></a>纯kubernetes</h3><p>不带mesos玩儿的kubernetes会简单一些，它的scheduler支持一个参数<code>--bind-pods-qps</code>，这个值决定每秒启动的pod数，默认为50。可以根据机器和网络性能相对应地调节。</p>
<h2 id="u6D4B_u8BD5_u7ED3_u679C"><a href="#u6D4B_u8BD5_u7ED3_u679C" class="headerlink" title="测试结果"></a>测试结果</h2><h3 id="Mesos+Kubernetes-1"><a href="#Mesos+Kubernetes-1" class="headerlink" title="Mesos+Kubernetes"></a>Mesos+Kubernetes</h3><p>Mesos+Kubernetes的情况下，100台mesos slave的情况下，启动100个pod需要将近50秒。由上可知，由于offer是比较均匀的，pod的创建时间基本上也是均匀的。这就意味着启动500个pod需要将近250秒。而且，pod还是比较平均地分布在所有slave上的。删除pod的话，因为无关offer，所以就不是线性关系了。100个pod需要10~15秒，如果一口气删得多一些，需要的时间会比线性增加的时间少一些。250台mesos slave的情况下，基本上kubernetes就带不动了，api server的cpu占用率很高。</p>
<h3 id="u7EAFkubernetes-1"><a href="#u7EAFkubernetes-1" class="headerlink" title="纯kubernetes"></a>纯kubernetes</h3><p>不带mesos玩儿的kubernetes在100台节点的情况下，速度要快得多：启动100个pod仅需10秒左右，1000个约80秒。如果分配超过3000的pod，就会出现部分pod起不来的情况。笔者试验了4000个pod，有280个起不来。250台节点的时候也没有什么压力，性能上比Mesos+Kubernetes好了不止一星半点。现在kubernetes 1.2版已经发布，支持1000个node不成问题。就是…如果按本文的标题来说，10000台机器，还是得建10个kubernetes集群才行呀。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Docker的存储是怎么工作的]]></title>
      <url>http://qinghua.github.io/docker-storage/</url>
      <content type="html"><![CDATA[<p>我们都知道docker支持多种存储驱动，默认在ubuntu上使用AUFS，其他Linux系统上使用devicemapper。这篇文章从零开始，用一些Linux的命令来使用这些不同的存储，包括AUFS、Device Mapper、Btrfs和Overlay。<br><a id="more"></a></p>
<h2 id="u80CC_u666F_u77E5_u8BC6"><a href="#u80CC_u666F_u77E5_u8BC6" class="headerlink" title="背景知识"></a>背景知识</h2><p>Docker最早只是运行在Ubuntu和Debian上，使用的存储驱动是AUFS。随着Docker越来越流行，很多人都希望能把它运行在RHEL系列上。可是Linux内核和RHEL并不支持AUFS，最后红帽公司和Docker公司一起合作开发了基于Device Mapper技术的devicemapper存储驱动，这也成为Docker支持的第二款存储。由于Linux内核2.6.9就已经包含Device Mapper技术了，所以它也非常的稳定，代价是比较慢。<a href="https://en.wikipedia.org/wiki/ZFS" target="_blank" rel="external">ZFS</a>是被Oracle收购的Sun公司为Solaris 10开发的新一代文件系统，支持快照，克隆，<a href="https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/#the-copy-on-write-strategy" target="_blank" rel="external">写时复制</a>（CoW）等。ZFS的“Z”是最后一个字母，表示终极文件系统，不需要开发其它的文件系统了。虽然ZFS各种好，但是毕竟它的Linux版本是移植过来的，Docker官方并不推荐在生产环境上使用，除非你对ZFS相当熟悉。而且由于软件许可证不同的关系，它也无法被合并进Linux内核里。这么NB的文件系统出来后，Linux社区也有所回应。<a href="https://btrfs.wiki.kernel.org/index.php/Main_Page" target="_blank" rel="external">Btrfs</a>就是和ZFS比较类似的Linux原生存储系统，在Linux内核2.6.29里就包含它了。虽然Btrfs未来是要替换devicemapper的，但是目前devicemapper更安全，更稳定，更适合生产环境。所以如果不是有很经验的话，也不那么推荐在生产环境使用。<a href="https://en.wikipedia.org/wiki/OverlayFS" target="_blank" rel="external">OverlayFS</a>是类似AUFS的<a href="https://en.wikipedia.org/wiki/UnionFS" target="_blank" rel="external">联合文件系统</a>，但是轻量级一些，而且还能快一点儿。更重要的是，它已经被合并到Linux内核3.18版了。虽然OverlayFS发展得很快，但是它还非常年轻，如果要上生产系统，还是要记得小心为上。Docker还支持一个<a href="https://en.wikipedia.org/wiki/Virtual_file_system" target="_blank" rel="external">VFS</a>驱动，它是一个中间层的抽象，底层支持ext系列，ntfs，nfs等等，对上层提供一个标准的文件操作接口，很早就被包含到Linux内核里了。但是由于它不支持写时复制，所以比较占磁盘空间，速度也慢，同样并不推荐上生产环境。</p>
<p>说到这里，好几个存储驱动都上Linux内核了，怎么AUFS一直被拒于门外呢？AUFS是一个日本人岡島順治郎开发的，他也曾希望能把这个存储驱动提交到内核中。但是据说<a href="http://www.programering.com/a/MTM0YDNwATQ.html" target="_blank" rel="external">Linus Torvalds有点儿嫌弃AUFS的代码写得烂</a>……</p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令启动并连接虚拟机：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="AUFS"><a href="#AUFS" class="headerlink" title="AUFS"></a>AUFS</h2><p>AUFS是一个联合文件系统，也就是说，它是一层层垒上去的文件系统。最上层能看到的就是下层的所有系统合并后的结果。我们创建几个文件夹，layer1是最底层，result用来挂载，再搞几个文件：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/aufs</span><br><span class="line"><span class="built_in">cd</span> ~/aufs</span><br><span class="line"></span><br><span class="line">mkdir layer1 layer2 result</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"file1 in layer1"</span> &gt; layer1/file1</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"file2 in layer1"</span> &gt; layer1/file2</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"file1 in layer2"</span> &gt; layer2/file1</span><br></pre></td></tr></table></figure></p>
<p>现在文件夹的层级结构看起来是酱紫的：</p>
<p><pre><br>└── aufs<br>    ├── layer1<br>    │   ├── file1    # file1 in layer1<br>    │   └── file2    # file2 in layer1<br>    ├── layer2<br>    │   └── file1    # file1 in layer2<br>    └── result<br></pre><br>然后一层层地挂载到result文件夹去（none的意思是挂载的不是设备文件），就能看到result现在有两个文件，以及它们的内容：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t aufs -o br=layer2=rw:layer1=ro none result</span><br><span class="line"></span><br><span class="line">ls result</span><br><span class="line">cat result/file1    <span class="comment"># file1 in layer2</span></span><br><span class="line">cat result/file2    <span class="comment"># file2 in layer1</span></span><br></pre></td></tr></table></figure></p>
<p>file1是由layer2提供的，file2是由layer1提供的，因为layer2里没有file2。如果我们在挂载后的目录写入file1~3：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"file1 in result"</span> &gt; result/file1</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"file2 in result"</span> &gt; result/file2</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"file3 in result"</span> &gt; result/file3</span><br><span class="line"></span><br><span class="line">cat layer1/file1    <span class="comment"># file1 in layer1</span></span><br><span class="line">cat layer1/file2    <span class="comment"># file2 in layer1</span></span><br><span class="line">cat layer2/file1    <span class="comment"># file1 in result</span></span><br><span class="line">cat layer2/file2    <span class="comment"># file2 in result</span></span><br><span class="line">cat layer2/file3    <span class="comment"># file3 in result</span></span><br></pre></td></tr></table></figure></p>
<p>就会看到这些文件都是写入到layer2的。测试完成，清场~<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo umount result</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">rm -rf aufs</span><br></pre></td></tr></table></figure></p>
<p>想要了解更细致点的话可以参考<a href="http://coolshell.cn/articles/17061.html" target="_blank" rel="external">Docker基础技术：AUFS</a>这篇文章。</p>
<p>回头来看Docker官方的这幅图：<br><img src="https://docs.docker.com/engine/userguide/storagedriver/images/aufs_delete.jpg" alt=""></p>
<p>虽然是删除文件的示例，但是也能清楚看到AUFS是怎么工作的。然后再结合docker一起看：<br><img src="https://docs.docker.com/engine/userguide/storagedriver/images/aufs_layers.jpg" alt=""></p>
<p>一切就都很清楚明了：一层层地累加所有的文件，最终加载到镜像里。</p>
<h2 id="Device_Mapper"><a href="#Device_Mapper" class="headerlink" title="Device Mapper"></a>Device Mapper</h2><p>Device Mapper是块设备的驱动，它的写时复制是基于块而非文件的。它包含3个概念：原设备，快照和映射表，它们的关系是：原设备通过映射表映射到快照去。一个快照只能有一个原设备，而一个原设备可以映射成多个快照。快照还能作为原设备映射到其他快照中，理论上可以无限迭代。</p>
<p>Device Mapper还提供了一种<a href="https://www.kernel.org/doc/Documentation/device-mapper/thin-provisioning.txt" target="_blank" rel="external">Thin-Provisioning</a>技术。它实际上就是允许存储的超卖，用以提升空间利用率。当它和快照结合起来的时候，就可以做到许多快照挂载在一个原设备上，除非某个快照发生写操作，不然不会真正给快照们分配空间。这样的原设备叫做<a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Logical_Volume_Manager_Administration/thinprovisioned_volumes.html" target="_blank" rel="external">Thin Volume</a>，它和快照都会由thin-pool来分配，超卖就发生在thin-pool之上。它需要两个设备用来存放实际数据和元数据。下面我们来创建两个文件，用来充当实际数据文件和元数据文件：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/devicemapper</span><br><span class="line"><span class="built_in">cd</span> ~/devicemapper</span><br><span class="line"></span><br><span class="line">mkdir thin</span><br><span class="line">mkdir snap1</span><br><span class="line">mkdir snap11</span><br><span class="line">mkdir snap12</span><br><span class="line"></span><br><span class="line">dd <span class="keyword">if</span>=/dev/zero of=metadata.img bs=<span class="number">1024</span>K count=<span class="number">1</span></span><br><span class="line">dd <span class="keyword">if</span>=/dev/zero of=data.img bs=<span class="number">1024</span>K count=<span class="number">10</span></span><br></pre></td></tr></table></figure></p>
<p>文件建好了之后，用<a href="https://en.wikipedia.org/wiki/Loop_device" target="_blank" rel="external">Loop device</a>把它们模拟成块设备：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo losetup /dev/loop0 metadata.img</span><br><span class="line">sudo losetup /dev/loop1 data.img</span><br><span class="line">sudo losetup <span class="operator">-a</span></span><br></pre></td></tr></table></figure></p>
<p>然后创建thin-pool（参数的含义可以参考<a href="http://coolshell.cn/articles/17200.html" target="_blank" rel="external">这篇文章</a>）：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo dmsetup create pool --table <span class="string">"0 20480 thin-pool /dev/loop0 /dev/loop1 128 32768 1 skip_block_zeroing"</span></span><br><span class="line">ls /dev/mapper/    <span class="comment"># 这里就会多一个pool</span></span><br></pre></td></tr></table></figure></p>
<p>之后创建Thin Volume并格式化：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo dmsetup message /dev/mapper/pool <span class="number">0</span> <span class="string">"create_thin 0"</span></span><br><span class="line">sudo dmsetup create thin --table <span class="string">"0 2048 thin /dev/mapper/pool 0"</span></span><br><span class="line">sudo mkfs.ext4 /dev/mapper/thin</span><br><span class="line">ls /dev/mapper/    <span class="comment"># 这里又会多一个thin</span></span><br></pre></td></tr></table></figure></p>
<p>加载这个Thin Volume并往里写个文件。我的测试机器上需80秒左右才能把这个文件同步回thin-pool去。如果不等待，可能接下来的快照里就不会有这个文件；如果等待时间不足（小于30秒），可能快照里会有这个文件，但是内容为空。这个时间跟thin-pool的参数，尤其是先前创建的实际数据和元数据文件有关。<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mount /dev/mapper/thin thin</span><br><span class="line">sudo sh -c <span class="string">"echo file1 in thin &gt; thin/file1"</span></span><br><span class="line">sleep <span class="number">80</span>s</span><br></pre></td></tr></table></figure></p>
<p>睡饱后，给thin这个原设备添加一份快照snap1：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo dmsetup message /dev/mapper/pool <span class="number">0</span> <span class="string">"create_snap 1 0"</span></span><br><span class="line">sudo dmsetup create snap1 --table <span class="string">"0 2048 thin /dev/mapper/pool 1"</span></span><br><span class="line">ls /dev/mapper/    <span class="comment"># 这里又会多一个snap1</span></span><br></pre></td></tr></table></figure></p>
<p>加载这个快照，能看见先前写的file1文件被同步过来了。再往里写个新文件。还是要保证睡眠充足：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo mount /dev/mapper/snap1 snap1</span><br><span class="line">sudo ls <span class="operator">-l</span> snap1</span><br><span class="line">sudo cat snap1/file1    <span class="comment"># file1 in thin</span></span><br><span class="line">sudo sh -c <span class="string">"echo file2 in snap1 &gt; snap1/file2"</span></span><br><span class="line">sleep <span class="number">80</span>s</span><br></pre></td></tr></table></figure></p>
<p>快照是能作为原设备映射成其他快照的，下面从snap1映射一份snap11：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo dmsetup message /dev/mapper/pool <span class="number">0</span> <span class="string">"create_snap 2 1"</span></span><br><span class="line">sudo dmsetup create snap11 --table <span class="string">"0 2048 thin /dev/mapper/pool 2"</span></span><br></pre></td></tr></table></figure></p>
<p>加载完后就能看到file1和file2都被同步过来了：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo mount /dev/mapper/snap11 snap11</span><br><span class="line">sudo ls <span class="operator">-l</span> snap11</span><br><span class="line">sudo cat snap11/file1    <span class="comment"># file1 in thin</span></span><br><span class="line">sudo cat snap11/file2    <span class="comment"># file2 in snap1</span></span><br></pre></td></tr></table></figure></p>
<p>一份原设备是可以映射成多个快照的，下面从snap1再映射一份snap12：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo dmsetup message /dev/mapper/pool <span class="number">0</span> <span class="string">"create_snap 3 1"</span></span><br><span class="line">sudo dmsetup create snap12 --table <span class="string">"0 2048 thin /dev/mapper/pool 3"</span></span><br><span class="line"></span><br><span class="line">sudo mount /dev/mapper/snap12 snap12</span><br><span class="line">sudo ls <span class="operator">-l</span> snap12</span><br><span class="line">sudo cat snap11/file1    <span class="comment"># file1 in thin</span></span><br><span class="line">sudo cat snap11/file2    <span class="comment"># file2 in snap1</span></span><br></pre></td></tr></table></figure></p>
<p>测试完成，清场~<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo umount snap1</span><br><span class="line">sudo umount snap11</span><br><span class="line">sudo umount snap12</span><br><span class="line">sudo umount thin</span><br><span class="line"></span><br><span class="line">sudo dmsetup remove snap11</span><br><span class="line">sudo dmsetup remove snap12</span><br><span class="line">sudo dmsetup remove snap1</span><br><span class="line">sudo dmsetup remove thin</span><br><span class="line">sudo dmsetup remove pool</span><br><span class="line"></span><br><span class="line">sudo losetup <span class="operator">-d</span> /dev/loop0</span><br><span class="line">sudo losetup <span class="operator">-d</span> /dev/loop1</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">rm -rf devicemapper</span><br></pre></td></tr></table></figure></p>
<p>在RHEL，CentOS系列上，Docker默认使用loop-lvm，类似上文的机制（配的是<a href="https://en.wikipedia.org/wiki/Sparse_file" target="_blank" rel="external">稀疏文件</a>），虽然性能比本文要好，但也是堪忧。官方推荐使用<a href="https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/#other-device-mapper-performance-considerations" target="_blank" rel="external">direct-lvm</a>，也就是直接使用raw分区。<a href="http://blog.opskumu.com/docker-storage-setup.html" target="_blank" rel="external">这篇文章</a>介绍了如何在CentOS 7上使用direct-lvm。另外，<a href="http://www.infoq.com/cn/articles/analysis-of-docker-file-system-aufs-and-devicemapper/" target="_blank" rel="external">剖析Docker文件系统</a>对AUFS和Device Mapper有很详细的讲解。</p>
<p>回头来看Docker官方的这幅图：<br><img src="http://farm1.staticflickr.com/703/22116692899_0471e5e160_b.jpg" alt=""></p>
<p>一切就都很清楚明了：最底层是两个文件：数据和元数据文件。这两个文件上面是一个pool，再上面是一个原设备，然后就是一层层的快照叠加上去，直至镜像，充分共享了存储空间。</p>
<h2 id="Btrfs"><a href="#Btrfs" class="headerlink" title="Btrfs"></a>Btrfs</h2><p>Btrfs的Btr是B-tree的意思，元数据用B树管理，比较高效。它也支持块级别的写时复制，性能也不错，对SSD有优化，但是不支持SELinux。它支持把文件系统的一部分配置为<a href="https://btrfs.wiki.kernel.org/index.php/SysadminGuide#Subvolumes" target="_blank" rel="external">Subvolume</a>子文件系统，父文件系统就像一个pool一样给这些子文件系统们提供底层的存储空间。这就意味着子文件系统无需关心设置各自的大小，反正背后有父文件系统撑腰。Btrfs还支持对子文件系统的快照，速度非常快，起码比Device Mapper快多了。快照在Btrfs里也是一等公民，同样也可以像Subvolume那样再快照、被加载，享受写时复制技术。</p>
<p>要使用Btrfs，得先安装工具包：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install btrfs-tools</span><br></pre></td></tr></table></figure></p>
<p>下面我们来创建一个文件，用Loop device把它模拟成块设备：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/btrfs</span><br><span class="line"><span class="built_in">cd</span> ~/btrfs</span><br><span class="line"></span><br><span class="line">mkdir result</span><br><span class="line"></span><br><span class="line">dd <span class="keyword">if</span>=/dev/zero of=data.img bs=<span class="number">1024</span>K count=<span class="number">10</span></span><br><span class="line"></span><br><span class="line">sudo losetup /dev/loop0 data.img</span><br><span class="line">sudo losetup <span class="operator">-a</span></span><br></pre></td></tr></table></figure></p>
<p>把这个块设备格式化成btrfs：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkfs.btrfs <span class="operator">-f</span> /dev/loop0</span><br><span class="line">sudo mount /dev/loop0 result/</span><br></pre></td></tr></table></figure></p>
<p>新建一个subvolumn并往里写个文件：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs subvolume create result/origin/</span><br><span class="line">sudo sh -c <span class="string">"echo file1 in origin &gt; result/origin/file1"</span></span><br></pre></td></tr></table></figure></p>
<p>给result/origin这个subvolumn添加一份快照snap1，能看见先前写的file1文件被同步过来了。再往里写个新文件：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs subvolume snapshot result/origin/ result/snap1</span><br><span class="line">sudo ls <span class="operator">-l</span> result/snap1</span><br><span class="line">sudo cat result/snap1/file1    <span class="comment"># file1 in origin</span></span><br><span class="line">sudo sh -c <span class="string">"echo file2 in snap1 &gt; result/snap1/file2"</span></span><br></pre></td></tr></table></figure></p>
<p>快照也像Device Mapper那样能生成其他的快照，下面从snap1生成一份snap11：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs subvolume snapshot result/snap1/ result/snap11</span><br><span class="line">sudo ls <span class="operator">-l</span> result/snap11</span><br><span class="line">sudo cat result/snap11/file1    <span class="comment"># file1 in origin</span></span><br><span class="line">sudo cat result/snap11/file2    <span class="comment"># file2 in snap1</span></span><br></pre></td></tr></table></figure></p>
<p>也是可以生成多个快照的，下面从snap1再生成一份snap12：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs subvolume snapshot result/snap1/ result/snap12</span><br><span class="line">sudo ls <span class="operator">-l</span> result/snap12</span><br><span class="line">sudo cat result/snap12/file1    <span class="comment"># file1 in origin</span></span><br><span class="line">sudo cat result/snap12/file2    <span class="comment"># file2 in snap1</span></span><br></pre></td></tr></table></figure></p>
<p>可以使用这个命令来查看所有快照：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs subvolume list result</span><br></pre></td></tr></table></figure></p>
<p>可以使用这个命令来查看这个文件系统：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo btrfs filesystem show /dev/loop0</span><br></pre></td></tr></table></figure></p>
<p>测试完成，清场~<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo umount result</span><br><span class="line"></span><br><span class="line">sudo losetup <span class="operator">-d</span> /dev/loop0</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">rm -rf btrfs</span><br></pre></td></tr></table></figure></p>
<p>我们看到它比Device Mapper更简单一些，并且速度很快，不需要sleep以待同步完成。<a href="http://www.ibm.com/developerworks/cn/linux/l-cn-btrfs/index.html" target="_blank" rel="external">这篇文章</a>虽然有点儿旧了，但是对Btrfs的原理讲得挺清楚的。</p>
<p>回头来看Docker官方的这幅图：<br><img src="https://docs.docker.com/engine/userguide/storagedriver/images/btfs_container_layer.jpg" alt=""></p>
<p>一切就都很清楚明了：最底层是个subvolume，再它之上层层累加快照，镜像也不例外。</p>
<h2 id="Overlay"><a href="#Overlay" class="headerlink" title="Overlay"></a>Overlay</h2><p>最初它叫做OverlayFS，后来被合并进Linux内核的时候被改名为Overlay。它和AUFS一样都是联合文件系统。Overlay由两层文件系统组成：upper（上层）和lower（下层）。下层可以是只读的任意的Linux支持的文件系统，甚至可以是另一个Overlay，而上层一般是可读写的。所以模型上比AUFS要简单一些，这就是为什么我们会认为它更轻量级一些。</p>
<p>用<code>uname -r</code>可以看到我们现在这个vagrant虚拟机的Linux内核版本是3.13，而内核3.18之后才支持Overlay，所以我们得先升级一下内核，否则在mount的时候会出错：<code>mount: wrong fs type, bad option, bad superblock on overlay</code>。运行以下命令来升级ubuntu 14.04的内核：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /tmp/</span><br><span class="line"></span><br><span class="line">wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v3.<span class="number">18</span>-vivid/linux-headers-<span class="number">3.18</span>.<span class="number">0</span>-<span class="number">031800</span>-generic_3.<span class="number">18.0</span>-<span class="number">031800.201412071935</span>_amd64.deb</span><br><span class="line">wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v3.<span class="number">18</span>-vivid/linux-headers-<span class="number">3.18</span>.<span class="number">0</span>-<span class="number">031800</span>_3.<span class="number">18.0</span>-<span class="number">031800.201412071935</span>_all.deb</span><br><span class="line">wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v3.<span class="number">18</span>-vivid/linux-image-<span class="number">3.18</span>.<span class="number">0</span>-<span class="number">031800</span>-generic_3.<span class="number">18.0</span>-<span class="number">031800.201412071935</span>_amd64.deb</span><br><span class="line">sudo dpkg -i linux-headers-<span class="number">3.18</span>.<span class="number">0</span>-*.deb linux-image-<span class="number">3.18</span>.<span class="number">0</span>-*.deb</span><br><span class="line">sudo update-grub</span><br><span class="line"></span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure></p>
<p>等待重启之后，重新连接进vagrant虚拟机：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<p>完成之后再用<code>uname -r</code>看一下，现在应该已经是3.18了。下面我们开搞吧：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/overlay</span><br><span class="line"><span class="built_in">cd</span> ~/overlay</span><br><span class="line"></span><br><span class="line">mkdir lower upper work merged</span><br><span class="line"><span class="built_in">echo</span> file1 <span class="keyword">in</span> lower &gt; lower/file1</span><br><span class="line"><span class="built_in">echo</span> file2 <span class="keyword">in</span> lower &gt; lower/file2</span><br><span class="line"><span class="built_in">echo</span> file1 <span class="keyword">in</span> upper &gt; upper/file1</span><br></pre></td></tr></table></figure></p>
<p>现在的文件层级结构看起来是酱紫的：</p>
<p><pre><br>├── lower<br>│   ├── file1   # file1 in lower<br>│   └── file2   # file2 in lower<br>├── merged<br>├── upper<br>│   └── file1   # file1 in upper<br>└── work<br></pre><br>然后我们加载merged，让它的下层是lower，上层是upper。除此之外还需要一个workdir，据说是用来<a href="https://github.com/codelibre-net/schroot/issues/1" target="_blank" rel="external">做一些内部文件原子性操作</a>的，必须是空文件夹：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t overlay overlay -olowerdir=lower,upperdir=upper,workdir=work merged</span><br><span class="line"></span><br><span class="line">cat merged/file1   <span class="comment"># file1 in upper</span></span><br><span class="line">cat merged/file2   <span class="comment"># file2 in lower</span></span><br></pre></td></tr></table></figure></p>
<p>所以我们最终得到了类似AUFS一样的结果。测试完成，清场~<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo umount merged</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">rm -rf overlay</span><br></pre></td></tr></table></figure></p>
<p>在Linux内核3.19之后，overlay还能够支持多层lower（Multiple lower layers），这样就能更好地支持docker镜像的模型了。多层的mount命令是酱紫的：<code>mount -t overlay overlay -olowerdir=/lower1:/lower2:/lower3 /merged</code>，有兴趣的朋友可以再次升级Linux内核试试。</p>
<p>回头来看Docker官方的这幅图：<br><img src="https://docs.docker.com/engine/userguide/storagedriver/images/overlay_constructs.jpg" alt=""></p>
<p>很好地说明了OverlayFS驱动下容器和镜像的存储是怎么工作的，lower、upper和merged各自的关系。然后看看docker镜像：<br><img src="https://docs.docker.com/engine/userguide/storagedriver/images/overlay_constructs2.jpg" alt=""></p>
<p>因为目前docker支持的还不是多层存储，所以在镜像里只是用硬链接来在较低层之间共享数据。今后docker应该会利用overlay的多层技术来改善镜像各层的存储。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>ZFS和VFS由于官方都不推荐上生产我们就不试了，虽然OverlayFS也不推荐，但是它毕竟代表着未来的趋势，还是值得我们看一看的。下表列出了docker所支持的存储驱动特性对比：</p>
<table>
<thead>
<tr>
<th style="text-align:center">驱动</th>
<th style="text-align:center">联合文件系统</th>
<th style="text-align:center">写时复制</th>
<th style="text-align:center">内核</th>
<th style="text-align:center">SELinux</th>
<th style="text-align:center">上生产环境</th>
<th style="text-align:center">速度</th>
<th style="text-align:center">存储空间占用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">AUFS</td>
<td style="text-align:center">是</td>
<td style="text-align:center">文件级别</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">支持</td>
<td style="text-align:center">推荐</td>
<td style="text-align:center">快</td>
<td style="text-align:center">小</td>
</tr>
<tr>
<td style="text-align:center">Device Mapper</td>
<td style="text-align:center">不是</td>
<td style="text-align:center">块级别</td>
<td style="text-align:center">2.6.9</td>
<td style="text-align:center">支持</td>
<td style="text-align:center">有限推荐</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">较大</td>
</tr>
<tr>
<td style="text-align:center">Btrfs</td>
<td style="text-align:center">不是</td>
<td style="text-align:center">块级别</td>
<td style="text-align:center">2.6.29</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">有限推荐</td>
<td style="text-align:center">较快</td>
<td style="text-align:center">较小</td>
</tr>
<tr>
<td style="text-align:center">OverlayFS</td>
<td style="text-align:center">是</td>
<td style="text-align:center">文件级别</td>
<td style="text-align:center">3.18</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">不推荐</td>
<td style="text-align:center">快</td>
<td style="text-align:center">小</td>
</tr>
<tr>
<td style="text-align:center">ZFS</td>
<td style="text-align:center">不是</td>
<td style="text-align:center">块级别</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">不推荐</td>
<td style="text-align:center">较快</td>
<td style="text-align:center">较小</td>
</tr>
<tr>
<td style="text-align:center">VFS</td>
<td style="text-align:center">不是</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">2.4</td>
<td style="text-align:center">支持</td>
<td style="text-align:center">不推荐</td>
<td style="text-align:center">很慢</td>
<td style="text-align:center">大</td>
</tr>
</tbody>
</table>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Docker的桥接网络是怎么工作的]]></title>
      <url>http://qinghua.github.io/docker-bridge-network/</url>
      <content type="html"><![CDATA[<p>我们都知道docker支持多种网络，默认网络bridge是通过一个网桥进行容器间通信的。这篇文章从零开始，用一些Linux的命令来查看主机和容器间的网络通信，也顺带介绍一些网络的基本知识。<br><a id="more"></a></p>
<h2 id="u51C6_u5907_u5DE5_u4F5C"><a href="#u51C6_u5907_u5DE5_u4F5C" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们需要先安装<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="external">virtualBox</a>和<a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="external">vagrant</a>。通过vagrant来驱动virtualBox搭建一个虚拟测试环境。首先在本地任意路径新建一个空文件夹比如<code>test</code>，运行以下命令：<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line">vagrant init minimum/ubuntu-trusty64-docker</span><br><span class="line">vi Vagrantfile</span><br></pre></td></tr></table></figure></p>
<p>里面应该有一句<code># config.vm.network &quot;private_network&quot;, ip: &quot;192.168.33.10&quot;</code>，删掉前面的<code>#</code>注释，相当于给它分配一个<code>192.168.33.10</code>的IP。这个vagrant镜像已经在ubuntu的基础上帮我们安装了docker，用起来很方便。然后运行以下命令启动并连接虚拟机。<br><figure class="highlight sh"><figcaption><span>virtual box host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure></p>
<h2 id="u4E3B_u673A_u548C_u5BB9_u5668_u95F4_u7684_u7F51_u7EDC_u8FDE_u63A5"><a href="#u4E3B_u673A_u548C_u5BB9_u5668_u95F4_u7684_u7F51_u7EDC_u8FDE_u63A5" class="headerlink" title="主机和容器间的网络连接"></a>主机和容器间的网络连接</h2><p>进入虚拟机后，在vagrant主机上运行<code>ifconfig</code>，就能看到有4个网络设备及它们的IPv4地址：</p>
<ul>
<li>docker0：172.17.0.1</li>
<li>eth0：10.0.2.15</li>
<li>eth1：192.168.33.10</li>
<li>lo：127.0.0.1</li>
</ul>
<p>其中的<code>eth0</code>和<code>eth1</code>是普通的以太网卡，<code>eth1</code>就是我们解除注释的IP：<code>192.168.33.10</code>。<code>lo</code>是所谓的<a href="https://en.wikipedia.org/wiki/Loopback" target="_blank" rel="external">环回网卡</a>，每台机器都有。它将这台机器/容器绑定到<code>127.0.0.1</code>的IP上，这样子就算没有真实的网卡，也能通过这个IP访问自己，对于测试来说尤其方便。最上面的<code>docker0</code>就是我们常说的网桥。怎么知道它是个网桥呢？安装<code>bridge-utils</code>的包就能看到了：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install bridge-utils</span><br><span class="line">brctl show</span><br></pre></td></tr></table></figure></p>
<p>网桥设备就好比交换机，可以和其他的网络设备相连接，就像在其他网络设备上拉根网线到这个交换机一样。那么docker怎么使用这个网桥呢，让我们来启动一个容器看看：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --name=ubuntu ubuntu:<span class="number">14.04</span> bash</span><br></pre></td></tr></table></figure></p>
<p>进入容器后，运行<code>ifconfig</code>，就能够看到有2个网络设备及它们的IPv4地址：</p>
<ul>
<li>eth0：172.17.0.2</li>
<li>lo：127.0.0.1</li>
</ul>
<p>它也有自己的<code>lo</code>，还有一块以太网卡<code>eth0</code>，目前的IP是<code>172.17.0.2</code>。使用快捷键<code>Ctrl+P</code>然后再<code>Ctrl+Q</code>，就能退出容器并保持它继续运行。在vagrant主机上运行<code>route</code>命令，可以看到类似下面这个表格：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Destination</th>
<th style="text-align:center">Gateway</th>
<th style="text-align:center">Genmask</th>
<th style="text-align:center">Flags</th>
<th style="text-align:center">Metric</th>
<th style="text-align:center">Ref</th>
<th style="text-align:center">Use</th>
<th style="text-align:center">Iface</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">default</td>
<td style="text-align:center">10.0.2.2</td>
<td style="text-align:center">0.0.0.0</td>
<td style="text-align:center">UG</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">eth0</td>
</tr>
<tr>
<td style="text-align:center">10.0.2.0</td>
<td style="text-align:center">*</td>
<td style="text-align:center">255.255.255.0</td>
<td style="text-align:center">U</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">eth0</td>
</tr>
<tr>
<td style="text-align:center">172.17.0.0</td>
<td style="text-align:center">*</td>
<td style="text-align:center">255.255.0.0</td>
<td style="text-align:center">U</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">docker0</td>
</tr>
<tr>
<td style="text-align:center">192.168.33.0</td>
<td style="text-align:center">*</td>
<td style="text-align:center">255.255.255.0</td>
<td style="text-align:center">U</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">eth1</td>
</tr>
</tbody>
</table>
<p>这个就是vagrant主机的路由表。我们重点看一下<code>172.17.0.0</code>这一行。它的Genmask为<code>255.255.0.0</code>，就意味着<code>255.255</code>对应着的IP<code>172.17</code>是网络地址，而Genmask中<code>0.0</code>对应着的IP<code>0.0</code>是主机地址。整行的意思就是当目标地址是<code>172.17.*.*</code>的时候，匹配这条路由规则。还有一种写法是<code>172.17.0.0/16</code>。当Gateway不为<code>*</code>号时，那就会路由到Gateway去，否则就路由到Iface去。刚才我们知道新容器的IP是<code>172.17.0.2</code>，所以当vagrant主机上的某个数据包的地址是这个新容器的IP时，就会匹配这条路由规则，由docker0来接受这个数据包。如果数据包的地址都不匹配这些规则，就送到<code>default</code>那一行的<code>Gateway</code>里。</p>
<p>那么docker0在接收数据包之后，又会送到哪里去呢？我们在vagrant主机再次运行<code>brctl show</code>，便能看到docker0这个网桥有所变化。它的<code>interfaces</code>里增加了一个<code>vethXXX</code>，在我的机器上叫<code>vethd6d3942</code>。在vagrant主机再次运行<code>ifconfig</code>，我们也能看到这一块新增的VETH虚拟网卡。实际上每启动一个容器，docker便会增加一个叫<code>vethXXX</code>的设备，并把它连接到docker0上，于是docker0就可以把收到的数据包发给这个VETH设备。VETH设备总是成对出现，一端进去的请求总会从peer也就是另一端出来，这样就能将一个namespace的数据发往另一个namespace，就像虫洞一样。那么现在这一端是<code>vethd6d3942</code>，它的另一端是哪儿呢？运行这个命令（记得把VETH设备名改成你自己主机上的设备名）：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -S vethd6d3942</span><br></pre></td></tr></table></figure></p>
<p>可以看到这个VETH设备的<code>peer_ifindex</code>是某个数字，在我的机器上是<code>5</code>。这个<a href="http://www.cisco.com/c/en/us/support/docs/ip/simple-network-management-protocol-snmp/28420-ifIndex-Persistence.html" target="_blank" rel="external">ifindex</a>是一个网络接口的唯一识别编号。通过<code>docker exec -it ubuntu bash</code>进入容器里，然后运行：<br><figure class="highlight sh"><figcaption><span>ubuntu container</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip link</span><br></pre></td></tr></table></figure></p>
<p>可以看到<code>5: eth0</code>，原来跨越namespace跑到容器里头来啦。这就是主机上的VETH设备能跟容器内部通信的原因。每当新启动一个容器，主机就会增加一对VETH设备，把一个连接到docker0上，另一个挂载到容器内部的eth0里。</p>
<h2 id="IP_u548Cmac_u5730_u5740_u6620_u5C04"><a href="#IP_u548Cmac_u5730_u5740_u6620_u5C04" class="headerlink" title="IP和mac地址映射"></a>IP和mac地址映射</h2><p>还有一个问题：每个网络设备都有自己的mac地址，通过ip怎么能找到它呢？在容器外运行这个命令：<br><figure class="highlight sh"><figcaption><span>vagrant host</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arp -n</span><br></pre></td></tr></table></figure></p>
<p>我们就能看到<code>Address</code>和<code>HWaddress</code>，它们分别对应着IP地址和mac地址，这样就匹配起来了。到容器里<code>ifconfig</code>一下，看看<code>172.17.0.2</code>的mac地址，是不是和主机<code>arp -n</code>运行结果中<code>172.17.0.2</code>那行的mac地址一样呢？</p>
<h2 id="u53C2_u8003_u8D44_u6599"><a href="#u53C2_u8003_u8D44_u6599" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://www.ibm.com/developerworks/cn/linux/1310_xiawc_networkdevice/" target="_blank" rel="external">Linux上的基础网络设备详解</a>，介绍了不同的网络设备工作原理。<br><a href="http://www.oschina.net/translate/docker-network-configuration" target="_blank" rel="external">Docker网络配置</a>，从零开始配置docker的网络。<br><a href="http://vbird.dic.ksu.edu.tw/linux_server/0110network_basic.php" target="_blank" rel="external">基础网络概念</a>，来自鸟哥，深入浅出地介绍了网络的基础知识。<br><a href="http://linux.vbird.org/linux_server/0140networkcommand.php" target="_blank" rel="external">Linux常用网络命令</a>，来自鸟哥，看完了就对茫茫的网络命令有了清晰的了解。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[用ansible管理大规模集群]]></title>
      <url>http://qinghua.github.io/ansible-large-scale-cluster/</url>
      <content type="html"><![CDATA[<p><a href="http://www.ansible.com/get-started" target="_blank" rel="external">Ansible</a>是一个配置管理工具，可以用脚本批量操作多台机器。它的特点是非常简洁，基于<a href="https://wiki.archlinux.org/index.php/Secure_Shell_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)" target="_blank" rel="external">SSH</a>，不需要安装代理。但它的缺点也很明显：效率较低，容易挂起，不那么适合大规模环境（如500台以上）。本文介绍了使用ansible管理大规模集群的几种方法。<br><a id="more"></a></p>
<h2 id="u589E_u52A0_u5E76_u884C_u8FDB_u7A0B_u6570"><a href="#u589E_u52A0_u5E76_u884C_u8FDB_u7A0B_u6570" class="headerlink" title="增加并行进程数"></a>增加并行进程数</h2><p>Ansible提供一个<a href="http://docs.ansible.com/ansible/intro_configuration.html#forks" target="_blank" rel="external">forks</a>的属性，可以设置运行并行进程数。这个值默认比较保守，只有5个并行进程。我们可以根据自己的机器性能以及网络情况来设定，很多人使用50，也有用500以上的。如果有很多机器要管理的话，可以尝试先增加这个值，看看效果。有三个地方可以设置forks的数量：</p>
<ul>
<li>环境变量：<code>export ANSIBLE_FORKS=100</code></li>
<li>ansible.cfg这个配置文件里设置：<code>forks=100</code></li>
<li>运行ansible命令时增加参数：<code>-f 100</code></li>
</ul>
<p>当机器数量比较大的时候，难免会有几台机器不能正常执行。这时候ansible会有提示<code>to retry, use: --limit @/xxx/xxx.retry</code>，把它增加到上个命令的后面就好了。</p>
<h2 id="u5F02_u6B65"><a href="#u5F02_u6B65" class="headerlink" title="异步"></a>异步</h2><p>有时候执行某个任务可能需要很长的时间，在集群规模较大的情况下慢得让人无法忍受。这时可以考虑使用<a href="http://docs.ansible.com/ansible/playbooks_async.html" target="_blank" rel="external">异步模式</a>。在<code>tasks</code>里增加<code>async</code>的属性，设成某个数字，比如60，意思就是这个任务最大运行时间不能超过60秒。也可以设成0，意思是不管任务运行多久，一直等待即可。如果没有指定<code>async</code>，则默认为同步模式。还可以设定<code>poll</code>，默认值为10，意思就是每隔10秒轮询查看结果。如果不需要查看结果，设为0就好了。还可以通过<code>register</code>和<code>async_status</code>设定暂时不查看结果，等需要的时候再查看。具体做法可以参考上面的<a href="http://docs.ansible.com/ansible/playbooks_async.html" target="_blank" rel="external">异步模式官网文档</a>，也可以看<a href="http://www.ansible.com.cn/docs/playbooks_async.html" target="_blank" rel="external">翻译的中文文档</a>。</p>
<h2 id="Pull_u6A21_u5F0F"><a href="#Pull_u6A21_u5F0F" class="headerlink" title="Pull模式"></a>Pull模式</h2><p>有些配置管理工具比如<a href="https://www.chef.io/chef/" target="_blank" rel="external">Chef</a>和<a href="https://puppetlabs.com/" target="_blank" rel="external">Puppet</a>，是基于拉模式的。所谓拉模式，是酱紫的：</p>
<ul>
<li>管理员写脚本</li>
<li>管理员上传脚本</li>
<li>agent定时取脚本（例如每隔1分钟）</li>
<li>agent运行新脚本</li>
</ul>
<p>Ansible是没有agent的，它默认基于推模式，也就是说：</p>
<ul>
<li>管理员写脚本</li>
<li>管理员运行脚本</li>
<li>ansible连接各主机运行脚本</li>
</ul>
<p>一般来说，拉模式能轻松应付大规模集群，因为每台机器都是自己去拉取脚本来完成任务。不过也有人用ansible的推模式管理着上千台机器。Ansible提供了<a href="http://docs.ansible.com/ansible/playbooks_intro.html#ansible-pull" target="_blank" rel="external">ansible-pull</a>的工具，能把它变成拉模式。官方资料不多，<a href="https://www.stavros.io/posts/automated-large-scale-deployments-ansibles-pull-mo/" target="_blank" rel="external">这篇文章</a>写得比较详细。大致思路是新建一个<a href="http://git-scm.com/" target="_blank" rel="external">git</a>的仓库，每台机器运行一个cron定时任务（扮演者agent的角色）每隔一段时间去仓库取最新脚本，然后运行之。在<a href="https://raw.githubusercontent.com/ansible/ansible/stable-2.0/CHANGELOG.md" target="_blank" rel="external">ansible 2.0</a>里<code>ansible-pull</code>也有若干改进。</p>
<h2 id="u591A_u7EA7_u8C03_u5EA6"><a href="#u591A_u7EA7_u8C03_u5EA6" class="headerlink" title="多级调度"></a>多级调度</h2><p>还有一种想法是：如果一台主机的性能只能撑100<code>forks</code>，那么10台主机应该就能撑1000台机器。将这1000台机器分区，比如A区到J区。所以由一台主机分发命令给10台主机，让它们各自运行<code>ansible-playbook</code>，而每台主机根据不同的<a href="http://docs.ansible.com/ansible/intro_inventory.html" target="_blank" rel="external">inventory</a>或者是不同的<a href="http://allandenot.com/devops/2015/01/16/ansible-with-multiple-inventory-files.html" target="_blank" rel="external">limit方式</a>来控制不同区的机器并返回结果。理论上这样的多级调度是能够撑起大规模集群的，就是脚本写起来比较麻烦，需要考虑一级主机和二级主机。</p>
<h2 id="u5176_u4ED6_u53C2_u8003_u8D44_u6599"><a href="#u5176_u4ED6_u53C2_u8003_u8D44_u6599" class="headerlink" title="其他参考资料"></a>其他参考资料</h2><p>说到底，如果运行得快，那么集群规模大一点也可以。<a href="http://www.ansible.com/blog/ansible-performance-tuning" target="_blank" rel="external">这篇文章</a>介绍了一些ansible性能调优的方法。<br><a href="https://mackerel.io/" target="_blank" rel="external">Mackerel</a>是一个监控平台。<a href="http://yuuki.hatenablog.com/entry/ansible-mackerel-1000" target="_blank" rel="external">这篇日文文章</a>介绍了使用Ansible和Mackerel API管理1000台规模集群的方法。<a href="http://www.ansible.com/tower" target="_blank" rel="external">Ansible tower</a>也提供了类似的可视化管理页面，官方出品，是不是更靠谱呢。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[传统企业敏捷转型纪实（二）]]></title>
      <url>http://qinghua.github.io/waterfall-to-agile-2/</url>
      <content type="html"><![CDATA[<p>这是启动会议结束之后一周内发生的事。我们现在有了迭代计划，一堆的story和一堆迷茫的人。接下来怎么做开发？本系列目前有两篇：</p>
<ul>
<li><a href="/waterfall-to-agile-1">传统企业敏捷转型纪实（一）</a></li>
<li><a href="/waterfall-to-agile-2">传统企业敏捷转型纪实（二）</a><a id="more"></a>
</li>
</ul>
<h2 id="u8FED_u4EE3_u5F00_u59CB"><a href="#u8FED_u4EE3_u5F00_u59CB" class="headerlink" title="迭代开始"></a>迭代开始</h2><p>上次刚刚理清楚了各组自己的需求，但是组员们并不是都完全了解。Ken先把所有人都召集起来，告诉大家：现在需求不仅仅是BA的事情了，需求不清是开发和测试的责任，大家有义务互相协作，把需求理清楚。各个PO开始讲依赖：有没有依赖于其他组的story？有没有依赖其他人（比如整个大组只有一位安全专家，可能有些story会对这个人有依赖）？PO们讲完了，有的组可能就凭空多了几张被别的组所依赖的卡，优先级还都比较高。所以需要重新安排一下迭代。计划调整完后，各个PO依次大致地给SPO讲一下自己组第一迭代的主要功能和风险，在获得SPO的认可之后，第一个迭代的计划就算确定下来。</p>
<p>然后就该每个组员认领story了。Ken要求每个story都要有对应的开发和测试人员，从新人开始认领。每个成员自己想学什么，想做什么，职业规划是什么，按照它们来决定自己要开发的story。这样的目的是激发每个人的潜能，提高团队的能力，而不仅仅是着眼于这个版本的交付。同样的，每个成员，都不仅仅是开发这个版本，而是开发一个产品。现实中，可能会出现胡乱挑卡的情况，比如说A卡可能很适合甲来做，但是乙是新人，抢先把卡挑走了，这时候就需要PO来与大家沟通，做决策。</p>
<p>落实完了每个人的工作，Ken又把大家叫到一起，问：你们对按时发布有没有信心？5分就是信心指数最高，1分最低，大家一起伸手指示意。大部分人都举4或者5，也许是无所谓，也许是还没适应一个有话就应该讲出来的环境。有个别成员伸3个指头的，就需要解释一下为什么信心不足，SPO需要当场把问题解决，尽量做到所有人都信心爆棚，起码看上去得是这样。</p>
<h2 id="u9700_u6C42_u5206_u6790"><a href="#u9700_u6C42_u5206_u6790" class="headerlink" title="需求分析"></a>需求分析</h2><p>到了具体开发阶段了，怎么做呢？第二天就是一堂需求分析的课程。大家探讨一下开发和测试怎么协作，需求应该怎么分析，测试用例应该怎么写。对于一个story，开发人员需要知道怎么测，做出来的东西由谁来用，才有能力开发。Ken引入了场景树来做需求分析。举个栗子：一个<strong>买手机</strong>的story。看起来好像需求很明确，但是具体做就会有各种问题：到底对方要的是什么样的手机？所以开发前必须搞清楚，这个story的目的是什么。买手机是内容，不是目的。用5个为什么来深挖，可能就能得到这样的目的：<strong>女朋友手机坏了，让我买个新手机</strong>。然后我们可以画出这样的图：<br><img src="/img/scene-tree-1.png" alt=""></p>
<p>第一个步骤可能就是去取款准备买手机。这个步骤可以用<strong>活动</strong>、<strong>实体</strong>、<strong>结果</strong>来建模。活动应该是动词，描述一个活动：取款。它产生了一个名词实体：人民币。校验这个实体可以得到结果，结果具有若干维度。有点晕？看图：<br><img src="/img/scene-tree-2.png" alt=""></p>
<p>取款这个活动，产生了人民币这个实体。结果的维度是金额。取完款之后，去手机店的动作，产生了手机店这个实体。结果的维度有哪家店和日期时间。到店之后，购买手机的活动产生了手机这个实体。结果的维度有品牌、型号、价格等等。这些维度越清晰，这个需求分析的质量越好。如图：<br><img src="/img/scene-tree-3.png" alt=""></p>
<p>有的朋友可能会问：除了最后得到新手机，是不是也得校验我取的款花了多少，那怎么体现在图里呢？这个还是看需求。如果必要的话，可以在购买完手机后増加一个计算余额的活动。</p>
<h2 id="u6D4B_u8BD5_u7528_u4F8B"><a href="#u6D4B_u8BD5_u7528_u4F8B" class="headerlink" title="测试用例"></a>测试用例</h2><p>画完场景图之后，就能比较容易地根据实体和维度导出测试用例来。还是以买手机为例：首先验证第一个实体：人民币。画张表格如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:right">金额</th>
<th style="text-align:center">预期结果</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">人民币</td>
<td style="text-align:right">1000</td>
<td style="text-align:center">OK</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">人民币</td>
<td style="text-align:right">0</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">银行账户余额不足</td>
</tr>
</tbody>
</table>
<p>从Given、When、Then的角度上看，再加上Given，这就是一个很具体的单元测试用例。然后是手机店：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">哪家店</th>
<th style="text-align:center">日期时间</th>
<th style="text-align:center">预期结果</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">手机店</td>
<td style="text-align:center">国美</td>
<td style="text-align:center">2016/01/20 10:00:00</td>
<td style="text-align:center">OK</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">手机店</td>
<td style="text-align:center">苏宁</td>
<td style="text-align:center">2016/01/20 22:00:00</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">下班了</td>
</tr>
<tr>
<td style="text-align:center">手机店</td>
<td style="text-align:center">苏美</td>
<td style="text-align:center">2016/01/20 10:00:00</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">没有这家店</td>
</tr>
</tbody>
</table>
<p>最后是新手机：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">品牌</th>
<th style="text-align:center">型号</th>
<th style="text-align:right">价格</th>
<th style="text-align:center">预期结果</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">新手机</td>
<td style="text-align:center">小米</td>
<td style="text-align:center">Mi Note</td>
<td style="text-align:right">1999</td>
<td style="text-align:center">OK</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">新手机</td>
<td style="text-align:center">魅族</td>
<td style="text-align:center">MX-5</td>
<td style="text-align:right">1799</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">没有这个型号</td>
</tr>
<tr>
<td style="text-align:center">新手机</td>
<td style="text-align:center">华为</td>
<td style="text-align:center">Mate8</td>
<td style="text-align:right">-3199</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">价格不正确</td>
</tr>
<tr>
<td style="text-align:center">新手机</td>
<td style="text-align:center">小魅</td>
<td style="text-align:center">MiMX</td>
<td style="text-align:right">999</td>
<td style="text-align:center">NG</td>
<td style="text-align:left">没有这个品牌</td>
</tr>
</tbody>
</table>
<p>从上面这几张表我们也能看出来，维度越多，测试案例也就越多，所以说需求的质量就会越高。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如果有10000台机器，你想怎么玩？（五）日志]]></title>
      <url>http://qinghua.github.io/kubernetes-in-mesos-5/</url>
      <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的集中化日志方案。日志通常是由许多文件组成，被分散地储存到不同的地方，所以需要集中化地进行日志的统计和检索。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a><a id="more"></a>
</li>
</ul>
<h2 id="u96C6_u4E2D_u5316_u65E5_u5FD7_u67B6_u6784"><a href="#u96C6_u4E2D_u5316_u65E5_u5FD7_u67B6_u6784" class="headerlink" title="集中化日志架构"></a>集中化日志架构</h2><p><a href="http://jasonwilder.com/blog/2013/07/16/centralized-logging-architecture/" target="_blank" rel="external">集中化日志架构</a>包括这几个阶段：收集、传输、存储和分析，有时候也许会涉及告警。</p>
<ul>
<li>收集：通常以代理的形式运行在各个节点上，负责收集日志。我们希望能尽可能地实时，因为当我们重现一个bug的时候，不会愿意再等上好几分钟才能看到当时的操作日志。</li>
<li>传输：把收集到的日志传给存储。这个阶段关注的是可靠性。万一日志丢失的话那可就麻烦了。</li>
<li>存储：按需选择用什么形式的存储。比如要存多久时间？要不要支持扩容？找历史数据的可能性有多大？</li>
<li>分析：不同的分析工具适用于不同的存储。这个也包含可视化的分析及报表导出等等。</li>
<li>告警：出现错误日志的时候通知运维人员。最好还能聚合相同的错误，因为作为运维来说，实在是不想看到同一个类型的错误不停地骚扰过来。</li>
</ul>
<h2 id="u4F20_u7EDF_u65E5_u5FD7_u65B9_u6848"><a href="#u4F20_u7EDF_u65E5_u5FD7_u65B9_u6848" class="headerlink" title="传统日志方案"></a>传统日志方案</h2><p>商业方案<a href="http://www.splunk.com/" target="_blank" rel="external">splunk</a>几乎拥有市面上最丰富的功能，高可用，可扩展，安全，当然很复杂也很贵。还有一个试图成为splunk的SaaS版本<a href="https://www.sumologic.com/" target="_blank" rel="external">Sumo Logic</a>，包含精简的免费版和收费版。免费方案中比较著名的有Elasticsearch公司（现在叫Elastic公司）的<a href="https://www.elastic.co/webinars/introduction-elk-stack" target="_blank" rel="external">ELK</a>和Apache的<a href="http://flume.apache.org/" target="_blank" rel="external">Flume</a>+<a href="http://kafka.apache.org/" target="_blank" rel="external">Kafka</a>+<a href="http://storm.apache.org/" target="_blank" rel="external">Storm</a>。</p>
<p>ELK是<a href="https://www.elastic.co/products/elasticsearch" target="_blank" rel="external">Elastic search</a>、<a href="https://www.elastic.co/products/logstash" target="_blank" rel="external">Logstash</a>和<a href="https://www.elastic.co/products/kibana" target="_blank" rel="external">Kibana</a>三个开源软件的组合。其中logstash可以对日志进行收集、过滤和简单处理，并将其存储到elastic search上，最终供kibana展示（和上一篇的监控很类似啊）。这套方案可以参考<a href="http://dockone.io/article/505" target="_blank" rel="external">新浪的实时日志架构</a>。这一本<a href="http://kibana.logstash.es/content/" target="_blank" rel="external">ELKstack 中文指南</a>也写得非常详细。</p>
<p>Apache的flume扮演者类似logstash的角色来收集数据，storm可以对flume采集到的数据进行实时分析。由于数据的采集和处理速度可能不一致，因此用消息中间件kafka来作为缓冲。但是kafka不可能存储所有的日志数据，所以会用其他的存储系统来负责持久化，如同样由Apache提供的<a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html" target="_blank" rel="external">HDFS</a>。这套方案可以参考<a href="http://tech.meituan.com/mt-log-system-arch.html" target="_blank" rel="external">美团的日志收集系统架构</a>。如果需要对分析后的结果持久化，还可以引入<a href="https://www.mysql.com/" target="_blank" rel="external">mysql</a>等数据库。</p>
<h2 id="kubernetes_u65B9_u6848"><a href="#kubernetes_u65B9_u6848" class="headerlink" title="kubernetes方案"></a>kubernetes方案</h2><p>虽然也支持logstash，Kubernetes官方使用的是<a href="http://www.fluentd.org/" target="_blank" rel="external">fluentd</a>（有<a href="http://www.tuicool.com/articles/7FzqeeI" target="_blank" rel="external">文章</a>称logstash侧重可扩展性而fluentd侧重可靠性）。比方说我们要收集tomcat的日志，可以在tomcat的pod里增加一个<a href="https://github.com/kubernetes/contrib/tree/master/logging/fluentd-sidecar-es" target="_blank" rel="external">fluentd-sidecar-es</a>的辅助容器，指定tomcat容器的日志文件地址，再指定elastic search服务的位置（对于fluentd-sidecar-es这个特定容器来说，是写死在td-agent.conf文件里的），fluentd便会自行将日志文件发送给elastic search。至于kibana，只需指定elastic search的url就能用了。这是kibana的日志页面：<br><img src="/img/kibana.jpg" alt=""></p>
<p>还可以根据日志来配置各种图表，生成很炫的Dashboard。这个是官方的<a href="http://demo.elastic.co/packetbeat/" target="_blank" rel="external">demo</a>：<br><img src="/img/kibana-official.jpg" alt=""></p>
<p>如果日志不是写到文件系统，而是写到stdout或者stderr，那么kubernetes直接就可以用logs命令看到，就不需要这一整套了。但是一个复杂的web应用，通常还是有复杂的日志文件配置的。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如果有10000台机器，你想怎么玩？（四）监控]]></title>
      <url>http://qinghua.github.io/kubernetes-in-mesos-4/</url>
      <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的监控告警方案。所谓监控主要就是收集和储存主机和容器的实时数据，根据运维人员的需求展示出来的过程。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a><a id="more"></a>
</li>
</ul>
<h2 id="u6570_u636E_u91C7_u96C6"><a href="#u6570_u636E_u91C7_u96C6" class="headerlink" title="数据采集"></a>数据采集</h2><p><a href="https://github.com/google/cadvisor" target="_blank" rel="external">cAdvisor</a>由谷歌出品，可以收集主机及容器的CPU、内存、网络和存储的各项指标。它也提供了REST API以供其他程序来收集这些指标。可以很简单地用容器将它启动起来。它提供了一个页面，通过下面这幅图可以有个直观地认识：<br><img src="/img/cAdvisor.jpg" alt=""><br>kubelet集成了cAdvisor，由于kubernetes会在每个slave上启动kubelet，所以我们不用额外运行cAdvisor容器，就能够监控所有slave的主机和容器。</p>
<p>从cAdvisor提供的漂亮页面上，我们能看到某台主机及其中的容器监控数据。但是还不够，我们想要的是整个集群的数据，而非一个个单体。这时候就轮到<a href="https://github.com/kubernetes/heapster" target="_blank" rel="external">heapster</a>出场了。它支持cAdvisor和kubernetes v1.0.6及后续的版本。运行heapster需要指定两个参数：一个是用https的方式启动的kubernetes api server用来收集数据，另一个是将收集到的数据储存起来的地方，以供随时查看。</p>
<h2 id="u6570_u636E_u5B58_u50A8"><a href="#u6570_u636E_u5B58_u50A8" class="headerlink" title="数据存储"></a>数据存储</h2><p><a href="https://influxdata.com/time-series-platform/influxdb/" target="_blank" rel="external">InfluxDB</a>正是这样一个数据存储的地方。它是InfluxData公司开发的一个分布式键值<a href="http://www.infoq.com/cn/articles/database-timestamp-01" target="_blank" rel="external">时序数据库</a>，也就是说，任何数据都包含时间属性。这样可以很方便地查询到某段时间内的监控数据。举个栗子，查找5分钟前的数据：<code>WHERE time &gt; NOW() - 5m</code>。InfluxDB提供了前端页面供我们查找数据：<br><img src="/img/InfluxDB.jpg" alt=""></p>
<p>听说InfluxDB的性能一般，如果使用中遇到坑，可以试试<a href="http://opentsdb.net/" target="_blank" rel="external">OpenTSDB</a>。</p>
<h2 id="u6570_u636E_u53EF_u89C6_u5316"><a href="#u6570_u636E_u53EF_u89C6_u5316" class="headerlink" title="数据可视化"></a>数据可视化</h2><p>数据也都整合起来了，现在缺的是一个页面将这些数据显示出来。<a href="http://grafana.org/" target="_blank" rel="external">Grafana</a>是纯js开发的、拥有很炫页面的，你们喜欢的Darcula风格的前端。只要指定InfluxDB的url，它就可以轻易地将数据显示出来。看看这个页面：<br><img src="/img/Grafana.jpg" alt=""></p>
<p>heapster的数据除了传送出去保存起来，也可以被<a href="https://github.com/kubernetes/kubedash" target="_blank" rel="external">kubedash</a>所用。它也提供了监控信息的实时聚合页面，可是由于没有地方储存，看不了历史数据：<br><img src="/img/kubedash.jpg" alt=""></p>
<h2 id="u544A_u8B66"><a href="#u544A_u8B66" class="headerlink" title="告警"></a>告警</h2><p>InfluxData公司除了InfluxDB，还提供了一整套的<a href="https://influxdata.com/get-started/what-is-the-tick-stack/" target="_blank" rel="external">TICK stack</a>开源方案，其中的<a href="https://influxdata.com/time-series-platform/kapacitor/" target="_blank" rel="external">Kapacitor</a>正是一个我们需要的告警平台。它使用叫做<a href="https://docs.influxdata.com/kapacitor/v0.2/tick/" target="_blank" rel="external">TICKscript</a>的DSL，通过数据流水线来定义各种任务。通知方式除了写log、发送http请求和执行脚本，还支持<a href="https://slack.com/" target="_blank" rel="external">Slack</a>、<a href="https://www.pagerduty.com/" target="_blank" rel="external">PagerDuty</a>和<a href="https://victorops.com/" target="_blank" rel="external">VictorOps</a>。因为Kapacitor和InfluxDB都是InfluxData公司的产品，所以它们之间的无缝集成也是理所当然的。</p>
<h2 id="u5176_u4ED6_u89E3_u51B3_u65B9_u6848"><a href="#u5176_u4ED6_u89E3_u51B3_u65B9_u6848" class="headerlink" title="其他解决方案"></a>其他解决方案</h2><p><a href="https://github.com/prometheus/prometheus" target="_blank" rel="external">Prometheus</a>是一个监控系统解决方案，包含了数据采集、时序数据库、UI可视化、告警等诸多功能。它的特点是可以实现多纬度的监控，在对比不同实例的监控数据图上有优势。它还有许多的<a href="https://prometheus.io/docs/instrumenting/exporters/" target="_blank" rel="external">exporter</a>可以很方便地从许多第三方应用中导出数据，如Apache、AWS、Redis等，也支持Mesos、Kubernetes和Kubernetes-Mesos。可以参考<a href="/prometheus">《用容器轻松搭建Prometheus运行环境》</a>来自己搭建一个测试环境。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[传统企业敏捷转型纪实（一）]]></title>
      <url>http://qinghua.github.io/waterfall-to-agile-1/</url>
      <content type="html"><![CDATA[<p>这是发生在某大型企业中的某个部门的事儿。他们有六七十名成员，运用瀑布开发模式，三个月发布一次内部产品。部署的过程长达一周，质量堪忧，交付日难以保证。于是请来一位很有经验的敏捷教练Ken，来帮助这个部门做敏捷转型，解决问题。本文的主要内容是在培训完敏捷的基本思想后，<a href="http://scaledagileframework.com/pi-planning/" target="_blank" rel="external">PI Planning</a>（启动会议）上发生的事。本系列目前有两篇：</p>
<ul>
<li><a href="/waterfall-to-agile-1">传统企业敏捷转型纪实（一）</a></li>
<li><a href="/waterfall-to-agile-2">传统企业敏捷转型纪实（二）</a><a id="more"></a>
</li>
</ul>
<h2 id="u56E2_u961F_u6784_u5EFA"><a href="#u56E2_u961F_u6784_u5EFA" class="headerlink" title="团队构建"></a>团队构建</h2><p>这个部门由四个团队组成，各自主管产品的一部分。每个团队都有对交付负责的人，称为PO（Product Owner）。Ken的第一步是让所有团队选出一名SPO（Super Product Owner），来对整个产品负责。SPO不做兼职，负责解决各种问题，其最重要的任务是做决策。而对于一款探索性的产品来说，决策是没有对错之分的，只是它需要靠执行力和客户反馈来修正。PO们需要力挺SPO，充分信任SPO的决策，并以团队的执行力来保证决策的执行。所以有个PO与SPO隔空喊话效忠的过程。</p>
<p>这个部门比较特殊的地方在于，大部分成员属于两个外包公司，其中有不少新人。大家对敏捷都没有什么概念，对要做的事也心有疑虑。有鉴于此，Ken让两个外包公司的人各选出1名leader来当<a href="http://scaledagileframework.com/scrum-master/" target="_blank" rel="external">SM</a>（scrum master）。如果团队成员士气低落，不管任何原因，都可以找SM沟通。如果涉及到甲方公司，便由SM来沟通PO处理。这样做的目的是让每个团队成员都有渠道摆脱自己受到的干扰，增加工作效率。团队成员也需要在团队中建立起自己的人脉，好让自己遇到问题时容易找人帮忙。SM主要负责沟通，PO带领团队前进。这样的构建适用于200~300人以下的团队。</p>
<h2 id="u4E86_u89E3_u4EA7_u54C1"><a href="#u4E86_u89E3_u4EA7_u54C1" class="headerlink" title="了解产品"></a>了解产品</h2><p>要做好产品，首先需要让团队成员理解产品，建立共识。如果只见树木不见森林，那么人人都只是开发自己的那一亩三分地，根本无从得知自己的工作在整个产品中处于什么样的位置，那怎么能做好这个产品呢？现实中，这个产品有着非常复杂的架构，甚至没有一个人能完整地解释整个架构图。Ken建议SPO找几个资深人员，专门抽出一天时间给所有人都讲清楚架构。这很重要，如果你连孩子是男是女都不知道，怎么抚养ta？团队成员需要非常了解产品，而不仅仅是某个需求或者某个版本。要做产品，不是为了做事而做事。同时，Ken也建议所有成员都花时间在架构课之前自学其中的一些重要技术，以便让自己能够在架构课上更加清除对方究竟在讲什么。也就是预习，省的回头听不懂。</p>
<h2 id="u65E5_u7A0B_u7BA1_u7406"><a href="#u65E5_u7A0B_u7BA1_u7406" class="headerlink" title="日程管理"></a>日程管理</h2><p>接下来用倒推法确定迭代截止日。假设产品5月底上线，需要提前一个月也就是4月底出beta版。需要3周的SIT时间，所以差不多是4月8号所有迭代完成。如果每两周一个迭代，从下周一算第一迭代开始，扣去春节，那么正好有5个迭代。如何能保证按时交付呢？需求可能发生变化，环境可能有问题，心情可能不太好影响了效率。迭代的意义在于提早发现风险。所以每个团队成员遇到问题时，需要尽快把这个问题暴露出来，否则，按时完成是不太可能的。</p>
<h2 id="u4F30_u7B97_u5DE5_u4F5C_u91CF"><a href="#u4F30_u7B97_u5DE5_u4F5C_u91CF" class="headerlink" title="估算工作量"></a>估算工作量</h2><p>因为是从瀑布开发模式转过来的，所以现在每个团队手里都有一大堆需求。这里使用估点的方式来估算每个需求的工作量，转化成各个<a href="https://en.wikipedia.org/wiki/User_story" target="_blank" rel="external">User Story</a>。先找一个清晰的需求，最好半天就能开发完成，再半天测试完成。对这个story估点为1。以其为基准，其他的story与它相比较，得到其他story的估点。估的点数是在一个斐波那契数列里的：<a href="https://en.wikipedia.org/wiki/Planning_poker" target="_blank" rel="external">1，2，3，5，8，13，20，40，100</a>（当然后面几个不是）。例如基准story是3点，如果一个story感觉比它难上两个等级，那这个story应该是8个点。如果比它容易一个等级，那应该是2个点。如果难上4个等级呢？因为估点是个主观的过程，而且是比较不精确的。所以当差别很大的时候误差也会很大的。20，40，100这三个数虽然不是斐波那契数列，但也有它的含义。如果一个story只有一行字，谁也说不清它包含着什么，那就是100点。如果知道一部分，那就是40点。如果知道得更多，那就是20点。当然这也是非常主观且粗糙的，但是当你看到这3个数的时候自然就知道应该先把需求搞清楚。</p>
<p>值得一提的是如果一个story估点为8，并不意味着它需要在整8天的时候做完。这个story和其他story一样，需要在最短的时间内有质量地完成。8代表着这个story的复杂度，或者说它是一个风险识别指标。如果做这个story的时候出了问题，需要开发人员尽快把这个问题暴露出来，就像上面讲的那样。而PO、SPO应该要想办法解决这个问题。如果问题超出SPO的权力，那就需要SPO的决策–可以选择不做这个story，或者只做一部分，或者绕过去。估点往往需要很长的时间。为了效率起见，当<a href="https://en.wikipedia.org/wiki/Business_analyst" target="_blank" rel="external">Business Analyst</a>讲完story时，团队成员应该有意识地思考：这个story有什么业务价值？是必须要做的吗？只有必要的story才估点。估点时新人由于还不熟悉背景，可以仅旁观不参与。参与的成员们同时伸手指表示点数，如果一样就记下点数，跳到下一个story。否则，大家就需要解释为什么自己估的点数是这个数，最后由PO拍脑袋做决策。</p>
<p>估点是个很费时，但又很重要的事情，所以先由一个团队演示几个story，其他团队观看，等大家都了解了，再由所有团队自行估点。</p>
<h2 id="u8FED_u4EE3_u8BA1_u5212"><a href="#u8FED_u4EE3_u8BA1_u5212" class="headerlink" title="迭代计划"></a>迭代计划</h2><p>最后就是安排工作量了。先要确认所有人力是否可以100%地投入。资深成员可以算全职，新人算半职，资深成员但还兼其他工作安排的也算半职。假如说最后我们得到了这样的数：</p>
<ul>
<li>全职开发：3个</li>
<li>半职开发：4个</li>
<li>全职测试：1个</li>
<li>半职测试：2个</li>
</ul>
<p>如果第一迭代有10个工作日，那么我们就能计算出来最大工作量：<code>(3+1)×10+(4+2)×10÷2=70</code>。由于是春节，可能请假会比较多，扣去请假天数，也许得到60点。然后是打折，由各组PO和SPO商量，得到一个折扣。这个折扣可能是：我们组对外部环境依赖很多，第一迭代刚开始效率会很低，春节假期效率不高，团队成员都是单身需要相亲无心干活等等等等。比如说第一迭代打个7折，就能得到合理工作量：<code>60×7=42</code>。由此，我们就得到最大工作量和合理工作量。算出各个迭代的工作量，把它们写在显眼处。最后将先前估好点的story按优先级及依赖顺序往里安排，点数到合理工作量即可。至此，一个看似合理的、由团队成员做出的计划便产生了。项目启动会议完成。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[基于docker的MySQL主从复制（replication）]]></title>
      <url>http://qinghua.github.io/mysql-replication/</url>
      <content type="html"><![CDATA[<p><a href="http://dev.mysql.com/doc/refman/5.7/en/replication.html" target="_blank" rel="external">MySQL复制技术</a>可以异步地将主数据库的数据同步到从数据库。根据设置可以复制所有数据库、指定数据库甚至可以指定表。本文的主要内容是怎样用docker从零开始搭建mysql主从复制环境，支持binary logging方式和GTIDs方式。<br><a id="more"></a></p>
<p>MySQL 5.7支持多种复制方法。<a href="http://dev.mysql.com/doc/refman/5.7/en/replication-howto.html" target="_blank" rel="external">传统的方法</a>是master使用binary logging，slave复制并重放日志中的事件。<a href="http://dev.mysql.com/doc/refman/5.7/en/replication-gtids.html" target="_blank" rel="external">另一种方法</a>是利用GTIDs（global transaction identifiers）将所有未执行的事务在slave重放。</p>
<h2 id="binary_logging_u65B9_u5F0F"><a href="#binary_logging_u65B9_u5F0F" class="headerlink" title="binary logging方式"></a>binary logging方式</h2><p>接下来先用传统的方法试一下。使用<a href="https://hub.docker.com/r/library/mysql/" target="_blank" rel="external">MySQL 5.7镜像</a>，将<code>/etc/mysql/conf.d/</code>复制到主机，然后修改配置：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> --name=mysql mysql:<span class="number">5.7</span></span><br><span class="line">docker cp mysql:/etc/mysql/my.cnf my.cnf</span><br></pre></td></tr></table></figure></p>
<p>master的配置在<code>my.cnf</code>文件中是这样的，改完后另存为<code>/vagrant/mysql/mymaster.cnf</code>：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[mysqld]</span></span><br><span class="line"><span class="setting">log-bin=<span class="value">mysql-bin # 使用binary logging，mysql-bin是log文件名的前缀</span></span></span><br><span class="line"><span class="setting">server-id=<span class="value"><span class="number">1</span>       # 唯一服务器ID，非<span class="number">0</span>整数，不能和其他服务器的server-id重复</span></span></span><br></pre></td></tr></table></figure></p>
<p>slave的配置就更简单了，改完后另存为<code>/vagrant/mysql/myslave.cnf</code>：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[mysqld]</span></span><br><span class="line"><span class="setting">server-id=<span class="value"><span class="number">2</span>       # 唯一服务器ID，非<span class="number">0</span>整数，不能和其他服务器的server-id重复</span></span></span><br></pre></td></tr></table></figure></p>
<p>slave没有必要非得用binary logging，但是如果用了，除了binary logging带来的好处以外，还能使这个slave成为其他slave的master。现在我们重新启动mysql master和slave：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> mysql</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --net=host \</span><br><span class="line">  --name=master \</span><br><span class="line">  <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">  -v /vagrant/mysql/mymaster.cnf:/etc/mysql/my.cnf \</span><br><span class="line">  mysql:<span class="number">5.7</span></span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=slave \</span><br><span class="line">  <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">  -v /vagrant/mysql/myslave.cnf:/etc/mysql/my.cnf \</span><br><span class="line">  mysql:<span class="number">5.7</span></span><br></pre></td></tr></table></figure></p>
<p>在master创建一个复制用的用户：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it master mysql -uroot -p123456</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">'repl'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'123456'</span>;</span>       <span class="comment">-- '%'意味着所有的终端都可以用这个用户登录</span></span><br><span class="line"><span class="operator"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>,<span class="keyword">REPLICATION</span> <span class="keyword">SLAVE</span> <span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'repl'</span>@<span class="string">'%'</span>;</span> <span class="comment">-- SELECT权限是为了让repl可以读取到数据，生产环境建议创建另一个用户</span></span><br></pre></td></tr></table></figure>
<p>在slave用新创建的用户连接master（记得把MASTER_HOST改为自己的主机IP）：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it slave mysql -uroot -p123456</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CHANGE</span> <span class="keyword">MASTER</span> <span class="keyword">TO</span> MASTER_HOST=<span class="string">'192.168.33.32'</span>, MASTER_USER=<span class="string">'repl'</span>, MASTER_PASSWORD=<span class="string">'123456'</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">START</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SHOW</span> <span class="keyword">SLAVE</span> <span class="keyword">STATUS</span>\<span class="keyword">G</span>       <span class="comment">-- \G用来代替";"，能把查询结果按键值的方式显示</span></span></span><br></pre></td></tr></table></figure>
<p>如果一切正常，应该在<code>Last_Error</code>中能看到<code>Can&#39;t create database &#39;mysql&#39;</code>的错误。这是因为slave也是像master一样正常地启动，mysql数据库已经被创建了，所以不能再将master的mysql数据库同步过来。有4种解决办法：</p>
<ol>
<li><p>通过在slave上运行SQL来跳过这个复制操作的方式来实现。在slave上运行：</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">STOP</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SET</span> <span class="keyword">GLOBAL</span> SQL_SLAVE_SKIP_COUNTER = <span class="number">1</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">START</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SHOW</span> <span class="keyword">SLAVE</span> <span class="keyword">STATUS</span>\<span class="keyword">G</span></span></span><br></pre></td></tr></table></figure>
<p> 不出意外的话，上面的错误应该已经换成了其他错误（例如：<code>Duplicate entry &#39;row_evaluate_cost&#39; for key &#39;PRIMARY&#39;</code>），都是跟mysql这个数据库有关。反复运行上面的SQL直至错误消失。</p>
</li>
<li><p>通过在slave上面配置log文件名及位置的方式来实现。在master上运行：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it master mysql -uroot -p123456</span><br></pre></td></tr></table></figure>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">FLUSH</span> <span class="keyword">TABLES</span> <span class="keyword">WITH</span> <span class="keyword">READ</span> <span class="keyword">LOCK</span>;</span> <span class="comment">--防止有人对master做更新操作使Position持续变化，先锁表</span></span><br><span class="line"><span class="operator"><span class="keyword">SHOW</span> <span class="keyword">MASTER</span> <span class="keyword">STATUS</span>\<span class="keyword">G</span></span></span><br></pre></td></tr></table></figure>
<p> 可以看到<code>File: mysql-bin.000003</code>和<code>Position: 154</code>这样的行。删掉这个旧的slave并重新启动一个新的容器，然后运行：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> slave</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=slave \</span><br><span class="line">  <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">  -v /vagrant/mysql/myslave.cnf:/etc/mysql/my.cnf \</span><br><span class="line">  mysql:<span class="number">5.7</span></span><br><span class="line">docker <span class="built_in">exec</span> -it slave mysql -uroot -p123456</span><br></pre></td></tr></table></figure>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">STOP</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">CHANGE</span> <span class="keyword">MASTER</span> <span class="keyword">TO</span> MASTER_HOST=<span class="string">'192.168.33.32'</span>, MASTER_USER=<span class="string">'repl'</span>, MASTER_PASSWORD=<span class="string">'123456'</span>, MASTER_LOG_FILE=<span class="string">'mysql-bin.000003'</span>, MASTER_LOG_POS=<span class="number">154</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">START</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SHOW</span> <span class="keyword">SLAVE</span> <span class="keyword">STATUS</span>\<span class="keyword">G</span></span></span><br></pre></td></tr></table></figure>
<p> 我们将会看到<code>Slave_IO_Running: Yes</code>和<code>Slave_SQL_Running: Yes</code>。这两项说明我们的slave已经成功启动了。如果先前锁了master的表，记得在master上运行<code>UNLOCK TABLES;</code>来恢复。</p>
</li>
<li><p>通过不记录<code>mysql</code>数据库binary logging的方式来实现。既然<code>mysql</code>不在binary logging里，那它也无法被同步到slave上。在<code>/vagrant/mysql/mymaster.cnf</code>里增加一个参数，如果有多个数据库，可以复制多行：</p>
 <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="setting">binlog-ignore-db=<span class="value">mysql</span></span></span><br></pre></td></tr></table></figure>
<p> 删除master和slave容器然后再重新创建之：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> master</span><br><span class="line">docker rm <span class="operator">-f</span> slave</span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --net=host \</span><br><span class="line">  --name=master \</span><br><span class="line">  <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">  -v /vagrant/mysql/mymaster.cnf:/etc/mysql/my.cnf \</span><br><span class="line">  mysql:<span class="number">5.7</span></span><br><span class="line">docker run <span class="operator">-d</span> \</span><br><span class="line">  --name=slave \</span><br><span class="line">  <span class="operator">-e</span> MYSQL_ROOT_PASSWORD=<span class="number">123456</span> \</span><br><span class="line">  -v /vagrant/mysql/myslave.cnf:/etc/mysql/my.cnf \</span><br><span class="line">  mysql:<span class="number">5.7</span></span><br></pre></td></tr></table></figure>
<p> 然后根据上文所述在master创建一个复制用的用户并在slave用新创建的用户连接master，最后观察<code>Slave_IO_Running</code>和<code>Slave_SQL_Running</code>。</p>
</li>
<li><p>通过不复制<code>mysql</code>数据库binary logging的方式来实现。这种方式很类似上面一种方法，只不过配置在slave端而非master端而已。在<code>/vagrant/mysql/myslave.cnf</code>里增加一个参数，删除master和slave容器然后再重新创建之：</p>
 <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="setting">replicate-ignore-db=<span class="value">mysql</span></span></span><br></pre></td></tr></table></figure>
<p> 其余操作同方法3。</p>
</li>
</ol>
<p>既然slave已经成功启动了，我们便可以测试一下。看看在master上创建一个新数据库是否能同步到slave上：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> master mysql -uroot -p123456 <span class="operator">-e</span> <span class="string">"CREATE DATABASE ggg"</span></span><br><span class="line">docker <span class="built_in">exec</span> slave mysql -uroot -p123456 <span class="operator">-e</span> <span class="string">"SHOW DATABASES"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="GTIDs_u65B9_u5F0F"><a href="#GTIDs_u65B9_u5F0F" class="headerlink" title="GTIDs方式"></a>GTIDs方式</h2><p>下面介绍一下GTIDs方式的主从复制方法。需要修改<code>/vagrant/mysql/mymaster.cnf</code>：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[mysqld]</span></span><br><span class="line"><span class="setting">log-bin=<span class="value">mysql-bin</span></span></span><br><span class="line"><span class="setting">server-id=<span class="value"><span class="number">1</span></span></span></span><br><span class="line"><span class="setting">gtid-mode=<span class="value"><span class="keyword">on</span></span></span></span><br><span class="line"><span class="setting">enforce-gtid-consistency=<span class="value"><span class="keyword">true</span></span></span></span><br></pre></td></tr></table></figure></p>
<p>还需要修改<code>/vagrant/mysql/myslave.cnf</code>（MySQL 5.7.4及之前的版本还需要开启log-bin）：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[mysqld]</span></span><br><span class="line"><span class="setting">server-id=<span class="value"><span class="number">2</span></span></span></span><br><span class="line"><span class="setting">replicate-ignore-db=<span class="value">mysql</span></span></span><br><span class="line"><span class="setting">gtid-mode=<span class="value"><span class="keyword">on</span></span></span></span><br><span class="line"><span class="setting">enforce-gtid-consistency=<span class="value"><span class="keyword">true</span></span></span></span><br></pre></td></tr></table></figure></p>
<p>启动容器，创建复制的用户都和上面一样，在slave增加<code>MASTER_AUTO_POSITION</code>参数来连接master（记得把MASTER_HOST改为自己的主机IP）：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it slave mysql -uroot -p123456</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CHANGE</span> <span class="keyword">MASTER</span> <span class="keyword">TO</span> MASTER_HOST=<span class="string">'192.168.33.32'</span>, MASTER_USER=<span class="string">'repl'</span>, MASTER_PASSWORD=<span class="string">'123456'</span>, MASTER_AUTO_POSITION=<span class="number">1</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">START</span> <span class="keyword">SLAVE</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SHOW</span> <span class="keyword">SLAVE</span> <span class="keyword">STATUS</span>\<span class="keyword">G</span></span></span><br></pre></td></tr></table></figure>
<p>搞定！这样就不需要用到<code>MASTER_LOG_FILE</code>和<code>MASTER_LOG_POS</code>了，省事儿啊。在<code>START SLAVE</code>之前master的其它更新也都会被同步到slave。</p>
<h2 id="u5176_u4ED6_u6280_u5DE7"><a href="#u5176_u4ED6_u6280_u5DE7" class="headerlink" title="其他技巧"></a>其他技巧</h2><p>最后再介绍一些实用技巧：</p>
<ol>
<li>如果master已经有数据了，怎么新增slave：可以先把master的数据导入到slave，再启动slave。具体可以参考<a href="http://dev.mysql.com/doc/refman/5.7/en/replication-setup-slaves.html#replication-howto-existingdata" target="_blank" rel="external">这里</a>。</li>
<li>如果已经有主从复制了，怎么增加slave：思路同上，不过不需要使用master的数据，直接用已有的slave数据就可以了。不需要停止master，新slave使用新的<code>server-id</code>。具体可以参考<a href="http://dev.mysql.com/doc/refman/5.7/en/replication-howto-additionalslaves.html" target="_blank" rel="external">这里</a>。</li>
<li><p>slave设置只读操作：在<code>/vagrant/mysql/myslave.cnf</code>里增加参数即可。    <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="setting">read-only=<span class="value"><span class="number">1</span>       # 除非有SUPER权限，否则只读</span></span></span><br><span class="line"><span class="setting">super-read-only=<span class="value"><span class="number">1</span> # SUPER权限也是只读</span></span></span><br></pre></td></tr></table></figure></p>
</li>
<li><p>前面介绍的都是主从，如果需要slave也能同步到master就要设置主主复制：也就是说反过来再做一遍。</p>
</li>
<li>当slave比较多得时候，master的负载可能会成为问题。可以用主从多级复制：以slave为master来再引入新的slave。</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如果有10000台机器，你想怎么玩？（三）持久化]]></title>
      <url>http://qinghua.github.io/kubernetes-in-mesos-3/</url>
      <content type="html"><![CDATA[<p>这次聊聊mesos+k8s的持久化问题。如果我用容器跑一个数据库，比如mysql，我关心的是数据保存在哪里。这样万一这个容器发生意外，起码我的数据还在，还可以东山再起。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a><a id="more"></a>
</li>
</ul>
<p>为了方便部署和升级，我们目前把mesos slave跑在容器里。如果我有一个网络存储比如nfs，ceph之类的，当我命令k8s给我跑一个mysql pod，存储挂载到ceph上的时候，k8s就会先找一个mesos slave，让它挂载远端的ceph。由于mesos slave是在容器里，所以挂载点也在这个容器里，姑且把这个路径叫做<code>/tmp/mesos/slaves/20160105-xxx</code>。然后mysql容器也启动了，挂载了<code>/tmp/mesos/slaves/20160105-xxx</code>–可惜的是这个路径是主机的路径，并不是mesos slave容器里的路径，所以它并不能把数据同步到远端的ceph存储去。持久化失败。</p>
<p>有三种方案可以解决持久化的问题。第一个方案：如果我们要继续使用容器化的mesos slave，有一个办法是提前在主机上挂载远端存储。这样的话，mysql容器就可以配置成hostPath的方式，直接挂载这个主机路径，这样就能把数据同步到远端去。这么做是可行的，但是也有不少缺点。首先，因为不知道mysql容器会在哪台机器上运行，所以不得不在所有的机器上都预加载，这样做就失去了动态性，可能引发更多的问题。其次，不是所有类型的存储都可以被很多机器加载。比如ceph的rbd存储就(最好)只能被一台机器加载。还有，何时卸载？如何卸载？存储太多的时候如何管理？这些都是问题。另一个办法是使用kubernetes的<a href="https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/container-environment.md#container-hooks" target="_blank" rel="external">container hook</a>。目前支持<a href="http://kubernetes.io/docs/api-reference/v1/definitions/#_v1_lifecycle" target="_blank" rel="external">postStart和preStop</a>两个时点。可惜mesos slave容器里的mount并不能为外部所用。直接在mysql容器去做mount理论能行，但是需要主机的root权限，或者是hook挂上http请求去外部挂载，不管怎样都相当于重新自己来一套，并不划算。</p>
<p>第二个方案：还是容器化的mesos slave，但是使用<a href="https://hub.docker.com/r/mesosphere/mesos-slave-dind/" target="_blank" rel="external">docker in docker</a>。这种容器方案会把mysql容器运行在mesos slave容器里面，而不像第一种那样把它运行在与mesos slave并列的主机级别。所以mysql使用的存储自然而然就落到了mesos slave容器里面，而这个路径正是加载了远端ceph的地方。这个方案相对来说在操作上也挺简单，仅仅是换个mesos slave dind的镜像而已。它的缺点也正是由于新容器会运行在mesos slave dind容器里，从而导致这个主容器里面可能同时运行许多个从容器，这样就有点儿把容器当虚拟机的意思了，不是最佳实践。另外在实际操作上还出现了新的问题：比如kubernetes使用rbd方式作为volumn的时候，mesos slave会尝试将一个rbd镜像映射成一个设备<code>/dev/rbd1</code>。这个设备就会跑到主机上而非mesos slave dind容器里，从而使我们不得不将主机的<code>/dev</code>也挂载到mesos slave dind容器里。而这样的操作又会带来更多的问题：比如容器删除时提示<code>device or resource busy</code>，从而无法轻易释放<code>/var/lib/docker/aufs</code>的磁盘资源等等。鉴于继续前行可能会碰到更多更深的坑，我们主动放弃了这个方案，但它的前途也有可能是光明的。</p>
<p>第三个方案：放弃mesos slave的容器化。回顾问题的根源，一切的一切都是因为引入mesos slave的容器造成的。如果把mesos slave还原成系统进程，那么这一堆存储问题都将不复存在。我们仍然有其他手段来实现mesos slave部署和升级的便利性，如自动化脚本、数据用容器等。虽然这样可能引入更大的部署工作量，但这可能是针对这个问题来说更加正统的解决方案。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[用容器轻松搭建ceph实验环境]]></title>
      <url>http://qinghua.github.io/ceph-demo/</url>
      <content type="html"><![CDATA[<p><a href="http://ceph.com/" target="_blank" rel="external">Ceph</a>是一个高性能的PB级分布式文件系统。它能够在一个系统中提供对象存储、块存储和文件存储。本文的主要内容是怎样用docker从零开始搭建ceph环境，并分别以cephfs和rbd的方式加载它。<br><a id="more"></a></p>
<p>这是ceph的模块架构图：<br><img src="http://docs.ceph.com/docs/master/_images/stack.png" alt=""><br>最底层的RADOS提供了对象存储的功能，是ceph的根基所在。所有的其他功能都是在RADOS之上构建的。LIBRADOS看名字就能猜到，它提供了一系列语言的接口，可以直接访问RADOS。RADOSGW基于LIBRADOS实现了REST的接口并兼容S3和Swift。RBD也基于LIBRADOS提供了块存储。最后是CEPH FS直接基于RADOS实现了文件存储系统。想要详细了解它的朋友可以看看<a href="/ceph-radosgw">这篇文章](http://www.wzxue.com/why-ceph-and-how-to-use-ceph/)，把ceph介绍得很清楚。笔者的另一篇文章《通过RADOSGW提供ceph的S3和Swift接口》</a>介绍了RADOSGW的使用方法。</p>
<h2 id="cephfs_u65B9_u5F0F"><a href="#cephfs_u65B9_u5F0F" class="headerlink" title="cephfs方式"></a>cephfs方式</h2><p>Talk is cheap，让我们来看看如何用最简单的方式来搭建一个ceph环境吧。Ceph提供了一个<a href="https://hub.docker.com/r/ceph/demo/" target="_blank" rel="external">deph/demo</a>的docker镜像来给我们做实验，注意<strong>别在产品环境用它（THIS CONTAINER IS NOT RECOMMENDED FOR PRODUCTION USAGE）</strong>。只要装好了docker，跑起来是很容易的：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="operator">-d</span> --net=host -v /etc/ceph:/etc/ceph <span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">0.20</span> <span class="operator">-e</span> CEPH_NETWORK=<span class="number">192.168</span>.<span class="number">0.0</span>/<span class="number">24</span> --name=ceph ceph/demo</span><br></pre></td></tr></table></figure></p>
<p>上面的<code>MON_IP</code>可以填写运行这个镜像的机器IP，<code>CEPH_NETWORK</code>填写允许访问这个ceph的IP范围。启动之后，由于挂载了宿主机的<code>/etc/ceph</code>，这个文件夹里面会生成几个配置文件。其中有一个叫<code>ceph.client.admin.keyring</code>的文件里面有一个<code>key</code>，作为cephfs加载的时候认证会用到。</p>
<p>直接作为cephfs来加载就是一句话的事情：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t ceph -o name=admin,secret=AEAq5XtW5SLsARBAAh6kwpBmGVVjUwPQmZeuik== <span class="number">192.168</span>.<span class="number">0.20</span>:/ /mnt/cephfs</span><br></pre></td></tr></table></figure></p>
<p>用的时候记得事先创建好<code>/mnt/cephfs/</code>这个文件夹，替换<code>secret</code>为你自己的<code>key</code>，再改成用你的ceph服务器ip就好了。</p>
<h2 id="rbd_u65B9_u5F0F"><a href="#rbd_u65B9_u5F0F" class="headerlink" title="rbd方式"></a>rbd方式</h2><p>还有一种方式是作为rbd来加载。这边需要啰嗦几句rbd的模型：最外层是<a href="http://docs.ceph.com/docs/master/rados/operations/pools/" target="_blank" rel="external">pool</a>，相当于一块磁盘，默认的pool名字叫做rbd。每个pool里面可以有多个image，相当于文件夹。每个image可以映射成一个块设备，有了设备就可以加载它。下面我们来尝试一下。如果打算用另一台机器，需要先把<code>/etc/ceph</code>这个文件夹复制过去，这个文件夹里面包含了ceph的连接信息。为了运行ceph的命令，我们还需要安装<code>ceph-common</code>，自己选一个命令吧：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt-get install -y ceph-common</span><br><span class="line">yum install -y ceph-common</span><br></pre></td></tr></table></figure></p>
<p>准备工作做完了，我们首先创建一个名为ggg的pool：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create ggg <span class="number">128</span></span><br></pre></td></tr></table></figure></p>
<p>128代表<a href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/" target="_blank" rel="external">placement-group</a>的数量。每个pg都是一个虚拟节点，将自己的数据存在不同的位置。这样一旦存储挂了，pg就会选择新的存储，从而保证了自动高可用。运行这个命令就可以看到现在系统中的所有pool：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd lspools</span><br></pre></td></tr></table></figure></p>
<p>然后在ggg这个pool里创建一个名为qqq的image：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd create ggg/qqq --size <span class="number">1024</span></span><br></pre></td></tr></table></figure></p>
<p>size的单位是MB，所以这个qqq image的大小为1GB。要是这条命令一直没有响应，试着重启一下ceph/demo容器<code>docker restart ceph</code>，说了这不适合用于生产环境…运行下列命令可以看到ggg的pool中的所有image和查看qqq image的详细信息：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rbd ls ggg</span><br><span class="line">rbd info ggg/qqq</span><br></pre></td></tr></table></figure></p>
<p>接下来要把qqq image映射到块设备中，可能需要root权限：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rbd map ggg/qqq</span><br></pre></td></tr></table></figure></p>
<p>运行这个命令就可以看到映射到哪个设备去了，我的是<code>/dev/rbd1</code>：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd showmapped</span><br></pre></td></tr></table></figure></p>
<p>格式化之：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mkfs.ext4 -m0 /dev/rbd1</span><br></pre></td></tr></table></figure></p>
<p>然后就可以加载了！里面应该有一个<code>lost+found</code>的文件夹：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /mnt/rbd/qqq</span><br><span class="line">sudo mount /dev/rbd1 /mnt/rbd/qqq</span><br><span class="line">ls /mnt/rbd/qqq/</span><br></pre></td></tr></table></figure></p>
<h2 id="u8FD8_u539F_u73AF_u5883"><a href="#u8FD8_u539F_u73AF_u5883" class="headerlink" title="还原环境"></a>还原环境</h2><p>最后把我们的环境恢复回去：卸载-&gt;解除映射-&gt;删除image-&gt;删除pool：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo umount /mnt/rbd/qqq</span><br><span class="line">sudo rbd unmap /dev/rbd1</span><br><span class="line">rbd rm ggg/qqq</span><br><span class="line">ceph osd pool delete ggg</span><br></pre></td></tr></table></figure></p>
<p>如果严格按照上面的命令，你应该会在最后一步得到一个错误提示：Error EPERM: WARNING: this will <em>PERMANENTLY DESTROY</em> all data stored in pool ggg.  If you are <em>ABSOLUTELY CERTAIN</em> that is what you want, pass the pool name <em>twice</em>, followed by –yes-i-really-really-mean-it.</p>
<p>删掉pool，里面的数据就真没有啦，所以要谨慎，除了pool名写两遍（重要的事情不应该是三遍么），还得加上<code>--yes-i-really-really-mean-it</code>的免责声明：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool delete ggg ggg --yes-i-really-really-mean-it</span><br></pre></td></tr></table></figure></p>
<p>最后删掉ceph容器（如果你愿意，还有ceph/demo镜像），就当一切都没有发生过：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker rm <span class="operator">-f</span> ceph</span><br><span class="line">docker rmi ceph/demo</span><br></pre></td></tr></table></figure></p>
<p>悄悄的我走了，正如我悄悄的来；我挥一挥衣袖，不带走一个byte。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如果有10000台机器，你想怎么玩？（二）高可用]]></title>
      <url>http://qinghua.github.io/kubernetes-in-mesos-2/</url>
      <content type="html"><![CDATA[<p>这次聊聊k8s的<a href="http://kubernetes.io/v1.1/docs/admin/high-availability.html" target="_blank" rel="external">高可用性</a>是怎么做的。所谓高可用性，就是在一些服务或机器挂掉了之后集群仍然能正常工作的能力。</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a><a id="more"></a>
</li>
</ul>
<p>作为背景知识，先介绍一下<a href="https://github.com/kubernetes/kubernetes/blob/master/docs/design/architecture.md" target="_blank" rel="external">k8s的架构</a>：<br><img src="https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/design/architecture.png" alt=""></p>
<p>它分为服务器端（master）和客户端（node）。服务器端主要是3个组件：API Server、Controller Manager和Scheduler。API Server是操作人员和k8s的接口。比如我想看一下当前k8s有几个pod在跑，就需要连接到这个API Server上。Controller Manager顾名思义就是管理各种各样的controller比如先前提到的Replication Controller。Scheduler做的事就是把用户想要启动/删除的pod分发到对应的客户端上。客户端主要是2个组件：Kubelet和Proxy。Kubelet负责响应服务器端的Scheduler分出来的任务。Proxy用来接通服务和对应的机器。举个栗子：如果我们运行这个命令：<code>kubectl -s 192.168.33.10:8080 run nginx —image=nginx</code>来启动一个nginx的rc和pod，API Server（192.168.33.10:8080）就会得到消息并把这些数据存放到<a href="https://github.com/coreos/etcd" target="_blank" rel="external">etcd</a>里。Controller Manager就会去创建rc，Scheduler则会找个客户端，把启动pod的描述放到客户端上的某个文件夹里。客户端上的Kubelet会监视这个文件夹，一旦发现有了新的pod描述文件，便会将这个pod启动起来。多说一句，<a href="http://kubernetes.io/docs/admin/kubelet/" target="_blank" rel="external">Kubelet</a>除了监听文件夹或是某个Url，还有种方式是干脆直接启动一个Http Server让别人来调用。</p>
<p>高可用的情况下，由于用户的命令直接操作的是API Server，所以当API Server挂掉的时候，需要能自动重启。我们可以使用k8s客户端上现成的Kubelet来满足这个需求。Kubelet有一个Standalone模式，把启动API Server的描述文件丢到Kubelet的监视文件夹里就好了。当Kubelet发现API Server挂掉了，就会自动再启动一个API Server，反正新旧API Server连接的存储etcd还是原来那一个。API Server高可用了，要是Kubelet挂了呢？这个…还得监视一下Kubelet…可以用monit之类的东东，这边就不细说了。当然etcd也需要高可用，但是作为分布式存储来说，它的高可用相对而言较为简单并且跟k8s关联不大，这里也不提了。</p>
<p>刚刚提到的都是进程或容器挂掉的高可用。但是万一整个机器都完蛋了，咋办呢？最直接的做法就是整它好几个服务端，一个挂了还有其他的嘛。好几个服务端就有好几个API Server，其中一个为主，其他为从，简单地挂在一个负载均衡如HAProxy上就可以了。如果还嫌HAProxy上可能有单点故障，那就再做负载均衡集群好了，本文不再赘述。API Server可以跑多份，但是Controller Manager和Scheduler现在不建议跑多份。怎么做到呢？官方提供了一个叫做podmaster的镜像，用它启动的容器可以连接到etcd上。当它从etcd上发现当前机器的API Server为主机的时候，便会把Controller Manager和Scheduler的描述文件丢到Kubelet的监视文件夹里，于是Kubelet就会把这俩启动起来。若当前机器的API Server为从机时，它会把Controller Manager和Scheduler的描述文件从Kubelet的监视文件夹里删掉，这样就可以保证整个集群中Controller Manager和Scheduler各只有一份。上面说的这些画到图里就是这样滴：<br><img src="http://kubernetes.io/images/docs/ha.svg" alt=""></p>
<p>和mesos配合的话，k8s还有<a href="https://github.com/kubernetes/kubernetes/blob/master/contrib/mesos/docs/ha.md" target="_blank" rel="external">另一种高可用方式</a>。这种方式会给Scheduler増加一个叫做–ha的参数，于是Scheduler就能多个同时工作。但是官方也说了，不建议同时起2个以上的Scheduler。这种高可用方式的其它配置还是跟上文所说的一样，照样得使用podmaster，只不过它这回只用管Controller Manager一个而已。</p>
<p>做了这么多，终于把k8s master搞定了。但是还没完，node们还在等着我们呢！如果没用mesos，那就需要把node们的Kubelet重启一下，让它们连接到API Server的负载均衡上去。要是用了mesos就会简单一点儿，因为node们的Kubelet就是由Scheduler帮忙起起来的。记得吗？服务器端我们已经搞定了~</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如果有10000台机器，你想怎么玩？（一）概述]]></title>
      <url>http://qinghua.github.io/kubernetes-in-mesos-1/</url>
      <content type="html"><![CDATA[<p>这一系列文章主要是关于kubernetes和mesos集群管理的内容，里面不会说用啥命令，怎么操作，而是了解一些基本概念，理清思路。如果你需要的是实操，请参考<a href="/kubernetes-installation">《轻松搭建Kubernetes 1.2版运行环境》</a>。</p>
<p>本系列目前有九篇：</p>
<ul>
<li><a href="/kubernetes-in-mesos-1">如果有10000台机器，你想怎么玩？（一）概述</a></li>
<li><a href="/kubernetes-in-mesos-2">如果有10000台机器，你想怎么玩？（二）高可用</a></li>
<li><a href="/kubernetes-in-mesos-3">如果有10000台机器，你想怎么玩？（三）持久化</a></li>
<li><a href="/kubernetes-in-mesos-4">如果有10000台机器，你想怎么玩？（四）监控</a></li>
<li><a href="/kubernetes-in-mesos-5">如果有10000台机器，你想怎么玩？（五）日志</a></li>
<li><a href="/kubernetes-in-mesos-6">如果有10000台机器，你想怎么玩？（六）性能</a></li>
<li><a href="/kubernetes-in-mesos-7">如果有10000台机器，你想怎么玩？（七）生命周期</a></li>
<li><a href="/kubernetes-in-mesos-8">如果有10000台机器，你想怎么玩？（八）网络</a></li>
<li><a href="/kubernetes-in-mesos-9">如果有10000台机器，你想怎么玩？（九）安全性</a><a id="more"></a>
</li>
</ul>
<p>少年，10000台机器只是哄你进来看看而已。这是个虚数，想做的事情其实是：我有那么几台虚拟机，要对外提供容器化PaaS服务，你想怎么玩？</p>
<p>不管这些机器是虚拟还是实体，是啥操作系统，实际上我拥有的是一堆的资源，如cpu、内存、硬盘等。当有人需要某个服务的时候，我从这堆资源中启动某个服务给对方即可。在单机环境中，操作系统有能力帮我们做这样的事情。当我们需要一个服务时，我们就启动一个应用，这个应用使用了操作系统的一些资源，为我们提供服务。剩下的资源可以为我们提供其他的服务。在集群环境中，<a href="http://mesos.apache.org/" target="_blank" rel="external">mesos</a>有能力帮我们做这样的事情。它就像一个操作系统，告诉我们现在集群中有多少的资源。当我们需要一个服务时，我们就启动一个任务，这个任务使用了集群环境的一些资源，为我们提供服务。剩下的资源可以为我们提供其他的服务。一般情况下我们看到的mesos主页是这样子滴：<br><img src="/img/mesos.jpg" alt=""></p>
<p>我们不希望各个任务太不一样，因为那管理起来很麻烦。神一般的<a href="http://www.docker.com/" target="_blank" rel="external">docker</a>把各种任务都抽象成一个容器，这样启动一个任务就变成启动一个容器了，大大解放了我们的双手，让我还有时间在这里码码字。尽管如此，我们还是需要管理我们的容器。<a href="http://kubernetes.io/" target="_blank" rel="external">Kubernetes</a>就是这样一个容器编排工具。大家叫它k8s，听起来就像i18n那么的亲切。它有自己的一些概念：首先是<a href="http://kubernetes.io/docs/user-guide/pods/" target="_blank" rel="external">pod</a>，它里头可以含着多个容器的实例，是k8s调度的原子单元。其次是<a href="http://kubernetes.io/docs/user-guide/replication-controller/" target="_blank" rel="external">Replication Controller</a>简称rc，它关联一个pod和一个pod数量。最后是<a href="http://kubernetes.io/docs/user-guide/services/" target="_blank" rel="external">service</a>，它通过rc暴露出来。这三个概念听起来没啥，混合起来使用威力十足。举个栗子：pod里面有一个nginx容器，有一个rc关联到这个pod，并暴露出服务以使外界可以访问这个nginx。当访问量很大的时候，运维人员可以把rc的pod数量这个值从1调整成10，k8s会自动把pod变成10份，从而让nginx容器也启动10份，而服务则会自动在这10份pod中做负载均衡（截稿为止，这个负载均衡的算法是随机）。一条命令就能轻易实现扩容，当然前提是mesos那头有足够的资源。Kubernetes有一个kube-ui的插件可以可视化当前的主机、资源、pod、rc、服务等：<br><img src="/img/kube-ui.jpg" alt=""></p>
<p>集群操作系统和容器编排工具都有了，假设我们需要一个mysql服务。用k8s启动一个docker hub下的官方镜像，于是它就会被mesos分配在某台有资源的机器上。用户并不关心到底被分配到哪台机器上，只关心服务能不能用，好不好用。现在问题来了：要是服务挂掉，数据会不会丢失？那么应该怎么做持久化？这里需要引入k8s的另外两个概念：<a href="http://kubernetes.io/docs/user-guide/persistent-volumes/#persistent-volumes" target="_blank" rel="external">PersistentVolume</a>（PV）和<a href="http://kubernetes.io/docs/user-guide/persistent-volumes/#persistentvolumeclaims" target="_blank" rel="external">PersistentVolumeClaim</a>（PVC）。简单说来，PV就是存储资源，它表示一块存储区域。比如：nfs上的、可读写的、10G空间。PVC就是对PV的请求，比如需要–可读写的1G空间。我们的mysql直接挂载在需要的PVC上就可以了，k8s自己会帮这个PVC寻找适配的PV。就算mysql挂掉或者是被停掉不用了，PVC仍然存在并可被其他pod使用，数据不会丢失。</p>
<p>现在数据库也有了，需要一个tomcat服务来使用刚才创建的mysql服务并把自己暴露到公网上。传统上说，要使用数据库那就得在自己应用的xml或config文件中配置一下数据库的链接，java平台上一般是酱紫滴：<code>jdbc:mysql://localhost:3306/dbname</code>。可是mysql服务并不在localhost上，我们也不知道它被分配到哪台机器上去了，怎么写这个链接呢？这里边就涉及到k8s服务发现的概念了。一种方法是，k8s在新启动一个pod的时候，会把当前所有的服务都写到这个pod的容器的环境变量里去。于是就可以使用环境变量来“发现”这个服务。但是这种做法并不推荐，因为它要求在启动pod的时候，它所需要的服务已经存在。是啊，如果服务不存在，怎么知道往环境变量写什么呢？由于环境变量大法严重依赖于启动顺序，所以一般使用DNS大法。k8s提供了kube2sky和skydns的插件，当mysql服务启动后，这哥俩就会监听到mysql服务，并为之提供dns服务。所以只要配置成<code>jdbc:mysql://mysql.default.svc.cluster.local:3306/dbname</code>便可以解决服务发现的问题了。</p>
<p>接着往下走，还会涉及到外部负载均衡、高可用、多租户、监控、安全等一系列挑战，你想怎么玩？</p>
]]></content>
    </entry>
    
  
  
</search>
